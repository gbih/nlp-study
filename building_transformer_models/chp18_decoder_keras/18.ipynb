{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f3ed5a0-3f7c-41bf-ae01-c82f0553875b",
   "metadata": {},
   "source": [
    "<a id='top'></a><a name='top'></a>\n",
    "# Chapter 18: Implementing the Transformer Decoder in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bd2604-eb52-47dc-9128-950cbbf9b582",
   "metadata": {},
   "source": [
    "* [Introduction](#introduction)\n",
    "* [18.0 Imports and Setup](#18.0)\n",
    "* [18.1 Recap of the Transformer Encoder](#18.1)\n",
    "* [18.2 Implementing Transformer Encoder from Scratch](#17.2)\n",
    "* [18.3 Testing Out the Code](#18.3)\n",
    "* [Extra](#extra)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10b3b3f-d441-47ee-8407-028c0445cb71",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='introduction'></a><a id='introduction'></a>\n",
    "# Introduction\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "### Dataset\n",
    "\n",
    "* TODO\n",
    "\n",
    "### Explore\n",
    "* The layers that form part of the transformer decoder\n",
    "* How to implement the transformer decoder from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc36887-7910-4f6d-acad-b6d0c406d892",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='18.0'></a><a id='18.0'></a>\n",
    "# 18.0 Imports and Setup\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99b5949c-5f46-428b-b69f-f95add9579c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "req_file = \"requirements_18.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb3fc865-8cb7-4dc9-9a0a-6be508c7c94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements_18.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile {req_file}\n",
    "isort\n",
    "scikit-learn-intelex\n",
    "watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6830f49b-40cd-4afa-a2f1-640ef256e8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "IS_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IS_COLAB:\n",
    "    print(\"Installing packages\")\n",
    "    !pip install --upgrade --quiet -r {req_file}\n",
    "else:\n",
    "    print(\"Running locally.\")\n",
    "\n",
    "# Need to import before sklearn\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "235d22d2-de94-4e15-b58e-412fd08cfdee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting imports.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile imports.py\n",
    "import locale\n",
    "import math\n",
    "import pprint\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.activations import softmax\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.layers import LayerNormalization\n",
    "from tensorflow.keras.layers import ReLU\n",
    "from tqdm.auto import tqdm\n",
    "from watermark import watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca95133c-512d-4128-b9ec-b26c7263a342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import locale\n",
      "import math\n",
      "import pprint\n",
      "import warnings\n",
      "\n",
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "from tensorflow.keras import Model\n",
      "from tensorflow.keras.activations import softmax\n",
      "from tensorflow.keras.layers import Dense\n",
      "from tensorflow.keras.layers import Dropout\n",
      "from tensorflow.keras.layers import Embedding\n",
      "from tensorflow.keras.layers import Layer\n",
      "from tensorflow.keras.layers import LayerNormalization\n",
      "from tensorflow.keras.layers import ReLU\n",
      "from tqdm.auto import tqdm\n",
      "from watermark import watermark\n"
     ]
    }
   ],
   "source": [
    "!isort imports.py --sl\n",
    "!cat imports.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f44469f-adae-4c91-8bc6-6d924a90131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "import math\n",
    "import pprint\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.activations import softmax\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.layers import LayerNormalization\n",
    "from tensorflow.keras.layers import ReLU\n",
    "from tqdm.auto import tqdm\n",
    "from watermark import watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83b38539-ed0a-4066-8663-02b26b99052a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.8.12\n",
      "IPython version      : 7.34.0\n",
      "\n",
      "Compiler    : Clang 13.0.0 (clang-1300.0.29.3)\n",
      "OS          : Darwin\n",
      "Release     : 21.6.0\n",
      "Machine     : x86_64\n",
      "Processor   : i386\n",
      "CPU cores   : 4\n",
      "Architecture: 64bit\n",
      "\n",
      "numpy     : 1.23.5\n",
      "tensorflow: 2.9.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def HR():\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "def getpreferredencoding(do_setlocale = True):\n",
    "    return \"UTF-8\"\n",
    "\n",
    "locale.getpreferredencoding = getpreferredencoding\n",
    "warnings.filterwarnings('default')\n",
    "BASE_DIR = '.'\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "seed = 42\n",
    "\n",
    "print(watermark(iversions=True,globals_=globals(),python=True,machine=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a5e937-3e4c-4260-8f19-45f7d0716361",
   "metadata": {},
   "source": [
    "# Previous Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16152948-fece-443b-ae4e-56bdc9f82174",
   "metadata": {},
   "source": [
    "## The Feedforward Network and Layer Normalization\n",
    "\n",
    "* The fully connected FFN in the Vaswani paper consists of two linear transformation with a ReLU activation in between. \n",
    "* Create classes for the `FeedForward` layer and `AddNormalization` layer.\n",
    "* The `FeedForward` class inherits from the `Layer` base class in Keras. \n",
    "* We initialize the `Dense` layers and the ReLU activation.\n",
    "* `d_ff` is the first linear transformation output dimensionality, 2048.\n",
    "* `d_model` is the second linear transformation output dimensionality, 512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "274fd480-a500-4d83-980d-0f42f631221e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(Layer):\n",
    "    def __init__(self, d_ff, d_model, **kwargs):\n",
    "        super().__init__(**kwargs) # handle multiple inheritance\n",
    "        self.fully_connected1 = Dense(d_ff) # First fully connected layer\n",
    "        self.fully_connected2 = Dense(d_model) # Second fully connected layer\n",
    "        self.activation = ReLU() # ReLU activation layer\n",
    "        \n",
    "    # Receive an input and pass it through the two fully connected layers with ReLU activation,\n",
    "    # returning an output of dimensionality equal to 512.\n",
    "    def call(self, x):\n",
    "        x_fc1 = self.fully_connected1(x)\n",
    "        return self.fully_connected2(self.activation(x_fc1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b305d610-1c9e-4558-88e4-c354180b485b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next step is to create the class `AddNormalization` which also inherits from the `Layer`\n",
    "# base class in Keras, and initialize a Layer normalization layer.\n",
    "class AddNormalization(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.layer_norm = LayerNormalization()  # Layer normalization layer\n",
    "\n",
    "    def call(self, x, sublayer_x):\n",
    "        # The sublayer input and output need to be of the same shape to be summed\n",
    "        add = x + sublayer_x\n",
    "\n",
    "        # Apply layer normalization to the sum\n",
    "        return self.layer_norm(add)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf1b19e-11b7-49c5-9c25-d4451a964f41",
   "metadata": {},
   "source": [
    "## MultiHeadAttention \n",
    "\n",
    "Created earlier in Chapter 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f485b380-5efe-4ef1-820b-56f56c36de7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, queries, keys, values, d_k, mask=None):\n",
    "        # Scoring the queries against the keys after transposing the latter, and scaling\n",
    "        scores = tf.matmul(queries, keys, transpose_b=True) / tf.math.sqrt(tf.cast(d_k, tf.float32))\n",
    "\n",
    "        # Apply mask to the attention scores\n",
    "        if mask is not None:\n",
    "            scores += -1e9 * mask\n",
    "\n",
    "        # Computing the weights by a softmax operation\n",
    "        weights = softmax(scores)\n",
    "\n",
    "        # Computing the attention by a weighted sum of the value vectors\n",
    "        return tf.matmul(weights, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4dcea3b7-421e-45b2-97aa-0dcadd72a6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the Multi-Head Attention\n",
    "class MultiHeadAttention(Layer):\n",
    "    def __init__(self, h, d_k, d_v, d_model, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.attention = DotProductAttention()  # Scaled dot product attention\n",
    "        self.heads = h  # Number of attention heads to use\n",
    "        self.d_k = d_k  # Dimensionality of the linearly projected queries and keys\n",
    "        self.d_v = d_v  # Dimensionality of the linearly projected values\n",
    "        self.d_model = d_model  # Dimensionality of the model\n",
    "        self.W_q = Dense(d_k)   # Learned projection matrix for the queries\n",
    "        self.W_k = Dense(d_k)   # Learned projection matrix for the keys\n",
    "        self.W_v = Dense(d_v)   # Learned projection matrix for the values\n",
    "        self.W_o = Dense(d_model) # Learned projection matrix for the multi-head output\n",
    "\n",
    "        \n",
    "    def reshape_tensor(self, x, heads, flag):\n",
    "        if flag:\n",
    "            # Tensor shape after reshaping and transposing:\n",
    "            # (batch_size, heads, seq_length, -1)\n",
    "            x = tf.reshape(x, shape=(tf.shape(x)[0], tf.shape(x)[1], heads, -1))\n",
    "            x = tf.transpose(x, perm=(0, 2, 1, 3))\n",
    "        else:\n",
    "            # Reverting the reshaping and transposing operations:\n",
    "            # (batch_size, seq_length, d_k)\n",
    "            x = tf.transpose(x, perm=(0, 2, 1, 3))\n",
    "            x = tf.reshape(x, shape=(tf.shape(x)[0], tf.shape(x)[1], self.d_k))\n",
    "        return x\n",
    "\n",
    "    \n",
    "    def call(self, queries, keys, values, mask=None):\n",
    "        # Rearrange the queries to be able to compute all heads in parallel\n",
    "        q_reshaped = self.reshape_tensor(self.W_q(queries), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        # Rearrange the keys to be able to compute all heads in parallel\n",
    "        k_reshaped = self.reshape_tensor(self.W_k(keys), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        # Rearrange the values to be able to compute all heads in parallel\n",
    "        v_reshaped = self.reshape_tensor(self.W_v(values), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        # Compute the multi-head attention output using the reshaped queries,\n",
    "        # keys, and values\n",
    "        o_reshaped = self.attention(q_reshaped, k_reshaped, v_reshaped, self.d_k, mask)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        # Rearrange back the output into concatenated form\n",
    "        output = self.reshape_tensor(o_reshaped, self.heads, False)\n",
    "        # Resulting tensor shape: (batch_size, input_seq_length, d_v)\n",
    "\n",
    "        # Apply one final linear projection to the output to generate the multi-head\n",
    "        # attention. Resulting tensor shape: (batch_size, input_seq_length, d_model)\n",
    "        return self.W_o(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a15ed6-719c-4e26-bdfb-463fe8da05d2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c3784a-4f35-4e06-bd94-d34629be48d6",
   "metadata": {},
   "source": [
    "## The Encoder Layer\n",
    "\n",
    "Next, we implement the encoder layer, which the transformer encoder will replicate identically N times. \n",
    "\n",
    "For this, we create the class `EncoderLayer` and initialize all the sub-layers that it consists of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0fbbb3b6-0759-47cd-98f2-ddfbaa9576d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(Layer):\n",
    "    def __init__(self, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.multihead_attention = MultiHeadAttention(h, d_k, d_v, d_model)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.add_norm1 = AddNormalization()\n",
    "        self.feed_forward = FeedForward(d_ff, d_model)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "        self.add_norm2 = AddNormalization()\n",
    "        \n",
    "    def call(self, x, padding_mask, training):\n",
    "        # Multi-head attention layer\n",
    "        multihead_output = self.multihead_attention(x, x, x, padding_mask)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "        \n",
    "        # Add in a dropout layer\n",
    "        multihead_output = self.dropout1(multihead_output, training=training)\n",
    "        \n",
    "        # Followed by an AddNormalization layer\n",
    "        addnorm_output = self.add_norm1(x, multihead_output)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "        \n",
    "        # Followed by a fully connected layer\n",
    "        feedforward_output = self.feed_forward(addnorm_output)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "        \n",
    "        # Add in another dropout layer\n",
    "        feedforward_output = self.dropout2(feedforward_output, training=training)\n",
    "        \n",
    "        # Followed by another AddNormalization layer\n",
    "        return self.add_norm2(addnorm_output, feedforward_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cbd14b2e-dad8-4564-a532-150efe124e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = EncoderLayer(1,2,3,4,5,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1497c741-3e3a-42da-ba90-f1fad76394d3",
   "metadata": {},
   "source": [
    "## The Transformer Encoder\n",
    "\n",
    "Create a class for the transformer encoder, which should be named `Encoder`.\n",
    "\n",
    "The transformer encoder receives an input sequence, consisting of word embedding and positional encoding. To compute the positional encoding, use the `PositionEmbeddingFixedWeights` class.\n",
    "\n",
    "We also create a class method `call()` that applies word embedding and positional encoding to the input sequence and feeds the result to `N` encoder layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "682928d2-e2ae-4e0f-acb3-3579068a0c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Layer):\n",
    "    def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate,\n",
    "                        **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pos_encoding = PositionEmbeddingFixedWeights(sequence_length, vocab_size, d_model)\n",
    "        self.dropout = Dropout(rate)\n",
    "        self.encoder_layer = [EncoderLayer(h, d_k, d_v, d_model, d_ff, rate) for _ in range(n)]\n",
    "        \n",
    "    def call(self, input_sentence, padding_mask, training):\n",
    "        # Generate the positional encoding\n",
    "        pos_encoding_output = self.pos_encoding(input_sentence)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "        \n",
    "        # Add a dropout layer\n",
    "        x = self.dropout(pos_encoding_output, training=training)\n",
    "        \n",
    "        for i, layer in enumerate(self.encoder_layer):\n",
    "            x = layer(x, padding_mask, training)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce83e28-c0f6-499e-ae4a-4af71cf20b49",
   "metadata": {},
   "source": [
    "## PositionalEmbeddingFixedWeights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa1aaffa-3d40-4ae4-bd51-4a096addf1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbeddingFixedWeights(Layer):\n",
    "    def __init__(self, seq_length, vocab_size, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        word_embedding_matrix = self.get_position_encoding(vocab_size, output_dim)\n",
    "        pos_embedding_matrix = self.get_position_encoding(seq_length, output_dim)\n",
    "        self.word_embedding_layer = Embedding(\n",
    "            input_dim=vocab_size, output_dim=output_dim,\n",
    "            weights=[word_embedding_matrix],\n",
    "            trainable=False\n",
    "        )\n",
    "        self.position_embedding_layer = Embedding(\n",
    "            input_dim=seq_length, output_dim=output_dim,\n",
    "            weights=[pos_embedding_matrix],\n",
    "            trainable=False\n",
    "        )\n",
    "\n",
    "    def get_position_encoding(self, seq_len, d, n=10000):\n",
    "        P = np.zeros((seq_len, d))\n",
    "        for k in range(seq_len):\n",
    "            for i in np.arange(int(d/2)):\n",
    "                denominator = np.power(n, 2*i/d)\n",
    "                P[k, 2*i] = np.sin(k/denominator)\n",
    "                P[k, 2*i+1] = np.cos(k/denominator)\n",
    "        return P\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        position_indices = tf.range(tf.shape(inputs)[-1])\n",
    "        embedded_words = self.word_embedding_layer(inputs)\n",
    "        embedded_indices = self.position_embedding_layer(position_indices)\n",
    "        return embedded_words + embedded_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3aab1fea-e950-458f-bbe4-1572c2e03ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, queries, keys, values, d_k, mask=None):\n",
    "        # Scoring the queries against the keys after transposing the latter, and scaling\n",
    "        scores = tf.matmul(queries, keys, transpose_b=True) / tf.math.sqrt(tf.cast(d_k, tf.float32))\n",
    "\n",
    "        # Apply mask to the attention scores\n",
    "        if mask is not None:\n",
    "            scores += -1e9 * mask\n",
    "\n",
    "        # Computing the weights by a softmax operation\n",
    "        weights = softmax(scores)\n",
    "\n",
    "        # Computing the attention by a weighted sum of the value vectors\n",
    "        return tf.matmul(weights, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69b9957b-247d-4d02-92dc-b5cb448cee18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the Multi-Head Attention\n",
    "class MultiHeadAttention(Layer):\n",
    "    def __init__(self, h, d_k, d_v, d_model, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.attention = DotProductAttention()  # Scaled dot product attention\n",
    "        self.heads = h  # Number of attention heads to use\n",
    "        self.d_k = d_k  # Dimensionality of the linearly projected queries and keys\n",
    "        self.d_v = d_v  # Dimensionality of the linearly projected values\n",
    "        self.d_model = d_model  # Dimensionality of the model\n",
    "        self.W_q = Dense(d_k)   # Learned projection matrix for the queries\n",
    "        self.W_k = Dense(d_k)   # Learned projection matrix for the keys\n",
    "        self.W_v = Dense(d_v)   # Learned projection matrix for the values\n",
    "        self.W_o = Dense(d_model) # Learned projection matrix for the multi-head output\n",
    "\n",
    "        \n",
    "    def reshape_tensor(self, x, heads, flag):\n",
    "        if flag:\n",
    "            # Tensor shape after reshaping and transposing:\n",
    "            # (batch_size, heads, seq_length, -1)\n",
    "            x = tf.reshape(x, shape=(tf.shape(x)[0], tf.shape(x)[1], heads, -1))\n",
    "            x = tf.transpose(x, perm=(0, 2, 1, 3))\n",
    "        else:\n",
    "            # Reverting the reshaping and transposing operations:\n",
    "            # (batch_size, seq_length, d_k)\n",
    "            x = tf.transpose(x, perm=(0, 2, 1, 3))\n",
    "            x = tf.reshape(x, shape=(tf.shape(x)[0], tf.shape(x)[1], self.d_k))\n",
    "        return x\n",
    "\n",
    "    \n",
    "    def call(self, queries, keys, values, mask=None):\n",
    "        # Rearrange the queries to be able to compute all heads in parallel\n",
    "        q_reshaped = self.reshape_tensor(self.W_q(queries), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        # Rearrange the keys to be able to compute all heads in parallel\n",
    "        k_reshaped = self.reshape_tensor(self.W_k(keys), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        # Rearrange the values to be able to compute all heads in parallel\n",
    "        v_reshaped = self.reshape_tensor(self.W_v(values), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        # Compute the multi-head attention output using the reshaped queries,\n",
    "        # keys, and values\n",
    "        o_reshaped = self.attention(q_reshaped, k_reshaped, v_reshaped, self.d_k, mask)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        # Rearrange back the output into concatenated form\n",
    "        output = self.reshape_tensor(o_reshaped, self.heads, False)\n",
    "        # Resulting tensor shape: (batch_size, input_seq_length, d_v)\n",
    "\n",
    "        # Apply one final linear projection to the output to generate the multi-head\n",
    "        # attention. Resulting tensor shape: (batch_size, input_seq_length, d_model)\n",
    "        return self.W_o(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "df9cd2f3-3796-4c25-9476-542b7ff01edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "input_seq_length = 5  # Maximum length of the input sequence\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_model = 512  # Dimensionality of the model sub-layers' outputs\n",
    "batch_size = 64  # Batch size from the training process\n",
    "\n",
    "queries = np.random.random((batch_size, input_seq_length, d_k))\n",
    "keys = np.random.random((batch_size, input_seq_length, d_k))\n",
    "values = np.random.random((batch_size, input_seq_length, d_v))\n",
    "\n",
    "multihead_attention = MultiHeadAttention(h, d_k, d_v, d_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbeb24f5-b5e8-4610-8576-afb8bafce9ee",
   "metadata": {},
   "source": [
    "\n",
    "## Testing out the Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c18f7607-ef3e-4598-a277-18234a31d28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_seq:\n",
      "(64, 5)\n",
      "[[0.375 0.951 0.732 0.599 0.156]\n",
      " [0.156 0.058 0.866 0.601 0.708]\n",
      " [0.021 0.97  0.832 0.212 0.182]]\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "h = 8 # Number of self-attention heads\n",
    "d_k = 64 # Dimensionality of linearly projected queries and keys\n",
    "d_v = 64 # Dimensionality of linearly projected values\n",
    "d_ff = 2048 # Dimensionality of inner fully connected layer\n",
    "d_model = 512 # Dimensionality of model sub-layers output\n",
    "n = 6 # Number of layers in the encoder stack\n",
    "\n",
    "batch_size = 64 # Batch size from the training process\n",
    "dropout_rate = 0.1 # Frequency of dropping the input units in the dropout layers\n",
    "\n",
    "\n",
    "# Use dummy data for input sequence\n",
    "enc_vocab_size = 20 # Vocabulary size for the encoder\n",
    "input_seq_length = 5 # Maximum length of the input sequence\n",
    "input_seq = np.random.random((batch_size, input_seq_length))\n",
    "\n",
    "print(\"input_seq:\")\n",
    "print(input_seq.shape)\n",
    "print(input_seq.round(3)[:3])\n",
    "HR()\n",
    "\n",
    "# Create a new instance of the Encoder class, assigning its\n",
    "# output to the encoder variable, feeding in the input arguments,\n",
    "# and printing the result.\n",
    "# Set the padding mask argument to None for now.\n",
    "encoder = Encoder(enc_vocab_size, input_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "\n",
    "test_encoder = encoder(input_seq, None, True).numpy().round(3)\n",
    "\n",
    "# print(\"encoder output:\")\n",
    "# print(test_encoder.shape)\n",
    "# print()\n",
    "# print(test_encoder[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560a82fc-ff4f-4193-8d38-0a1302faf9ff",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='18.1'></a><a id='18.1'></a>\n",
    "# 18.1 Recap of the Transformer Decoder\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf562bc-a211-45e8-b190-38b0ceadd48b",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='18.2'></a><a id='18.2'></a>\n",
    "# 18.2 Implementing the Transformer Decoder from Scratch\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "Create a class for the decoder layer that makes use of the sub-layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7762cfdf-64e4-4f50-b5f5-6c7a7a813cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder output:\n",
      "(64, 5, 512)\n",
      "\n",
      "[[[ 0.198 -0.251 -0.351 ... -0.737 -1.346  1.131]\n",
      "  [ 0.267 -0.327 -0.215 ... -0.775 -1.429  1.141]\n",
      "  [ 0.255 -0.401 -0.156 ... -0.836 -1.477  1.164]\n",
      "  [ 0.143 -0.398 -0.224 ... -0.886 -1.445  1.178]\n",
      "  [ 0.049 -0.291 -0.348 ... -0.886 -1.37   1.165]]\n",
      "\n",
      " [[-0.062 -0.014 -0.396 ... -0.634 -1.062  0.866]\n",
      "  [-0.003 -0.08  -0.284 ... -0.665 -1.151  0.868]\n",
      "  [-0.034 -0.14  -0.261 ... -0.719 -1.201  0.888]\n",
      "  [-0.157 -0.13  -0.344 ... -0.785 -1.182  0.907]\n",
      "  [-0.27  -0.028 -0.47  ... -0.805 -1.109  0.92 ]]\n",
      "\n",
      " [[ 0.087 -0.048 -0.446 ... -0.61  -1.215  1.024]\n",
      "  [ 0.145 -0.13  -0.332 ... -0.654 -1.317  1.027]\n",
      "  [ 0.094 -0.199 -0.306 ... -0.711 -1.357  1.041]\n",
      "  [-0.038 -0.192 -0.388 ... -0.754 -1.306  1.046]\n",
      "  [-0.147 -0.08  -0.519 ... -0.766 -1.23   1.051]]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# Implementing the Decoder Layer\n",
    "class DecoderLayer(Layer):\n",
    "    def __init__(self, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.multihead_attention1 = MultiHeadAttention(h, d_k, d_v, d_model)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.add_norm1 = AddNormalization()\n",
    "        self.multihead_attention2 = MultiHeadAttention(h, d_k, d_v, d_model)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "        self.add_norm2 = AddNormalization()\n",
    "        self.feed_forward = FeedForward(d_ff, d_model)\n",
    "        self.dropout3 = Dropout(rate)\n",
    "        self.add_norm3 = AddNormalization()\n",
    "\n",
    "    def call(self, x, encoder_output, lookahead_mask, padding_mask, training):\n",
    "        # Multi-head attention layer\n",
    "        multihead_output1 = self.multihead_attention1(x, x, x, lookahead_mask)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Add in a dropout layer\n",
    "        multihead_output1 = self.dropout1(multihead_output1, training=training)\n",
    "\n",
    "        # Followed by an Add & Norm layer\n",
    "        addnorm_output1 = self.add_norm1(x, multihead_output1)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Followed by another multi-head attention layer\n",
    "        multihead_output2 = self.multihead_attention2(\n",
    "            addnorm_output1, encoder_output, encoder_output, padding_mask)\n",
    "\n",
    "        # Add in another dropout layer\n",
    "        multihead_output2 = self.dropout2(multihead_output2, training=training)\n",
    "\n",
    "        # Followed by another Add & Norm layer\n",
    "        addnorm_output2 = self.add_norm1(addnorm_output1, multihead_output2)\n",
    "\n",
    "        # Followed by a fully connected layer\n",
    "        feedforward_output = self.feed_forward(addnorm_output2)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Add in another dropout layer\n",
    "        feedforward_output = self.dropout3(feedforward_output, training=training)\n",
    "\n",
    "        # Followed by another Add & Norm layer\n",
    "        return self.add_norm3(addnorm_output2, feedforward_output)\n",
    "\n",
    "    \n",
    "\n",
    "# Implementing the Decoder\n",
    "# The transformer decoder takes the decoder layer and replicates it identically N-times.\n",
    "# We create the Decoder() class to implement the transformer decoder:\n",
    "class Decoder(Layer):\n",
    "    def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pos_encoding = PositionEmbeddingFixedWeights(sequence_length, vocab_size, d_model)\n",
    "        self.dropout = Dropout(rate)\n",
    "        self.decoder_layer = [DecoderLayer(h, d_k, d_v, d_model, d_ff, rate) for _ in range(n)]\n",
    "\n",
    "    # This applies word_embedding and positional embedding to the input sequence and\n",
    "    # feeds the result, together with the encoder output, to N decoder layers.\n",
    "    def call(self, output_target, encoder_output, lookahead_mask, padding_mask, training):\n",
    "        # Generate the positional encoding\n",
    "        pos_encoding_output = self.pos_encoding(output_target)\n",
    "        # Expected output shape = (number of sentences, sequence_length, d_model)\n",
    "\n",
    "        # Add in a dropout layer\n",
    "        x = self.dropout(pos_encoding_output, training=training)\n",
    "\n",
    "        # Pass on the positional encoded values to each encoder layer\n",
    "        for i, layer in enumerate(self.decoder_layer):\n",
    "            x = layer(x, encoder_output, lookahead_mask, padding_mask, training)\n",
    "\n",
    "        return x\n",
    "\n",
    "    \n",
    "    \n",
    "# Parameter values from the \"Attention is All You Need\" paper\n",
    "dec_vocab_size = 20  # Vocabulary size for the decoder\n",
    "input_seq_length = 5  # Maximum length of the input sequence\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_ff = 2048  # Dimensionality of the inner fully connected layer\n",
    "d_model = 512  # Dimensionality of the model sub-layers' outputs\n",
    "n = 6  # Number of layers in the decoder stack\n",
    "\n",
    "batch_size = 64  # Batch size from the training process\n",
    "dropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n",
    "\n",
    "input_seq = np.random.random((batch_size, input_seq_length))\n",
    "enc_output = np.random.random((batch_size, input_seq_length, d_model))\n",
    "\n",
    "decoder = Decoder(dec_vocab_size, input_seq_length, h, d_k, d_v, d_model, d_ff, n,\n",
    "                  dropout_rate)\n",
    "\n",
    "\n",
    "test_decoder = decoder(input_seq, enc_output, None, True)\n",
    "\n",
    "print(\"decoder output:\")\n",
    "print(test_decoder.numpy().shape)\n",
    "print()\n",
    "print(test_decoder.numpy()[:3].round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6bb082-3753-47e3-acf5-3475eca5b065",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

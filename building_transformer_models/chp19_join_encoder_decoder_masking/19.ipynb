{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f3ed5a0-3f7c-41bf-ae01-c82f0553875b",
   "metadata": {},
   "source": [
    "<a id='top'></a><a name='top'></a>\n",
    "# Chapter 19: Joining the Transformer Encoder and Decoder with Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bd2604-eb52-47dc-9128-950cbbf9b582",
   "metadata": {},
   "source": [
    "* [Introduction](#introduction)\n",
    "* [19.0 Imports and Setup](#19.0)\n",
    "* [19.1 Recap of the Transformer Architecture](#19.1)\n",
    "* [19.2 Masking](#19.2)\n",
    "* [19.3 Joining the Transformer Encoder and Decoder](#19.3)\n",
    "* [19.4 Creating an Instance of the Transformer Model](#19.4)\n",
    "* [Extra](#extra)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10b3b3f-d441-47ee-8407-028c0445cb71",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='introduction'></a><a id='introduction'></a>\n",
    "# Introduction\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "### Dataset\n",
    "\n",
    "* TODO\n",
    "\n",
    "### Explore\n",
    "* How to create a padding mask for the encoder and decoder\n",
    "* How to create a look-ahead mask for the decoder\n",
    "* How to join the transformer encoder and decoder into a single model\n",
    "* How to print out a summary for the encoder and decoder layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc36887-7910-4f6d-acad-b6d0c406d892",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='19.0'></a><a id='19.0'></a>\n",
    "# 19.0 Imports and Setup\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99b5949c-5f46-428b-b69f-f95add9579c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "req_file = \"requirements_19.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb3fc865-8cb7-4dc9-9a0a-6be508c7c94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements_19.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile {req_file}\n",
    "isort\n",
    "scikit-learn-intelex\n",
    "watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3ad3857-9426-4274-b1ad-5b2ce03ee820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "IS_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IS_COLAB:\n",
    "    print(\"Installing packages\")\n",
    "    !pip install --upgrade --quiet -r {req_file}\n",
    "else:\n",
    "    print(\"Running locally.\")\n",
    "\n",
    "# Need to import before sklearn\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "235d22d2-de94-4e15-b58e-412fd08cfdee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting imports.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile imports.py\n",
    "import locale\n",
    "import math\n",
    "import pprint\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import math, cast, float32, linalg, ones\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.activations import softmax\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.layers import LayerNormalization\n",
    "from tensorflow.keras.layers import ReLU\n",
    "from tqdm.auto import tqdm\n",
    "from watermark import watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca95133c-512d-4128-b9ec-b26c7263a342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing /Users/gb/Desktop/jb_transformer_nb/chp19_join_encoder_decoder_masking/imports.py\n",
      "import locale\n",
      "import math\n",
      "import pprint\n",
      "import warnings\n",
      "\n",
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "from tensorflow import cast\n",
      "from tensorflow import float32\n",
      "from tensorflow import linalg\n",
      "from tensorflow import math\n",
      "from tensorflow import ones\n",
      "from tensorflow.keras import Model\n",
      "from tensorflow.keras.activations import softmax\n",
      "from tensorflow.keras.layers import Dense\n",
      "from tensorflow.keras.layers import Dropout\n",
      "from tensorflow.keras.layers import Embedding\n",
      "from tensorflow.keras.layers import Layer\n",
      "from tensorflow.keras.layers import LayerNormalization\n",
      "from tensorflow.keras.layers import ReLU\n",
      "from tqdm.auto import tqdm\n",
      "from watermark import watermark\n"
     ]
    }
   ],
   "source": [
    "!isort imports.py --sl\n",
    "!cat imports.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f44469f-adae-4c91-8bc6-6d924a90131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "import math\n",
    "import pprint\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import cast\n",
    "from tensorflow import float32\n",
    "from tensorflow import linalg\n",
    "from tensorflow import math\n",
    "from tensorflow import ones\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.activations import softmax\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.layers import LayerNormalization\n",
    "from tensorflow.keras.layers import ReLU\n",
    "from tqdm.auto import tqdm\n",
    "from watermark import watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83b38539-ed0a-4066-8663-02b26b99052a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.8.12\n",
      "IPython version      : 7.34.0\n",
      "\n",
      "Compiler    : Clang 13.0.0 (clang-1300.0.29.3)\n",
      "OS          : Darwin\n",
      "Release     : 21.6.0\n",
      "Machine     : x86_64\n",
      "Processor   : i386\n",
      "CPU cores   : 4\n",
      "Architecture: 64bit\n",
      "\n",
      "numpy     : 1.23.5\n",
      "tensorflow: 2.9.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def HR():\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "def getpreferredencoding(do_setlocale = True):\n",
    "    return \"UTF-8\"\n",
    "\n",
    "locale.getpreferredencoding = getpreferredencoding\n",
    "warnings.filterwarnings('default')\n",
    "BASE_DIR = '.'\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "seed = 42\n",
    "\n",
    "print(watermark(iversions=True,globals_=globals(),python=True,machine=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a5e937-3e4c-4260-8f19-45f7d0716361",
   "metadata": {},
   "source": [
    "# Previous Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebfaa201-7fdb-4c86-8cc0-4227ee54dbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# positional_encoding.py\n",
    "class PositionEmbeddingFixedWeights(Layer):\n",
    "    def __init__(self, seq_length, vocab_size, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        word_embedding_matrix = self.get_position_encoding(vocab_size, output_dim)\n",
    "        pos_embedding_matrix = self.get_position_encoding(seq_length, output_dim)\n",
    "        self.word_embedding_layer = Embedding(\n",
    "            input_dim=vocab_size, output_dim=output_dim,\n",
    "            weights=[word_embedding_matrix],\n",
    "            trainable=False\n",
    "        )\n",
    "        self.position_embedding_layer = Embedding(\n",
    "            input_dim=seq_length, output_dim=output_dim,\n",
    "            weights=[pos_embedding_matrix],\n",
    "            trainable=False\n",
    "        )\n",
    "\n",
    "    def get_position_encoding(self, seq_len, d, n=10000):\n",
    "        P = np.zeros((seq_len, d))\n",
    "        for k in range(seq_len):\n",
    "            for i in np.arange(int(d/2)):\n",
    "                denominator = np.power(n, 2*i/d)\n",
    "                P[k, 2*i] = np.sin(k/denominator)\n",
    "                P[k, 2*i+1] = np.cos(k/denominator)\n",
    "        return P\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        position_indices = tf.range(tf.shape(inputs)[-1])\n",
    "        embedded_words = self.word_embedding_layer(inputs)\n",
    "        embedded_indices = self.position_embedding_layer(position_indices)\n",
    "        return embedded_words + embedded_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "027b9297-77f4-4f18-9363-ce2c128d9efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the Scaled-Dot Product Attention\n",
    "# multihead_attention.py\n",
    "class DotProductAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, queries, keys, values, d_k, mask=None):\n",
    "        # Scoring the queries against the keys after transposing the latter, and scaling\n",
    "        scores = matmul(queries, keys, transpose_b=True) / math.sqrt(cast(d_k, float32))\n",
    "\n",
    "        # Apply mask to the attention scores\n",
    "        if mask is not None:\n",
    "            scores += -1e9 * mask\n",
    "\n",
    "        # Computing the weights by a softmax operation\n",
    "        weights = softmax(scores)\n",
    "\n",
    "        # Computing the attention by a weighted sum of the value vectors\n",
    "        return matmul(weights, values)\n",
    "\n",
    "# Implementing the Multi-Head Attention\n",
    "class MultiHeadAttention(Layer):\n",
    "    def __init__(self, h, d_k, d_v, d_model, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.attention = DotProductAttention()  # Scaled dot product attention\n",
    "        self.heads = h  # Number of attention heads to use\n",
    "        self.d_k = d_k  # Dimensionality of the linearly projected queries and keys\n",
    "        self.d_v = d_v  # Dimensionality of the linearly projected values\n",
    "        self.d_model = d_model  # Dimensionality of the model\n",
    "        self.W_q = Dense(d_k)   # Learned projection matrix for the queries\n",
    "        self.W_k = Dense(d_k)   # Learned projection matrix for the keys\n",
    "        self.W_v = Dense(d_v)   # Learned projection matrix for the values\n",
    "        self.W_o = Dense(d_model) # Learned projection matrix for the multi-head output\n",
    "\n",
    "    def reshape_tensor(self, x, heads, flag):\n",
    "        if flag:\n",
    "            # Tensor shape after reshaping and transposing:\n",
    "            # (batch_size, heads, seq_length, -1)\n",
    "            x = reshape(x, shape=(shape(x)[0], shape(x)[1], heads, -1))\n",
    "            x = transpose(x, perm=(0, 2, 1, 3))\n",
    "        else:\n",
    "            # Reverting the reshaping and transposing operations:\n",
    "            # (batch_size, seq_length, d_k)\n",
    "            x = transpose(x, perm=(0, 2, 1, 3))\n",
    "            x = reshape(x, shape=(shape(x)[0], shape(x)[1], self.d_k))\n",
    "        return x\n",
    "\n",
    "    def call(self, queries, keys, values, mask=None):\n",
    "        # Rearrange the queries to be able to compute all heads in parallel\n",
    "        q_reshaped = self.reshape_tensor(self.W_q(queries), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        # Rearrange the keys to be able to compute all heads in parallel\n",
    "        k_reshaped = self.reshape_tensor(self.W_k(keys), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        # Rearrange the values to be able to compute all heads in parallel\n",
    "        v_reshaped = self.reshape_tensor(self.W_v(values), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        # Compute the multi-head attention output using the reshaped queries,\n",
    "        # keys, and values\n",
    "        o_reshaped = self.attention(q_reshaped, k_reshaped, v_reshaped, self.d_k, mask)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        # Rearrange back the output into concatenated form\n",
    "        output = self.reshape_tensor(o_reshaped, self.heads, False)\n",
    "        # Resulting tensor shape: (batch_size, input_seq_length, d_v)\n",
    "\n",
    "        # Apply one final linear projection to the output to generate the multi-head\n",
    "        # attention. Resulting tensor shape: (batch_size, input_seq_length, d_model)\n",
    "        return self.W_o(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bea761b-1686-48c3-9b3b-f8d26d7f3e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder.py\n",
    "# Implementing the Add & Norm Layer\n",
    "class AddNormalization(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.layer_norm = LayerNormalization()  # Layer normalization layer\n",
    "\n",
    "    def call(self, x, sublayer_x):\n",
    "        # The sublayer input and output need to be of the same shape to be summed\n",
    "        add = x + sublayer_x\n",
    "\n",
    "        # Apply layer normalization to the sum\n",
    "        return self.layer_norm(add)\n",
    "\n",
    "# Implementing the Feed-Forward Layer\n",
    "class FeedForward(Layer):\n",
    "    def __init__(self, d_ff, d_model, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.fully_connected1 = Dense(d_ff)  # First fully connected layer\n",
    "        self.fully_connected2 = Dense(d_model)  # Second fully connected layer\n",
    "        self.activation = ReLU()  # ReLU activation layer\n",
    "\n",
    "    def call(self, x):\n",
    "        # The input is passed into the two fully-connected layers, with a ReLU in between\n",
    "        x_fc1 = self.fully_connected1(x)\n",
    "\n",
    "        return self.fully_connected2(self.activation(x_fc1))\n",
    "\n",
    "# Implementing the Encoder Layer\n",
    "class EncoderLayer(Layer):\n",
    "    def __init__(self, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.multihead_attention = MultiHeadAttention(h, d_k, d_v, d_model)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.add_norm1 = AddNormalization()\n",
    "        self.feed_forward = FeedForward(d_ff, d_model)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "        self.add_norm2 = AddNormalization()\n",
    "\n",
    "    def call(self, x, padding_mask, training):\n",
    "        # Multi-head attention layer\n",
    "        multihead_output = self.multihead_attention(x, x, x, padding_mask)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Add in a dropout layer\n",
    "        multihead_output = self.dropout1(multihead_output, training=training)\n",
    "\n",
    "        # Followed by an Add & Norm layer\n",
    "        addnorm_output = self.add_norm1(x, multihead_output)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Followed by a fully connected layer\n",
    "        feedforward_output = self.feed_forward(addnorm_output)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Add in another dropout layer\n",
    "        feedforward_output = self.dropout2(feedforward_output, training=training)\n",
    "\n",
    "        # Followed by another Add & Norm layer\n",
    "        return self.add_norm2(addnorm_output, feedforward_output)\n",
    "\n",
    "# Implementing the Encoder\n",
    "class Encoder(Layer):\n",
    "    def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate,\n",
    "                       **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pos_encoding = PositionEmbeddingFixedWeights(sequence_length, vocab_size,\n",
    "                                                          d_model)\n",
    "        self.dropout = Dropout(rate)\n",
    "        self.encoder_layer = [EncoderLayer(h, d_k, d_v, d_model, d_ff, rate)\n",
    "                              for _ in range(n)]\n",
    "\n",
    "    def call(self, input_sentence, padding_mask, training):\n",
    "        # Generate the positional encoding\n",
    "        pos_encoding_output = self.pos_encoding(input_sentence)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Add in a dropout layer\n",
    "        x = self.dropout(pos_encoding_output, training=training)\n",
    "\n",
    "        # Pass on the positional encoded values to each encoder layer\n",
    "        for i, layer in enumerate(self.encoder_layer):\n",
    "            x = layer(x, padding_mask, training)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd5d7bcb-570c-4d51-9c17-9f5a79cc7b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder.py\n",
    "# Implementing the Decoder Layer\n",
    "class DecoderLayer(Layer):\n",
    "    def __init__(self, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.multihead_attention1 = MultiHeadAttention(h, d_k, d_v, d_model)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.add_norm1 = AddNormalization()\n",
    "        self.multihead_attention2 = MultiHeadAttention(h, d_k, d_v, d_model)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "        self.add_norm2 = AddNormalization()\n",
    "        self.feed_forward = FeedForward(d_ff, d_model)\n",
    "        self.dropout3 = Dropout(rate)\n",
    "        self.add_norm3 = AddNormalization()\n",
    "\n",
    "    def call(self, x, encoder_output, lookahead_mask, padding_mask, training):\n",
    "        # Multi-head attention layer\n",
    "        multihead_output1 = self.multihead_attention1(x, x, x, lookahead_mask)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Add in a dropout layer\n",
    "        multihead_output1 = self.dropout1(multihead_output1, training=training)\n",
    "\n",
    "        # Followed by an Add & Norm layer\n",
    "        addnorm_output1 = self.add_norm1(x, multihead_output1)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Followed by another multi-head attention layer\n",
    "        multihead_output2 = self.multihead_attention2(addnorm_output1, encoder_output,\n",
    "                                                      encoder_output, padding_mask)\n",
    "\n",
    "        # Add in another dropout layer\n",
    "        multihead_output2 = self.dropout2(multihead_output2, training=training)\n",
    "\n",
    "        # Followed by another Add & Norm layer\n",
    "        addnorm_output2 = self.add_norm1(addnorm_output1, multihead_output2)\n",
    "\n",
    "        # Followed by a fully connected layer\n",
    "        feedforward_output = self.feed_forward(addnorm_output2)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Add in another dropout layer\n",
    "        feedforward_output = self.dropout3(feedforward_output, training=training)\n",
    "\n",
    "        # Followed by another Add & Norm layer\n",
    "        return self.add_norm3(addnorm_output2, feedforward_output)\n",
    "\n",
    "# Implementing the Decoder\n",
    "class Decoder(Layer):\n",
    "    def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate,\n",
    "                       **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pos_encoding = PositionEmbeddingFixedWeights(sequence_length, vocab_size,\n",
    "                                                          d_model)\n",
    "        self.dropout = Dropout(rate)\n",
    "        self.decoder_layer = [DecoderLayer(h, d_k, d_v, d_model, d_ff, rate)\n",
    "                              for _ in range(n)]\n",
    "\n",
    "    def call(self, output_target, encoder_output, lookahead_mask, padding_mask, training):\n",
    "        # Generate the positional encoding\n",
    "        pos_encoding_output = self.pos_encoding(output_target)\n",
    "        # Expected output shape = (number of sentences, sequence_length, d_model)\n",
    "\n",
    "        # Add in a dropout layer\n",
    "        x = self.dropout(pos_encoding_output, training=training)\n",
    "\n",
    "        # Pass on the positional encoded values to each encoder layer\n",
    "        for i, layer in enumerate(self.decoder_layer):\n",
    "            x = layer(x, encoder_output, lookahead_mask, padding_mask, training)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a15ed6-719c-4e26-bdfb-463fe8da05d2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560a82fc-ff4f-4193-8d38-0a1302faf9ff",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='19.1'></a><a id='19.1'></a>\n",
    "# 19.1 Recap of the Transformer Architecture\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b34790-89ae-4aa7-98fd-3c2a539ced62",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='19.2'></a><a id='19.2'></a>\n",
    "# 19.2 Masking\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d22febf-1820-4931-961b-37d5ca0138ca",
   "metadata": {},
   "source": [
    "## Create a Padding Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66fc50e9-5b9c-4208-a2e0-f18648446711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a padding mask for both the encoder and decoder\n",
    "def padding_mask(input):\n",
    "    # Create mask which marks the zero padding values in the input by a 1\n",
    "    mask = math.equal(input, 0)\n",
    "    mask = cast(mask, float32)\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb9f58cf-58f4-4200-bb36-f514645d2836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0. 0. 0. 0. 1. 1. 1.], shape=(7,), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-22 13:08:10.536980: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Upon receiving an input, this function will generate a tensor that marks by a value of one\n",
    "# whever the input contains a value of zero.\n",
    "input = np.array([1,2,3,4,0,0,0])\n",
    "\n",
    "print(padding_mask(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b57b5e-6815-44ea-bafc-73b2142901e2",
   "metadata": {},
   "source": [
    "## Creating a Look-Ahead Mask for Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f08ee8a-2c53-4433-8af4-8c8aabaf9b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookahead_mask(shape):\n",
    "    num_lower = -1\n",
    "    num_upper = 0\n",
    "        \n",
    "    # Mask out future entries by marking them with a 1.0\n",
    "    mask = 1 - linalg.band_part(ones((shape, shape)), num_lower, num_upper)\n",
    "    print(mask)\n",
    "    HR()\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3dc56a81-fe3f-4e22-8dba-7fd584b3e8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]], shape=(5, 5), dtype=float32)\n",
      "----------------------------------------\n",
      "tf.Tensor(\n",
      "[[0. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]], shape=(5, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Pass the length of the decoder output\n",
    "# In this example, the length is 5\n",
    "test = lookahead_mask(5)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a52d862-983e-409f-946c-c3af2d8b26a9",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='19.3'></a><a id='19.3'></a>\n",
    "# 19.3 Joining the Transformer Encoder and Decoder\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "Create the class `TransformerModel`, which inherits from the `Model` base class in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0134600-8dbc-4513-a0a6-0ec2cb833be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize instances of the Encoder and Decoder classes, and assign outputs to the \n",
    "# variables encoder and decoder.\n",
    "class TransformerModel(Model):\n",
    "    def __init__(self, enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, \n",
    "                 h, d_k, d_v, d_model, d_ff_inner, n, rate, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        # Set up the encoder\n",
    "        self.encoder = Encoder(enc_vocab_size, enc_seq_length, h, d_k, d_v, \n",
    "                               d_model, d_ff_inner, n, rate)\n",
    "    \n",
    "        # Define the final dense layer\n",
    "        self.model_last_layer = Dense(dec_vocab_size)\n",
    "        \n",
    "\n",
    "    # Generate a padding mask for both the encoder and decoder\n",
    "    def padding_mask(input):\n",
    "        # Create mask which marks the zero padding values in the input by a 1\n",
    "        mask = math.equal(input, 0)\n",
    "        mask = cast(mask, float32)\n",
    "\n",
    "        return mask\n",
    "\n",
    "    \n",
    "    def lookahead_mask(shape):\n",
    "        num_lower = -1\n",
    "        num_upper = 0\n",
    "\n",
    "        # Mask out future entries by marking them with a 1.0\n",
    "        mask = 1 - linalg.band_part(ones((shape, shape)), num_lower, num_upper)\n",
    "        print(mask)\n",
    "        HR()\n",
    "\n",
    "        return mask\n",
    "        \n",
    "        \n",
    "        # Create the class method call() to feed the relevant inputs into the encoder and decoder.\n",
    "        def call(self, encoder_input, decoder_input, training):\n",
    "            # Create padding mask to mask the encoder inputs and outputs in the decoder\n",
    "            enc_padding_mask = self.padding_mask(encoder_input)\n",
    "            \n",
    "            # Create and combine padding and look-ahead masks to be fed into the decoder\n",
    "            dec_in_padding_mask = self.padding_mask(decoder_input)\n",
    "            dec_in_lookahead_mask = self.lookahead_mask(decoder_input.shape[1])\n",
    "            dec_in_lookahead_mask = maximum(dec_in_padding_mask, dec_in_lookahead_mask)\n",
    "            \n",
    "            # Feed the input into the encoder\n",
    "            encoder_output = self.encoder(encoder_input, enc_padding_mask, training)\n",
    "            \n",
    "            # Feed the encoder output into the decoder\n",
    "            decoder_output = self.decoder(decocer_input, encoder_output, dec_in_lookahead_mask, enc_padding_mask, training)\n",
    "            \n",
    "            # Pass the decoder output through a final dense layer\n",
    "            model_output = self.model_last_layer(decoder_output)\n",
    "            \n",
    "            return model_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac3cfae-107b-437d-88a8-92f508cca6a1",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='19.4'></a><a id='19.4'></a>\n",
    "# 19.4 Creating an Instance of the Transformer Model\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "Use the parameter values from the \"Attention is all You Need\" paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9086067b-5752-4fe2-813d-fa44fe2ca42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Hyper-parameters::\n",
    "h = 8 # Num of self-attention heads\n",
    "d_k = 64 # Dim of linearly projected queries and keys\n",
    "d_v = 64 # Dim of linearly projected values\n",
    "d_ff = 2048 # Dim of inner fully connected layers\n",
    "d_model = 512 # Dim of model sub-layer output\n",
    "n = 6 # Num of layers in the encoder stack\n",
    "dropout_rate = 0.1 # Frequency of dropping the input units in the dropout layers\n",
    "\n",
    "# Input-related parameters:\n",
    "enc_vocab_size = 20 # Vocab size for encoder\n",
    "dec_vocab_size = 20 # Vocab size for decoder\n",
    "\n",
    "enc_seq_length = 5 # Maximum length of the input sequence \n",
    "dec_seq_length = 5 # Maximum length of the target sequence\n",
    "\n",
    "\n",
    "# Create an instance of the TransformerModel class\n",
    "training_model = TransformerModel(\n",
    "    enc_vocab_size, dec_vocab_size, enc_seq_length,\n",
    "    dec_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dff4d5aa-3ae1-46d9-bbef-b3140e670868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.TransformerModel at 0x12ba04f10>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ed5120-4222-49af-a634-fa36c283c122",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

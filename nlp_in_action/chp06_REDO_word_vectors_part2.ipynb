{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74afbaa3-8c31-44c0-b6b7-63ddacf7146f",
   "metadata": {},
   "source": [
    "<a id='top'></a><a name='top'></a>\n",
    "# Chapter 6: Reasoning with word vectors (Word2vec)\n",
    "\n",
    "## Word vectors (6.2.5 - 6.2.10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b35b83a-8447-4991-a012-bbda3590e310",
   "metadata": {},
   "source": [
    "* [Introduction](#introduction)\n",
    "* [6.0 Imports and Setup](#6.0)\n",
    "* [6.2 Word vectors](#6.2)\n",
    "    - [6.2.5 Word2vec vs. GloVe (Global Vectors)](#6.2.5)\n",
    "    - [6.2.6 fastText](#6.2.6)\n",
    "    - [6.2.7 Word2Vec vs. LSA](#6.2.7)\n",
    "    - [6.2.8 Visualizing word relationships](#6.2.8)\n",
    "    - [6.2.9 Unnatural words](#6.2.9)\n",
    "    - [6.2.10 Document similarity with Doc2vec](#6.2.10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74aeaed4-b804-4f80-a6fc-ddcd97af819f",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='introduction'></a><a id='introduction'></a>\n",
    "# Introduction\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "\n",
    "### Datasets\n",
    "\n",
    "* glove.6B.zip: [script](#glove.6B.zip), [source](https://huggingface.co/stanfordnlp/glove/resolve/main/glove.6B.zip)\n",
    "\n",
    "\n",
    "### Explore\n",
    "\n",
    "* How word vectors are created\n",
    "* Using pretrained models for applications\n",
    "* Reasoning with word vectors to solve real problems\n",
    "* Visualizing word vectors\n",
    "* Uses for word embeddings\n",
    "* How every word has some geography, sentiment (positivity), and gender associated with it\n",
    "\n",
    "### Key points\n",
    "\n",
    "* Word vectors and vector-oriented reasoning can solve problems like analogy questions and non-synonomy relationships between words.\n",
    "* It is possible to train Word2vec and other word vector embeddings on words in an application so a NLP pipeline isn't \"polluted\" by the GoogleNews meaning of words inherent in most Word2vec pretrained models. \n",
    "* Gensim can be used to explore, visualize, and build word vector vocabularies.\n",
    "* A PCA projection of geographic word vectors like US city names can reveal the cultural closeness of places that are geographically far apart.\n",
    "* If you respect sentence boundaries with n-grams and are efficient at setting up word pairs for training, you can greatly improve the accuracy of your latent semantic analysis word embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee93c16-dc63-4904-9635-44a459f1c345",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='6.0'></a><a id='6.0'></a>\n",
    "# 6.0 Imports and Setup\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "062d0384-02bd-41d1-9e9a-b6f99048f8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('setup'):\n",
    "    os.mkdir('setup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d7dd0e9-182e-440b-8ff2-a8edfb2fd2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "req_file = \"setup/requirements_06.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40319867-22e3-4c54-8234-e6545afc6568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting setup/requirements_06.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile {req_file}\n",
    "annoy\n",
    "isort\n",
    "watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc10f09e-134d-4634-9fac-bda30cca0561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "IS_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IS_COLAB:\n",
    "    print(\"Installing packages\")\n",
    "    !pip install --upgrade --quiet -r {req_file}\n",
    "else:\n",
    "    print(\"Running locally.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adeed030-cdb2-4279-9b75-83156f24fcae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "# if IS_COLAB:\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd35bde2-ab5d-44e2-8783-94c2ced011a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting setup/chp06_imports.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile setup/chp06_imports.py\n",
    "import pickle\n",
    "import locale\n",
    "import pprint\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import seaborn as sns\n",
    "from annoy import AnnoyIndex\n",
    "from tqdm.auto import tqdm\n",
    "from watermark import watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c60011e-2634-4ebe-9824-ae0dfe4783fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing /Users/gb/Desktop/examples/setup/chp06_imports.py\n",
      "import locale\n",
      "import pickle\n",
      "import pprint\n",
      "import random\n",
      "import warnings\n",
      "\n",
      "import seaborn as sns\n",
      "from annoy import AnnoyIndex\n",
      "from tqdm.auto import tqdm\n",
      "from watermark import watermark\n"
     ]
    }
   ],
   "source": [
    "!isort setup/chp06_imports.py --sl\n",
    "!cat setup/chp06_imports.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7d3cf2c-a728-4a2d-8b31-7ce0c27046bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "import pickle\n",
    "import pprint\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import seaborn as sns\n",
    "from annoy import AnnoyIndex\n",
    "from tqdm.auto import tqdm\n",
    "from watermark import watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6e554ca-3c36-4cad-bba7-ade60c377bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.8.12\n",
      "IPython version      : 7.34.0\n",
      "\n",
      "Compiler    : Clang 13.0.0 (clang-1300.0.29.3)\n",
      "OS          : Darwin\n",
      "Release     : 21.6.0\n",
      "Machine     : x86_64\n",
      "Processor   : i386\n",
      "CPU cores   : 4\n",
      "Architecture: 64bit\n",
      "\n",
      "sys    : 3.8.12 (default, Dec 13 2021, 20:17:08) \n",
      "[Clang 13.0.0 (clang-1300.0.29.3)]\n",
      "seaborn: 0.12.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def HR():\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "def getpreferredencoding(do_setlocale = True):\n",
    "    return \"UTF-8\"\n",
    "\n",
    "locale.getpreferredencoding = getpreferredencoding\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style(\"darkgrid\")\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "random.seed(23)\n",
    "\n",
    "print(watermark(iversions=True,globals_=globals(),python=True,machine=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f252b5fe-c9bc-4899-a83a-8ef28eba91df",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5629f40-4840-4eeb-ba22-afd99ea88ac0",
   "metadata": {},
   "source": [
    "<a name='6.2.5'></a><a id='6.2.5'></a>\n",
    "## 6.2.5 Word2vec vs GloVe (Global Vectors)\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "Problem: How does the GloVe model differ Word2vec?\n",
    "\n",
    "Idea: Word2vec relies on backpropagation to update weights that form the word embeddings. GloVe produces matrices equivalent to the input weight matrix and output weight matrix of Word2vec, but via direct optimization of a cost function using gradient descent. GloVe achieves direct optimization of the global vectors of word co-occurrences (co-occurrences across the entire corpus). Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations can reveal interesting linear substructures of the word vector space.\n",
    "\n",
    "Importance: In general, GloVe is faster, and more likely to find the global optimum for vector representations, giving more accurate results. GloVe can also be trained on smaller corpora and still converge.\n",
    "\n",
    "Below example is from *Real World Natural Language Processing* by Masato Hagiwara.\n",
    "\n",
    "Reference: \n",
    "* https://github.com/mhagiwara/realworldnlp/blob/master/examples/embeddings/glove_lookup.py\n",
    "* https://nlp.stanford.edu/projects/glove/\n",
    "* https://www.geeksforgeeks.org/pre-trained-word-embedding-using-glove-in-nlp-models/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b088286f-3455-47e4-8c7f-f4b16ac83c12",
   "metadata": {},
   "source": [
    "<a id='glove.42B.300d.zip'></a><a name='glove.42B.300d.zip'></a>\n",
    "### Dataset: glove.42B.300d.zip\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b587ace-c9ec-48d5-bdec-d64de5164006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/data_glove/glove.42B.300d.txt\n",
      "----------------------------------------\n",
      "File ‘data/data_glove/glove.42B.300d.zip’ already there; not retrieving.\n",
      "\n",
      "----------------------------------------\n",
      "-rw-r--r--@ 1 gb  staff  1877800501 Oct 25  2015 data/data_glove/glove.42B.300d.zip\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'data/data_glove'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "    \n",
    "data_glove_path = f'{data_dir}/glove.42B.300d.txt'\n",
    "data_glove_src = f'{data_dir}/glove.42B.300d.zip'\n",
    "print(data_glove_path)\n",
    "HR()\n",
    "\n",
    "!wget -P {data_dir} -nc https://nlp.stanford.edu/data/glove.42B.300d.zip\n",
    "HR()\n",
    "!ls -l {data_glove_src}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de9dd37e-3a40-4be6-9d4f-cdef0b81bca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset project\n",
    "# !rm -fr {data_glove_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac3f9560-10c2-4f87-9473-4d494883170c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.15 ms, sys: 1.24 ms, total: 2.38 ms\n",
      "Wall time: 2.24 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os.path\n",
    "\n",
    "if not os.path.isfile(data_glove_path):\n",
    "    print(f\"{data_glove_path} not found, extracting now.\")\n",
    "    !unzip {data_glove_src} -d {data_dir}\n",
    "    print(\"Done\")\n",
    "    !ls -l {data_glove_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2bb5366-f46b-471f-ba77-924c0e9de29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "data/data_glove/glove.42B.300d.txt\n",
      "data/data_glove/glove.42B.300d.ann\n",
      "data/data_glove/glove.42B.300d.pkl\n",
      "data/data_glove/glove.42B.300d.i2w\n"
     ]
    }
   ],
   "source": [
    "GLOVE_FILE_PREFIX_TXT = f\"{data_dir}/glove.42B.300d.txt\"\n",
    "GLOVE_FILE_PREFIX_ANN = f\"{data_dir}/glove.42B.300d.ann\"  # Annoy index file\n",
    "GLOVE_FILE_PREFIX_PKL = f\"{data_dir}/glove.42B.300d.pkl\"\n",
    "GLOVE_FILE_PREFIX_I2W = f\"{data_dir}/glove.42B.300d.i2w\"\n",
    "\n",
    "HR()\n",
    "\n",
    "print(GLOVE_FILE_PREFIX_TXT)\n",
    "print(GLOVE_FILE_PREFIX_ANN)\n",
    "print(GLOVE_FILE_PREFIX_PKL)\n",
    "print(GLOVE_FILE_PREFIX_I2W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eb9f8d-5e95-4688-99ed-8bcb0d8e6c49",
   "metadata": {},
   "source": [
    "### Annoy\n",
    "\n",
    "https://github.com/spotify/annoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6f57be4-f4ee-4708-bf99-f1059c901f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Real World NLP Book\n",
    "# https://github.com/mhagiwara/realworldnlp/blob/master/examples/embeddings/glove_lookup.py\n",
    "# Using annoy-1.15.1\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "def build_index():\n",
    "    print(\"Start building index..\")\n",
    "    num_trees = 10\n",
    "\n",
    "    idx = AnnoyIndex(EMBEDDING_DIM)\n",
    "\n",
    "    index_to_word = {}\n",
    "    \n",
    "    with open(GLOVE_FILE_PREFIX_TXT) as f:\n",
    "        for i, line in enumerate(tqdm(f)):\n",
    "            fields = line.rstrip().split(' ')\n",
    "            vec = [float(x) for x in fields[1:]]\n",
    "            idx.add_item(i, vec)\n",
    "            index_to_word[i] = fields[0]\n",
    "            # if i > 100_000:\n",
    "            #     break\n",
    "\n",
    "    idx.build(num_trees)\n",
    "    idx.save(GLOVE_FILE_PREFIX_ANN)\n",
    "    pickle.dump(index_to_word, open(GLOVE_FILE_PREFIX_I2W, mode='wb'))\n",
    "    print(\"Done building index.\")\n",
    "\n",
    "    \n",
    "def search(query, top_n=10):\n",
    "    idx = AnnoyIndex(EMBEDDING_DIM)\n",
    "    idx.load(GLOVE_FILE_PREFIX_ANN)\n",
    "    index_to_word = pickle.load(open(GLOVE_FILE_PREFIX_I2W, mode='rb'))\n",
    "    word_to_index = {word: index for index, word in index_to_word.items()}\n",
    "    query_id = word_to_index[query]\n",
    "    print(query_id)\n",
    "    word_ids = idx.get_nns_by_item(query_id, top_n)\n",
    "    for word_id in word_ids:\n",
    "        print(index_to_word[word_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8cf347a-7699-450a-b04c-cbcd15f1f08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annoy index file data/data_glove/glove.42B.300d.ann already exists.\n",
      "----------------------------------------\n",
      "CPU times: user 1.26 ms, sys: 1.83 ms, total: 3.09 ms\n",
      "Wall time: 3.35 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os.path\n",
    "if not os.path.isfile(GLOVE_FILE_PREFIX_ANN):\n",
    "    print(\"Build Annoy index file.\")\n",
    "    build_index()\n",
    "else:\n",
    "    print(f\"Annoy index file {GLOVE_FILE_PREFIX_ANN} already exists.\")\n",
    "HR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8eba2043-7953-4f0d-b36e-438016089771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 29M\tdata/data_glove/glove.42B.300d.i2w\n",
      "1.7G\tdata/data_glove/glove.42B.300d.zip\n",
      "2.4G\tdata/data_glove/glove.42B.300d.ann\n",
      "4.7G\tdata/data_glove/glove.42B.300d.txt\n"
     ]
    }
   ],
   "source": [
    "!du -h {data_dir}/* | sort -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ba18987-7723-46b6-aa26-ae8508ef2514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "828\n",
      "dog\n",
      "dogs\n",
      "puppy\n",
      "cat\n",
      "cats\n",
      "puppies\n",
      "rabbit\n",
      "paws\n",
      "pig\n",
      "toy\n"
     ]
    }
   ],
   "source": [
    "search('dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b3e45c9-36f0-44ee-8056-7322214176b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "543\n",
      "december\n",
      "january\n",
      "october\n",
      "november\n",
      "september\n",
      "february\n",
      "august\n",
      "july\n",
      "april\n",
      "march\n"
     ]
    }
   ],
   "source": [
    "search('december')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27f95014-aa7e-4b6c-9fab-386fba5e3d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "821\n",
      "sun\n",
      "30\n",
      "am\n",
      "planet\n",
      "ocean\n",
      "see\n",
      "looking\n",
      "good\n",
      "full\n",
      "side\n"
     ]
    }
   ],
   "source": [
    "search('sun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2cd61fb4-fe6d-46d6-aa5d-92e6d9a3cebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1947\n",
      "snow\n",
      "winter\n",
      "snowfall\n",
      "ice\n",
      "ski\n",
      "mountain\n",
      "skiing\n",
      "weather\n",
      "riding\n",
      "sand\n"
     ]
    }
   ],
   "source": [
    "search('snow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92933134-c10c-4568-a53f-80026ede9d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "543\n",
      "december\n",
      "january\n",
      "october\n",
      "november\n",
      "september\n",
      "february\n",
      "august\n",
      "july\n",
      "april\n",
      "march\n"
     ]
    }
   ],
   "source": [
    "search('december')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03296b44-1f2f-4e1e-b0b1-35083e4fb29b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

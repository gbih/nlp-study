{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87a021c9-609a-430a-a8d2-c960be377773",
   "metadata": {},
   "source": [
    "<a id='top'></a><a name='top'></a>\n",
    "# Chapter 3: Math with words (TF-IDF vectors)\n",
    "\n",
    "**Transforming text data from discrete integers to continuous vector representations.**\n",
    "\n",
    "We want to know which words are more important to a particular document and across the corpus as a whole. Then you can use the \"importance\" value to find relevant documents in a corpus based on keyword importance within each document.\n",
    "\n",
    "We turn the words of chapter 2 into continuous numbers rather than just integers representing word counts or binary \"bit vectors\" that detect the presence or absence of particular words. With representations of words in a continuous space, you can operate on their representation with more exciting math.\n",
    "\n",
    "In this chapter, we look at three increasingly powerful ways to represent words and their importance in a document:\n",
    "\n",
    "* Bags of words — Vectors of word counts or frequencies\n",
    "* Bags of n-grams — Counts of word pairs (bigrams), triplets (trigrams), and so on\n",
    "* TF-IDF vectors — Word scores that better represent their importance\n",
    "\n",
    "\n",
    "These are all statistical models in that they are frequency based. They search for linear patterns in word relationships. These \"shallow\" NLP machines are powerful and useful for many practical applications such as spam filtering and sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97c6b04-4dd0-4449-bbc3-b1b9a1a8b3a6",
   "metadata": {},
   "source": [
    "* [Introduction](#introduction)\n",
    "* [3.0 Imports and Setup](#3.0)\n",
    "* [3.1 Bag of words](#3.1)\n",
    "* [3.2 Vectorizing](#3.2)\n",
    "     - [3.2.1 Vector spaces](#3.2.1)\n",
    "* [3.3 Zipf's Law](#3.3)\n",
    "* [3.4 Topic modeling](#3.4)\n",
    "    - [3.4.1 Return of Zipf](#3.4.1)\n",
    "    - [3.4.2 Relevance ranking](#3.4.2)\n",
    "    - [3.4.3 Tools](#3.4.3)\n",
    "    - [3.4.4 Alternatives](#3.4.4)\n",
    "    - [3.4.5 Okapi BM25](#3.4.5)\n",
    "    - [3.4.6 What's next](#3.4.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8028dbf7-5357-46ab-b883-7fdceef6641c",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='introduction'></a><a id='introduction'></a>\n",
    "# Introduction\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "### Dataset\n",
    "\n",
    "* Wikipedia Kite Article: [script](#kite.html), [source](https://en.wikipedia.org/wiki/Kite)\n",
    "\n",
    "### Explore\n",
    "\n",
    "* Counting words and term frequencies to analyze meaning.\n",
    "* Predicting word occurrence probabilities with Zipf's Law.\n",
    "* Vector representation of words and how to start using them.\n",
    "* Finding relevant documents from a corpus using inverse document frequencies.\n",
    "* Estimating the similarity of pairs of documents with cosine similarity and Okapi BM25\n",
    "\n",
    "### Key points\n",
    "\n",
    "* Any web-scale search engine with millisecond response times has the power of a TF-IDF term document matrix hidden under the hood.\n",
    "* Term frequencies must be weighted by their inverse document frequency to ensure the most important, most meaningful words are given the heft they deserve.\n",
    "* Zipf's law can help you predict the frequencies of all sorts of things, including words, characters, and people.\n",
    "* The rows of a TF-IDF term document matrix can be used as a vector representation of the meanings of those individual words to create a vector space model of word semantics.\n",
    "* Euclidean distance and similarity between pairs of high dimensional vectors doesn't adequately represent their similarity for most NLP applications.\n",
    "* Cosine distance, the amount of \"overlap\" between vectors, can be calculated efficiently by just multiplying the elements of normalized vectors together and summing up those products.\n",
    "* Cosine distance is the go-to similarity score for most natural language vector representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df4eb6c-9b67-4ea7-ac5b-939827f51270",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='3.0'></a><a id='3.0'></a>\n",
    "# 3.0 Imports and Setup\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "177bb367-9f9b-4e82-a472-62affd5d6a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('setup'):\n",
    "    os.mkdir('setup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d572c55e-c15d-4c78-916c-f6a39ea1f572",
   "metadata": {},
   "outputs": [],
   "source": [
    "req_file = \"setup/requirements_03.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe708996-ab15-4d5d-b646-09d442784ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting setup/requirements_03.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile {req_file}\n",
    "isort\n",
    "scikit-learn-intelex\n",
    "scrapy\n",
    "vaderSentiment\n",
    "watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "096a0de9-b5a0-4709-8cc3-1396eda36908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "IS_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IS_COLAB:\n",
    "    print(\"Installing packages\")\n",
    "    !pip install --upgrade --quiet -r {req_file}\n",
    "else:\n",
    "    print(\"Running locally.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5221075-64ca-4a75-b52f-b0a175324b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "# if IS_COLAB:\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b6212cb0-81a3-4c6a-8b0f-38fb84dc4d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting setup/chp03_imports.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile setup/chp03_imports.py\n",
    "import copy\n",
    "import locale\n",
    "import math\n",
    "import pprint\n",
    "import random\n",
    "import re\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import requests\n",
    "import seaborn as sns\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm.auto import tqdm\n",
    "from watermark import watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdb19e58-a391-48b1-a639-a0b6e1c3f3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing /Users/gb/Desktop/examples/setup/chp03_imports.py\n",
      "import copy\n",
      "import locale\n",
      "import math\n",
      "import pprint\n",
      "import random\n",
      "import re\n",
      "import warnings\n",
      "from collections import Counter\n",
      "from collections import OrderedDict\n",
      "\n",
      "import nltk\n",
      "import pandas as pd\n",
      "import requests\n",
      "import seaborn as sns\n",
      "from bs4 import BeautifulSoup\n",
      "from nltk.corpus import brown\n",
      "from nltk.tokenize import TreebankWordTokenizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from tqdm.auto import tqdm\n",
      "from watermark import watermark\n"
     ]
    }
   ],
   "source": [
    "!isort setup/chp03_imports.py --sl\n",
    "!cat setup/chp03_imports.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0408a77c-8e82-4dd5-bfdf-00c11e76a423",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import locale\n",
    "import math\n",
    "import pprint\n",
    "import random\n",
    "import re\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import requests\n",
    "import seaborn as sns\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm.auto import tqdm\n",
    "from watermark import watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36c90e0a-853a-49fc-a8d1-f31e10ebf544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.8.12\n",
      "IPython version      : 7.34.0\n",
      "\n",
      "Compiler    : Clang 13.0.0 (clang-1300.0.29.3)\n",
      "OS          : Darwin\n",
      "Release     : 21.6.0\n",
      "Machine     : x86_64\n",
      "Processor   : i386\n",
      "CPU cores   : 4\n",
      "Architecture: 64bit\n",
      "\n",
      "sys     : 3.8.12 (default, Dec 13 2021, 20:17:08) \n",
      "[Clang 13.0.0 (clang-1300.0.29.3)]\n",
      "nltk    : 3.8\n",
      "seaborn : 0.12.1\n",
      "re      : 2.2.1\n",
      "requests: 2.28.1\n",
      "pandas  : 1.5.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def HR():\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "def getpreferredencoding(do_setlocale = True):\n",
    "    return \"UTF-8\"\n",
    "\n",
    "locale.getpreferredencoding = getpreferredencoding\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style(\"darkgrid\")\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "random.seed(23)\n",
    "\n",
    "print(watermark(iversions=True,globals_=globals(),python=True,machine=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d055c477-de8e-48a2-a117-cdc373c5610b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/gb/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7641e8a-7fa2-4d31-8c85-e3b531c83efe",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='3.1'></a><a id='3.1'></a>\n",
    "# 3.1 Bag of Words\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "Problem: We have a use case where we can disregard grammar and word order, and focus mainly on multiplicity (set properties). \n",
    "\n",
    "Idea: Use a *Bag of words* model to represent words and their importance in a document. This creates vectors of word counts of frequences. Bag of words is useful mainly as a method of document classification where the occurrence of each word is used as a feature for training a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "785e50a5-713b-49db-bcd6-b27f2304c697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'faster',\n",
       " 'harry',\n",
       " 'got',\n",
       " 'to',\n",
       " 'the',\n",
       " 'store',\n",
       " ',',\n",
       " 'the',\n",
       " 'faster',\n",
       " 'harry',\n",
       " ',',\n",
       " 'the',\n",
       " 'faster',\n",
       " ',',\n",
       " 'would',\n",
       " 'get',\n",
       " 'home',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"\"\"The faster Harry got to the store, the faster Harry, the faster, would get home.\"\"\"\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(sentence.lower())\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f73e65-1860-44d4-bad7-0861b2395573",
   "metadata": {},
   "source": [
    "---\n",
    "Get the unique words from the document and their counts. Because you want to count the words as well, we will use `Counter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70e68733-bd53-4001-bf77-8a28fad220bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'the': 4,\n",
       "         'faster': 3,\n",
       "         'harry': 2,\n",
       "         'got': 1,\n",
       "         'to': 1,\n",
       "         'store': 1,\n",
       "         ',': 3,\n",
       "         'would': 1,\n",
       "         'get': 1,\n",
       "         'home': 1,\n",
       "         '.': 1})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dict subclass for counting hashable items.\n",
    "bag_of_words = Counter(tokens)\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe366fce-d4a2-432c-81bb-135b255689f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 4)\n",
      "('faster', 3)\n",
      "(',', 3)\n",
      "('harry', 2)\n"
     ]
    }
   ],
   "source": [
    "for x in bag_of_words.most_common(4):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26e12e0-b88c-439e-af4a-834c9670a351",
   "metadata": {},
   "source": [
    "---\n",
    "The number of times a word occurs in a given document is called the *term frequency*, commonly abbreviated TF. \n",
    "\n",
    "Calculate the *term frequency* of \"harry\" from the Counter object (bag_of_words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31dc2ab2-4978-4388-8120-641cca6f565d",
   "metadata": {},
   "outputs": [],
   "source": [
    "times_harry_appears = bag_of_words['harry']\n",
    "\n",
    "# Number of unique tokens from the original source\n",
    "num_unique_words = len(bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d21426b-2226-471c-a41b-6ed6e64209c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1818"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = times_harry_appears / num_unique_words\n",
    "round(tf, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf4f85d-64de-47f1-9f53-7e720505a073",
   "metadata": {},
   "source": [
    "### Dataset: Wikipedia kites article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac3d5b45-40c3-4c0b-abcb-b1eaeeece1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_kite = 'data/data_kite'\n",
    "if not os.path.exists(data_kite):\n",
    "    os.makedirs(data_kite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42471a9a-7f93-4676-987f-9f594d079fe6",
   "metadata": {},
   "source": [
    "<a id='kite.html'></a><a name='kite.html'></a>\n",
    "### Dataset: kite.html\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f4cc4ad-413e-4522-b555-073a3272faf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kite.html\n"
     ]
    }
   ],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/Kite'\n",
    "\n",
    "file = url.split(\"/\")[-1] + \".html\"\n",
    "print(file)\n",
    "\n",
    "r = requests.get(url)\n",
    "\n",
    "with open(f\"{data_kite}/{file}\", \"w+\") as f:\n",
    "    f.write(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3cc94ede-1c1d-41cf-8076-d3de9e648c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kite - Wikipedia\n"
     ]
    }
   ],
   "source": [
    "with open(f\"{data_kite}/{file}\", \"r\") as f:\n",
    "    html = f.read()\n",
    "    g = re.search(r'<title>(.*)</title>', html, re.MULTILINE|re.DOTALL)\n",
    "    if g:\n",
    "        print(g.groups()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f23932ff-b354-4112-9224-35b4db215003",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "kite_text_online = soup.select_one(\"div.mw-parser-output\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75c667a6-4bad-4602-ac06-7a3d9b896727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Tethered aircraft\\nFor other uses, see Kite (disambiguation).\\n The Yokaichi Giant Kite Festival is held every July in Higashiomi, Shiga, Japan.[1]  Various kites being flown\\n Star-shaped kite above a meadow south of Hockenheim. This sparless, ram-air inflated kite, has a complex bridle formed of many strings attached to the face of the wing.\\nA kite is a tethered heavier-than-air or lighter-than-air craft with wing surfaces that react against the air to create lift and drag forces.[2] A kite consists of wings, tethers and anchors. Kites often have a bridle and tail to guide the face of the kite so the wind can lift it.[3] Some kite designs don't need a bridle; box kites can have a single attachment point. A kite may have fixed or moving anchors that can balance the kite. The name is derived from the kite, the hovering bird of prey.[4]\\nThe lift that sustains the kite in flight is generated when air moves around the kite's surface, producing low pressure above and high pressure below the wings.[5] The interaction with the wind also generates horizontal drag along the direction of the wind. The resultant force vector from the lift and drag force components is opposed by the tension of one or more of the lines or tethers to which the kite is attached.[6] The anchor point of the kite line may be static or moving (e.g., the towing of a kite by a running person, boat, free-falling anchors as in paragliders and fugitive parakites[7][8] or vehicle).[9][10]\\nThe same principles of fluid f\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kite_text_online[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37391673-1be4-4179-811d-3adc557eee38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_online length: 6721\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(',', 312),\n",
       " ('the', 296),\n",
       " ('kite', 234),\n",
       " ('in', 150),\n",
       " ('and', 150),\n",
       " ('of', 144),\n",
       " ('a', 128),\n",
       " ('kites', 124),\n",
       " ('to', 93),\n",
       " ('[', 87)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_online = TreebankWordTokenizer()\n",
    "tokens_online = tokenizer.tokenize(kite_text_online.lower())\n",
    "print(f\"tokens_online length: {len(tokens_online)}\")\n",
    "HR()\n",
    "\n",
    "token_counts_online = Counter(tokens_online)\n",
    "token_counts_online.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a5f90a-97c1-4238-a5bc-183c64763b65",
   "metadata": {},
   "source": [
    "### Use a cleaned up article version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77adb985-2c4b-427a-aaf1-b26194290557",
   "metadata": {},
   "outputs": [],
   "source": [
    "kite_text = \"\"\"A kite is traditionally a tethered heavier-than-air craft with wing surfaces that react against the air to create lift and drag. A kite consists of wings, tethers, and anchors. Kites often have a bridle to guide the face of the kite at the correct angle so the wind can lift it. A kite’s wing also may be so designed so a bridle is not needed; when kiting a sailplane for launch, the tether meets the wing at a single point. A kite may have fixed or moving anchors. Untraditionally in technical kiting, a kite consists of tether-set-coupled wing sets; even in technical kiting, though, a wing in the system is still often called the kite.\n",
    "The lift that sustains the kite in flight is generated when air flows around the kite’s surface, producing low pressure above and high pressure below the wings. The interaction with the wind also generates horizontal drag along the direction of the wind. The resultant force vector from the lift and drag force components is opposed by the tension of one or more of the lines or tethers to which the kite is attached. The anchor point of the kite line may be static or moving (such as the towing of a kite by a running person, boat, free-falling anchors as in paragliders and fugitive parakites or vehicle).\n",
    "The same principles of fluid flow apply in liquids and kites are also used under water.\n",
    "A hybrid tethered craft comprising both a lighter-than-air balloon as well as a kite lifting surface is called a kytoon.\n",
    "Kites have a long and varied history and many different types are flown individually and at festivals worldwide. Kites may be flown for recreation, art or other practical uses. Sport kites can be flown in aerial ballet, sometimes as part of a competition. Power kites are multi-line steerable kites designed to generate large forces which can be used to power activities such as kite surfing, kite landboarding, kite fishing, kite buggying and a new trend snow kiting. Even Man-lifting kites have been made.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17f74d3d-76b6-49ee-a0fe-7a8a90336df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens length: 361\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the', 26),\n",
       " ('a', 20),\n",
       " ('kite', 14),\n",
       " (',', 14),\n",
       " ('and', 10),\n",
       " ('of', 10),\n",
       " ('kites', 8),\n",
       " ('is', 7),\n",
       " ('in', 7),\n",
       " ('or', 6)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(kite_text.lower())\n",
    "print(f\"tokens length: {len(tokens)}\")\n",
    "HR()\n",
    "\n",
    "token_counts = Counter(tokens)\n",
    "token_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd00e52e-2afb-4e14-8875-b778765d29a1",
   "metadata": {},
   "source": [
    "### Reuse var `tokens`, but eliminate the stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74dd97d7-be3d-486b-8321-d183204a260f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens length: 218\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('kite', 14),\n",
       " (',', 14),\n",
       " ('kites', 8),\n",
       " ('wing', 5),\n",
       " ('lift', 4),\n",
       " ('may', 4),\n",
       " ('also', 3),\n",
       " ('kiting', 3),\n",
       " ('flown', 3),\n",
       " ('tethered', 2)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "tokens = [x for x in tokens if x not in stopwords]\n",
    "print(f\"tokens length: {len(tokens)}\")\n",
    "HR()\n",
    "\n",
    "kite_counts = Counter(tokens)\n",
    "\n",
    "kite_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c510af13-e61d-43b6-9f66-69d227fa096f",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='3.2'></a><a id='3.2'></a>\n",
    "# 3.2 Vectorizing\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "Problem: Discrete data structures such as a frequency dictionary are limited in what they can represent mathematically and conceptually.\n",
    "\n",
    "Idea: Transform discrete representations into continuous vector representations. This enables additional operations such as direction, similarity, difference. This is more graceful way to represent these numbers for mathematical intents.\n",
    "\n",
    "Here, instead of describing a document in terms of a frequency dictionary, we'll make a vector of those word counts. In Python, this will be a list. This list, or vector, is something you can do math on directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ca3dc36-e46e-454a-b108-1751176e6392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens length: 218\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06422018348623854,\n",
       " 0.06422018348623854,\n",
       " 0.03669724770642202,\n",
       " 0.022935779816513763,\n",
       " 0.01834862385321101]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_vector = []\n",
    "doc_length = len(tokens)\n",
    "print(f\"tokens length: {doc_length}\")\n",
    "HR()\n",
    "\n",
    "for key, value in kite_counts.most_common():\n",
    "    document_vector.append(value / doc_length)\n",
    "    \n",
    "document_vector[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ad9047-afbe-44bb-b4f2-4730c1b17327",
   "metadata": {},
   "source": [
    "Having one vector for one document isn’t enough. You can grab a couple more documents and make vectors for each of them as well. But the values within each vector need to be relative to something consistent across all the vectors. \n",
    "\n",
    "If you’re going to do math on them, they need to represent a position in a common space, relative to something consistent. \n",
    "\n",
    "Your vectors need to have the **same origin** and share the same scale, or “units,” on each \n",
    "of their dimensions. \n",
    "\n",
    "The first step in this process is to normalize the counts by calculating normalized term frequency instead of raw count in the document (as you did in the last section); the second step is to make all the vectors of standard length or dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353e19a8-f848-43f9-8a73-454712cfc631",
   "metadata": {},
   "source": [
    "---\n",
    "Create a small lexicon from more vectorized corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c76d4c07-8a4d-499e-a0cd-f1396c3781d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The faster Harry got to the store, the faster and faster Harry would get home.',\n",
       " 'Harry is hairy and faster than Jill.',\n",
       " 'Jill is not as hairy as Harry.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [\"The faster Harry got to the store, the faster and faster Harry would get home.\"]\n",
    "docs.append(\"Harry is hairy and faster than Jill.\")\n",
    "docs.append(\"Jill is not as hairy as Harry.\")\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6f1e33-dcdc-4ef5-8305-eb8efb33790d",
   "metadata": {},
   "source": [
    "---\n",
    "Examine the lexicon for this corpus containing three documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ed53906-62d0-46a9-a8e2-5f427d1ac029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_tokens = []\n",
    "for doc in docs:\n",
    "    doc_tokens += [sorted(tokenizer.tokenize(doc.lower()))]\n",
    "    \n",
    "len(doc_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8de9dccf-819d-44a2-9da6-10f6b10b47fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_doc_tokens = sum(doc_tokens, [])\n",
    "len(all_doc_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21c5b214-8e0a-417a-9e92-5f030ed0a357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon = sorted(set(all_doc_tokens))\n",
    "len(lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f97af8-301f-4fa0-a9b4-d107b161e7f9",
   "metadata": {},
   "source": [
    "Each of your three document vectors will need to have 18 values, even if the docu- ment for that vector doesn’t contain all 18 words in your lexicon. Each token is assigned a \"slot\" in your vectors corresponding to its position in your lexicon. Some of those token counts in the vector will be zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3c70f7d-e331-4c9b-86c5-35b996b86069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(',', 0),\n",
       "             ('.', 0),\n",
       "             ('and', 0),\n",
       "             ('as', 0),\n",
       "             ('faster', 0),\n",
       "             ('get', 0),\n",
       "             ('got', 0),\n",
       "             ('hairy', 0),\n",
       "             ('harry', 0),\n",
       "             ('home', 0),\n",
       "             ('is', 0),\n",
       "             ('jill', 0),\n",
       "             ('not', 0),\n",
       "             ('store', 0),\n",
       "             ('than', 0),\n",
       "             ('the', 0),\n",
       "             ('to', 0),\n",
       "             ('would', 0)])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_vector = OrderedDict((token, 0) for token in lexicon)\n",
    "zero_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c08139-2d44-4801-bb6b-3cd0778cd4d4",
   "metadata": {},
   "source": [
    "---\n",
    "Make copies of that base vector, update the values of the vector for each document, and store them in an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fcf43c18-b7fb-4c89-9cd2-fbcb5e09e969",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vectors = []\n",
    "\n",
    "for doc in docs:\n",
    "    vec = copy.copy(zero_vector)\n",
    "    tokens = tokenizer.tokenize(doc.lower())\n",
    "    token_counts = Counter(tokens)\n",
    "    for key, value in token_counts.items():\n",
    "        vec[key] = value / len(lexicon)\n",
    "    doc_vectors.append(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bf29fa-3250-4f1a-8a9a-e1f7deae01a2",
   "metadata": {},
   "source": [
    "We have three vectors, one for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5ed36c4-6067-469d-9595-382b330a319a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([   (',', 0.05555555555555555),\n",
      "                ('.', 0.05555555555555555),\n",
      "                ('and', 0.05555555555555555),\n",
      "                ('as', 0),\n",
      "                ('faster', 0.16666666666666666),\n",
      "                ('get', 0.05555555555555555),\n",
      "                ('got', 0.05555555555555555),\n",
      "                ('hairy', 0),\n",
      "                ('harry', 0.1111111111111111),\n",
      "                ('home', 0.05555555555555555),\n",
      "                ('is', 0),\n",
      "                ('jill', 0),\n",
      "                ('not', 0),\n",
      "                ('store', 0.05555555555555555),\n",
      "                ('than', 0),\n",
      "                ('the', 0.16666666666666666),\n",
      "                ('to', 0.05555555555555555),\n",
      "                ('would', 0.05555555555555555)])\n",
      "----------------------------------------\n",
      "OrderedDict([   (',', 0),\n",
      "                ('.', 0.05555555555555555),\n",
      "                ('and', 0.05555555555555555),\n",
      "                ('as', 0),\n",
      "                ('faster', 0.05555555555555555),\n",
      "                ('get', 0),\n",
      "                ('got', 0),\n",
      "                ('hairy', 0.05555555555555555),\n",
      "                ('harry', 0.05555555555555555),\n",
      "                ('home', 0),\n",
      "                ('is', 0.05555555555555555),\n",
      "                ('jill', 0.05555555555555555),\n",
      "                ('not', 0),\n",
      "                ('store', 0),\n",
      "                ('than', 0.05555555555555555),\n",
      "                ('the', 0),\n",
      "                ('to', 0),\n",
      "                ('would', 0)])\n",
      "----------------------------------------\n",
      "OrderedDict([   (',', 0),\n",
      "                ('.', 0.05555555555555555),\n",
      "                ('and', 0),\n",
      "                ('as', 0.1111111111111111),\n",
      "                ('faster', 0),\n",
      "                ('get', 0),\n",
      "                ('got', 0),\n",
      "                ('hairy', 0.05555555555555555),\n",
      "                ('harry', 0.05555555555555555),\n",
      "                ('home', 0),\n",
      "                ('is', 0.05555555555555555),\n",
      "                ('jill', 0.05555555555555555),\n",
      "                ('not', 0.05555555555555555),\n",
      "                ('store', 0),\n",
      "                ('than', 0),\n",
      "                ('the', 0),\n",
      "                ('to', 0),\n",
      "                ('would', 0)])\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for x in doc_vectors:\n",
    "    pp.pprint(x)\n",
    "    HR()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb378ca6-6b36-40b9-9700-07d90d8c386e",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='3.2.1'></a><a id='3.2.1'></a>\n",
    "## 3.2.1 Vector spaces\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "Problem: We need a way to generalize vectors, to allow modeling quantities such as forces and velocity. These have not only magnitude, but also direction.\n",
    "\n",
    "Idea: Use the concept of **vector spaces**, which is fundamental for linear algebra. Next, the concept of the **matrix** enables computing in vector spaces. This provides a concise way of manipulating and studying system of linear equations. Vector spaces are characterized by their dimension, which specifies the number of independent directions in the space. Two vectors are \"similar\" if they share similar directions. \n",
    "\n",
    "Vectors are the primary building blocks of linear algebra, or vector algebra. They’re an ordered list of numbers, or coordinates, in a vector space. They describe a location or position in that space. Or they can be used to identify a particular direction and magnitude or distance in that space. A space is the collection of all possible vectors that could appear in that space. So a vector with two values would lie in a 2D vector space, a vector with three values in 3D vector space, and so on.\n",
    "\n",
    "**For a natural language document vector space, the dimensionality of your vector space is the count of the number of distinct words that appear in the entire corpus.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9386bd-577c-49b2-b9cd-247450a0ad59",
   "metadata": {},
   "source": [
    "---\n",
    "Next, compute cosine similarity in python.\n",
    "\n",
    "This gives a value for how much the vectors point in the same direction.\n",
    "\n",
    "A cosine similarity of 1 represents identical normalized vectors that point in exactly the same direction along all dimensions. The vectors may have different lengths or magnitudes, but they point in the same direction. \n",
    "\n",
    "The closer a cosine similarity value is to 1, the closer the two vectors are in angle. For NLP document vectors that have a cosine similarity close to 1, you know that the documents are using similar words in similar proportion. So the documents whose document vectors are close to each other are likely talking about the same thing.\n",
    "\n",
    "A cosine similarity of 0 represents two vectors that share no components. They are orthogonal, perpendicular in all dimensions. For NLP TF vectors, this situation occurs only if the two documents share no words in common. Because these documents use completely different words, they must be talking about completely different things. This doesn’t necessarily mean they have different meanings or topics, just that they use completely different words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "442ee587-2fe8-4a7d-9604-b1e231b6deb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(vec1, vec2):\n",
    "    \"\"\" Let's convert our dictionaries to lists for easier matching.\"\"\"\n",
    "    vec1 = [val for val in vec1.values()]\n",
    "    vec2 = [val for val in vec2.values()]\n",
    "\n",
    "    dot_prod = 0\n",
    "    for i, v in enumerate(vec1):\n",
    "        dot_prod += v * vec2[i]\n",
    "        \n",
    "    mag_1 = math.sqrt(sum([x**2 for x in vec1]))\n",
    "    mag_2 = math.sqrt(sum([x**2 for x in vec2]))\n",
    "\n",
    "    return dot_prod / (mag_1 * mag_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b81b51-1d1a-4ccf-8be5-c9d687674b18",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='3.3'></a><a id='3.3'></a>\n",
    "# 3.3 Zipf's Law\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "Problem: How to represent a relation between rank order and frequency of occurrence?\n",
    "\n",
    "Idea: *Zipf's Law* is a commonly used power-distribution representation. It states that when observations (such as words) are ranked by their frequency, the frequency of a particular observation is inversely proportional to its rank, `Frequency ∝ 1 Rank`. *Zipf's Law* is used to model the size or ranks of randomly chosen objects from certain population types. In general, it's used to show the relative popularity of a small subset of a population. \n",
    "\n",
    "Importance: *Zipf's Law* and other power-law probability distributions can represent many commonly occurring patterns, in various fields ranging from the natural world to linguistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d0df8f7c-910c-43cc-8bd7-e16b3182feff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/gb/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1161192\n"
     ]
    }
   ],
   "source": [
    "nltk.download('brown')\n",
    "\n",
    "# words is a builtin method of the nltk corpus object that gives \n",
    "# a list of tokens\n",
    "print(len(brown.words()))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b94fb553-0b55-405b-9b31-ecdaaadb3935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Fulton',\n",
       " 'County',\n",
       " 'Grand',\n",
       " 'Jury',\n",
       " 'said',\n",
       " 'Friday',\n",
       " 'an',\n",
       " 'investigation',\n",
       " 'of']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5a1c379d-c6bc-4e89-a854-34bf327236a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'),\n",
       " ('Fulton', 'NP-TL'),\n",
       " ('County', 'NN-TL'),\n",
       " ('Grand', 'JJ-TL'),\n",
       " ('Jury', 'NN-TL')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part-of-speech tagging\n",
    "brown.tagged_words()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9116863c-7213-42b4-8025-bb38f1483405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('as', 2),\n",
       " ('jill', 1),\n",
       " ('is', 1),\n",
       " ('not', 1),\n",
       " ('hairy', 1),\n",
       " ('harry', 1),\n",
       " ('.', 1)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "puncs = [',', '.', '--', '-', '!', '?', ':', ';', '``', \"''\", '(', ')', '[', ']']\n",
    "word_list = [x.lower() for x in brown.words() if x not in puncs]\n",
    "print(len(token_counts))\n",
    "token_counts.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b3969a93-29e8-4318-953f-70603d479d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jill</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>not</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>as</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hairy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  count\n",
       "0   jill      1\n",
       "1     is      1\n",
       "2    not      1\n",
       "3     as      2\n",
       "4  hairy      1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Zipf's Law\n",
    "test = pd.DataFrame.from_dict(token_counts, orient='index', columns=['count']).reset_index()\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0d0e5d67-a375-42a8-8e4e-b39f6d6075bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7 entries, 0 to 6\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   index   7 non-null      object\n",
      " 1   count   7 non-null      int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 240.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbe706e-0c22-4214-ab37-05d9e9399e8d",
   "metadata": {},
   "source": [
    "The word frequencies in the Brown corpus follow the logarithmic relationship Zipf predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f5d84140-e75e-43ff-b4ba-f3cc16b758d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78910f1-b27f-4f21-8f89-f24d8b1c367b",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='3.4'></a><a id='3.4'></a>\n",
    "# 3.4 Topic modeling\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "Probem: We have a clustering problem, where given a corpus, we want to group together the documents which are topically similar. \n",
    "\n",
    "Idea: We can accomplish this in an unsupervised manner via first creating TF-IDF vectors, representing the document over the global vocabulary. Then we use a clustering method over the vector representations of the documents, such as K-means,  hierarchical clustering, etc. \n",
    "\n",
    "Importance: We can uncover the latent semantic structure of a corpus in an unsupervised manner, which is very useful. This doesn't require labeling nor training, so it can be a quick way to start analyzing data. Topic models were originally developed as a text-mining tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fbd01863-6d9d-40e9-b913-45e38eced32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0e57811a-94a1-4e6c-b972-78502d9bda7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "363"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kite_text = \"\"\"A kite is traditionally a tethered heavier-than-air craft with wing \n",
    "surfaces that react against the air to create lift and drag. A kite consists of \n",
    "wings, tethers, and anchors. Kites often have a bridle to guide the face of the \n",
    "kite at the correct angle so the wind can lift it. A kite's wing also may be so \n",
    "designed so a bridle is not needed; when kiting a sailplane for launch, the tether \n",
    "meets the wing at a single point. A kite may have fixed or moving anchors. \n",
    "Untraditionally in technical kiting, a kite consists of tether-set-coupled wing sets; \n",
    "even in technical kiting, though, a wing in the system is still often called \n",
    "the kite. The lift that sustains the kite in flight is generated when air flows \n",
    "around the kite's surface, producing low pressure above and high pressure below \n",
    "the wings. The interaction with the wind also generates horizontal drag along the \n",
    "direction of the wind. The resultant force vector from the lift and drag force \n",
    "components is opposed by the tension of one or more of the lines or tethers to \n",
    "which the kite is attached. The anchor point of the kite line may be static or \n",
    "moving (e.g., the towing of a kite by a running person, boat, free-falling anchors \n",
    "as in paragliders and fugitive parakites or vehicle). The same principles of fluid \n",
    "flow apply in liquids and kites are also used under water. A hybrid tethered craft \n",
    "comprising both a lighter-than-air balloon as well as a kite lifting surface is \n",
    "called a kytoon. Kites have a long and varied history and many different types \n",
    "are flown individually and at festivals worldwide. Kites may be flown for \n",
    "recreation, art or other practical uses. Sport kites can be flown in aerial ballet, \n",
    "sometimes as part of a competition. Power kites are multi-line steerable kites \n",
    "designed to generate large forces which can be used to power activities such as \n",
    "kite surfing, kite landboarding, kite fishing, kite buggying and a new trend \n",
    "snow kiting. Even Man-lifting kites have been made.\"\"\"\n",
    "\n",
    "kite_text = kite_text.lower()\n",
    "kite_text_tokens = tokenizer.tokenize(kite_text)\n",
    "kite_text_tokens_total = len(kite_text_tokens)\n",
    "kite_text_tokens_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdce3fc-8fb6-4d04-a8cc-9798ab5ed6ca",
   "metadata": {},
   "source": [
    "Get the total word count for each document in the corpus, intro_doc and history_doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fefcdd7b-4f2d-4b4a-bad2-5bc163b495b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "297"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kite_history = \"\"\"Kites were invented in China, where materials ideal for kite \n",
    "building were readily available: silk fabric for sail material; fine, \n",
    "high-tensile-strength silk for flying line; and resilient bamboo for a \n",
    "strong, lightweight framework. The kite has been claimed as the invention \n",
    "of the 5th-century BC Chinese philosophers Mozi (also Mo Di) and Lu Ban \n",
    "(also Gongshu Ban). By 549 AD paper kites were certainly being flown, \n",
    "as it was recorded that in that year a paper kite was used as a message \n",
    "for a rescue mission. Ancient and medieval Chinese sources describe kites \n",
    "being used for measuring distances, testing the wind, lifting men, signaling, \n",
    "and communication for military operations. The earliest known Chinese kites \n",
    "were flat (not bowed) and often rectangular. Later, tailless kites incorporated \n",
    "a stabilizing bowline. Kites were decorated with mythological motifs and \n",
    "legendary figures; some were fitted with strings and whistles to make musical \n",
    "sounds while flying. From China, kites were introduced to Cambodia, Thailand, \n",
    "India, Japan, Korea and the western world. After its introduction into India, \n",
    "the kite further evolved into the fighter kite, known as the patang in India, \n",
    "where thousands are flown every year on festivals such as Makar Sankranti. \n",
    "Kites were known throughout Polynesia, as far as New Zealand, with the assumption \n",
    "being that the knowledge diffused from China along with the people. \n",
    "Anthropomorphic kites made from cloth and wood were used in religious ceremonies \n",
    "to send prayers to the gods. Polynesian kite traditions are used by anthropologists \n",
    "get an idea of early \"primitive\" Asian traditions that are believed to have at \n",
    "one time existed in Asia.\"\"\"\n",
    "\n",
    "kite_history = kite_history.lower()\n",
    "kite_history_tokens = tokenizer.tokenize(kite_history)\n",
    "kite_history_tokens_total = len(kite_history_tokens)\n",
    "kite_history_tokens_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac73332e-46c3-4c37-82e2-a86251c4f4e2",
   "metadata": {},
   "source": [
    "We have created our tokenized kite documents. Now, look at the term frequencies of \"kite\" in each document. We store the TFs we find in two dictionaries, one for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d1263554-7717-46e5-9bdc-200c7596de6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Frequency of 'kite' in intro is 0.0441\n"
     ]
    }
   ],
   "source": [
    "intro_tf = {}\n",
    "intro_counts = Counter(kite_text_tokens)\n",
    "intro_tf['kite'] = intro_counts['kite'] / kite_text_tokens_total\n",
    "\n",
    "print(f\"Term Frequency of 'kite' in intro is {intro_tf['kite']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aa764871-06ca-4b17-afcc-58d1a24a14ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Frequency of 'kite' in history is 0.0202\n"
     ]
    }
   ],
   "source": [
    "history_tf = {}\n",
    "history_counts = Counter(kite_history_tokens)\n",
    "history_tf['kite'] = history_counts['kite'] / kite_history_tokens_total\n",
    "\n",
    "print(f\"Term Frequency of 'kite' in history is {history_tf['kite']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "58872cd0-f9ae-4648-a559-4a99a2175fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Frequency of 'and' in intro is 0.0275\n",
      "Term Frequency of 'and' in history is 0.0303\n"
     ]
    }
   ],
   "source": [
    "intro_tf['and'] = intro_counts['and'] / kite_text_tokens_total\n",
    "history_tf['and'] = history_counts['and'] / kite_history_tokens_total\n",
    "\n",
    "print(f\"Term Frequency of 'and' in intro is {intro_tf['and']:.4f}\")\n",
    "print(f\"Term Frequency of 'and' in history is {history_tf['and']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45074942-842e-4592-add9-1689c33c3980",
   "metadata": {},
   "source": [
    "A term's IDF is merely the ratio of the total number of documents to the number of documents the term appears in. In the case of \"and\" and \"kite\" in the current example the answer is the same for both.\n",
    "\n",
    "* 2 total documents / 2 documents contain “and” = 2/2 = 1\n",
    "* 2 total documents / 2 documents contain “kite” = 2/2 = 1\n",
    "* Not very interesting. So let’s look at another word “China.”\n",
    "* 2 total documents / 1 document contains “China” = 2/1 = 2\n",
    "\n",
    "Next, use this \"rarity\" measure to weight the term frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f2b9bee2-57b1-4936-8039-28d5429a671f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_docs_containing_and = 0\n",
    "for doc in [kite_text_tokens, kite_history_tokens]:\n",
    "    if 'and' in doc:\n",
    "        num_docs_containing_and += 1\n",
    "\n",
    "num_docs_containing_and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "794a0239-101e-4c32-b6f4-4a34263c03c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_docs_containing_kite = 0\n",
    "for doc in [kite_text_tokens, kite_history_tokens]:\n",
    "    if 'kite' in doc:\n",
    "        num_docs_containing_kite += 1\n",
    "        \n",
    "num_docs_containing_kite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f069dcfe-efc2-4592-abce-1d73bfc3db7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_docs_containing_china = 0\n",
    "for doc in [kite_text_tokens, kite_history_tokens]:\n",
    "    if 'china' in doc:\n",
    "        num_docs_containing_china += 1\n",
    "        \n",
    "num_docs_containing_china"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "97d7a6f2-9f17-4608-8136-6c3469293558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the TF of \"China\" in the two documents\n",
    "intro_tf['china'] = intro_counts['china'] / kite_text_tokens_total\n",
    "history_tf['china'] = history_counts['china'] / kite_history_tokens_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8935350c-9a5d-46c5-b304-c062fbb6a00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the TDF for all three. Store the IDFs in dictionaries per document.\n",
    "num_docs = 2\n",
    "intro_idf = {}\n",
    "history_idf = {}\n",
    "\n",
    "intro_idf['and'] = num_docs / num_docs_containing_and \n",
    "history_idf['and'] = num_docs / num_docs_containing_and \n",
    "\n",
    "intro_idf['kite'] = num_docs / num_docs_containing_kite \n",
    "history_idf['kite'] = num_docs / num_docs_containing_kite \n",
    "\n",
    "intro_idf['china'] = num_docs / num_docs_containing_china \n",
    "history_idf['china'] = num_docs / num_docs_containing_china "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d0e4c1b0-caa8-4680-bf72-971ffa848456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0275\n",
      "0.0441\n",
      "0.0000\n"
     ]
    }
   ],
   "source": [
    "# Calculate for the intro document\n",
    "intro_tfidf = {}\n",
    "\n",
    "intro_tfidf['and'] = intro_tf['and'] * intro_idf['and']\n",
    "intro_tfidf['kite'] = intro_tf['kite'] * intro_idf['kite']\n",
    "intro_tfidf['china'] = intro_tf['china'] * intro_idf['china']\n",
    "\n",
    "print(f\"{intro_tfidf['and']:.4f}\")\n",
    "print(f\"{intro_tfidf['kite']:.4f}\")\n",
    "print(f\"{intro_tfidf['china']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c1b7bad2-a671-4e9d-b5b3-900e9e44ad29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0303\n",
      "0.0202\n",
      "0.0202\n"
     ]
    }
   ],
   "source": [
    "# Calculate for the history document\n",
    "history_tfidf = {}\n",
    "\n",
    "history_tfidf['and'] = history_tf['and'] * history_idf['and']\n",
    "history_tfidf['kite'] = history_tf['kite'] * history_idf['kite']\n",
    "history_tfidf['china'] = history_tf['china'] * history_idf['china']\n",
    "\n",
    "print(f\"{history_tfidf['and']:.4f}\")\n",
    "print(f\"{history_tfidf['kite']:.4f}\")\n",
    "print(f\"{history_tfidf['china']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b011e4-cd24-429a-baac-8f8a4e5df760",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='3.4.1'></a><a id='3.4.1'></a>\n",
    "## 3.4.1 Return of Zipf\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "Problem: Zipf's Law implies when you compare the frequencies of two words, like \"cat\" and \"dog\", even if they occur a similar number of times, the more frequent word will have an exponentially higher frequency than the less frequent one. \n",
    "\n",
    "Idea: Scale all the word frequencies (and document frequencies) with the `log()` function. This ensures that words such as \"cat\" and \"dog\", which may have similar counts, are not exponentially different in frequency. This log distribution of word frequencies also ensure the TF-IDF scores are more uniformly distributed. \n",
    "\n",
    "Importance: Scaling frequencies with log() is used for a proper TF-IDF score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59205c5-84ab-4186-ab4f-a10da8e38e80",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='3.4.2'></a><a id='3.4.2'></a>\n",
    "## 3.4.2 Relevance ranking\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "Problem: Given a query and corpus, we want to determing how relevant each text document is for the given query.\n",
    "\n",
    "Idea: Use *relevance ranking* algorithms to determine the degree of relevance. This allows the ranking of text documents and actions such as returning the best matching documents for the query.\n",
    "\n",
    "Importance: *Relevance ranking* is a core problem of Information Retrieval,  playing a fundamental role in real world applications such as search engines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ab00335d-fcb3-4814-8024-774b41ed1120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([   (',', 0.16666666666666666),\n",
      "                ('.', 0.05555555555555555),\n",
      "                ('and', 0.08333333333333333),\n",
      "                ('as', 0),\n",
      "                ('faster', 0.25),\n",
      "                ('get', 0.16666666666666666),\n",
      "                ('got', 0.16666666666666666),\n",
      "                ('hairy', 0),\n",
      "                ('harry', 0.0),\n",
      "                ('home', 0.16666666666666666),\n",
      "                ('is', 0),\n",
      "                ('jill', 0),\n",
      "                ('not', 0),\n",
      "                ('store', 0.16666666666666666),\n",
      "                ('than', 0),\n",
      "                ('the', 0.5),\n",
      "                ('to', 0.16666666666666666),\n",
      "                ('would', 0.16666666666666666)])\n",
      "----------------------------------------\n",
      "OrderedDict([   (',', 0),\n",
      "                ('.', 0.05555555555555555),\n",
      "                ('and', 0.08333333333333333),\n",
      "                ('as', 0),\n",
      "                ('faster', 0.08333333333333333),\n",
      "                ('get', 0),\n",
      "                ('got', 0),\n",
      "                ('hairy', 0.08333333333333333),\n",
      "                ('harry', 0.0),\n",
      "                ('home', 0),\n",
      "                ('is', 0.08333333333333333),\n",
      "                ('jill', 0.0),\n",
      "                ('not', 0),\n",
      "                ('store', 0),\n",
      "                ('than', 0.16666666666666666),\n",
      "                ('the', 0),\n",
      "                ('to', 0),\n",
      "                ('would', 0)])\n",
      "----------------------------------------\n",
      "OrderedDict([   (',', 0),\n",
      "                ('.', 0.05555555555555555),\n",
      "                ('and', 0),\n",
      "                ('as', 0.1111111111111111),\n",
      "                ('faster', 0),\n",
      "                ('get', 0),\n",
      "                ('got', 0),\n",
      "                ('hairy', 0.08333333333333333),\n",
      "                ('harry', 0.0),\n",
      "                ('home', 0),\n",
      "                ('is', 0.08333333333333333),\n",
      "                ('jill', 0.0),\n",
      "                ('not', 0.16666666666666666),\n",
      "                ('store', 0),\n",
      "                ('than', 0),\n",
      "                ('the', 0),\n",
      "                ('to', 0),\n",
      "                ('would', 0)])\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create a K-dimensional vector representation of each document in the corpus.\n",
    "doc_0 = \"The faster Harry got to the store, the faster Harry, the faster, would get home.\"\n",
    "doc_1 = \"Harry is hairy and faster than Jill.\"\n",
    "doc_2 = \"Jill is not as hairy as Harry.\"\n",
    "\n",
    "document_tfidf_vectors = []\n",
    "documents = [doc_0, doc_1, doc_2]\n",
    "\n",
    "for doc in docs:\n",
    "    vec = copy.copy(zero_vector)\n",
    "    tokens = tokenizer.tokenize(doc.lower())\n",
    "    token_counts = Counter(tokens)\n",
    "    \n",
    "    for key, value in token_counts.items():\n",
    "        docs_containing_key = 0\n",
    "        for _doc in docs:\n",
    "            if key in _doc:\n",
    "                docs_containing_key += 1\n",
    "        tf = value / len(lexicon)\n",
    "        if docs_containing_key:\n",
    "            idf = len(docs) / docs_containing_key\n",
    "        else:\n",
    "            idf = 0\n",
    "        vec[key] = tf * idf\n",
    "    document_tfidf_vectors.append(vec)\n",
    "\n",
    "for x in document_tfidf_vectors:\n",
    "    pp.pprint(x)\n",
    "    HR()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e9e1e5-3a43-44ca-ba87-f33918b03762",
   "metadata": {},
   "source": [
    "Now you have all you need to do a basic TF-IDF-based search. You can treat the search query itself as a document, and therefore get the TF-IDF-based vector representation of it. The last step is then to find the documents whose vectors have the highest cosine similarities to the query and return those as the search results.\n",
    "If you take your three documents about Harry, and make the query \"How long does it take to get to the store?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b1599eba-ac4a-4172-99a2-3b092f21cf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0: 0.6133\n",
      "Document 1: 0.0000\n",
      "Document 2: 0.0000\n"
     ]
    }
   ],
   "source": [
    "query = \"How long does it take to get to the store?\"\n",
    "\n",
    "# Make sure we are dealing with new objects, not multiple references to the same object\n",
    "query_vec = copy.copy(zero_vector)  \n",
    "\n",
    "tokens = tokenizer.tokenize(query.lower())\n",
    "token_counts = Counter(tokens)\n",
    "\n",
    "for key, value in token_counts.items():\n",
    "    docs_containing_key = 0\n",
    "    for _doc in documents:\n",
    "        if key in _doc.lower():\n",
    "            docs_containing_key += 1\n",
    "    if docs_containing_key == 0:  # We didn't find that token in the lexicon go to next key\n",
    "        continue\n",
    "    tf = value / len(tokens)\n",
    "    idf = len(documents) / docs_containing_key \n",
    "    query_vec[key] = tf * idf \n",
    "\n",
    "print(f\"Document 0: {cosine_sim(query_vec, document_tfidf_vectors[0]):.4f}\")\n",
    "print(f\"Document 1: {cosine_sim(query_vec, document_tfidf_vectors[1]):.4f}\")\n",
    "print(f\"Document 2: {cosine_sim(query_vec, document_tfidf_vectors[2]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c24e9b-8cc0-4694-9eab-92ef716e6136",
   "metadata": {},
   "source": [
    "* Three documents: doc_0, doc_1, doc_2\n",
    "* Query: \"How long does it take to get to the store?\"\n",
    "* Result: document 0 has the most relevance for this query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d777cfb-2251-430b-ad2e-f0741f709c18",
   "metadata": {},
   "source": [
    "<a name='3.4.3'></a><a id='3.4.3'></a>\n",
    "## 3.4.3 Tools\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "Problem: Use a library to quickly build a TF-IDF matrix.\n",
    "\n",
    "Idea: Use the sklearn library's TF-IDF class.  \n",
    "\n",
    "---\n",
    "\n",
    "API Notes:\n",
    "\n",
    "* [sklearn TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer)\n",
    "\n",
    "Using the scikit-learn library, we create a matrix for the three documents and the inverse document frequency for each term in the lexicon. We have a matrix (a list of Python lists) that represents the three documents (the three rows of the matrix). The TF-IDF of each term, token, or word in the lexicon make up the columns of the matrix (here, the indices of each row). They only have 16, as they tokenize differently and drop the punctuation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "384326aa-390e-4979-a2ee-7d2946360d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2 0.  0.5 0.2 0.2 0.  0.3 0.2 0.  0.  0.  0.2 0.  0.6 0.2 0.2]\n",
      " [0.4 0.  0.4 0.  0.  0.4 0.3 0.  0.4 0.4 0.  0.  0.5 0.  0.  0. ]\n",
      " [0.  0.8 0.  0.  0.  0.3 0.2 0.  0.3 0.3 0.4 0.  0.  0.  0.  0. ]]\n",
      "----------------------------------------\n",
      "  (0, 7)\t0.21233717847222938\n",
      "  (0, 3)\t0.21233717847222938\n",
      "  (0, 15)\t0.21233717847222938\n",
      "  (0, 0)\t0.1614878973151404\n",
      "  (0, 11)\t0.21233717847222938\n",
      "  (0, 14)\t0.21233717847222938\n",
      "  (0, 4)\t0.21233717847222938\n",
      "  (0, 6)\t0.25081951635416216\n",
      "  (0, 2)\t0.48446369194542127\n",
      "  (0, 13)\t0.6370115354166881\n",
      "  (1, 9)\t0.36930805406135764\n",
      "  (1, 12)\t0.4855957102062416\n",
      "  (1, 5)\t0.36930805406135764\n",
      "  (1, 8)\t0.36930805406135764\n",
      "  (1, 0)\t0.36930805406135764\n",
      "  (1, 6)\t0.28680064898176716\n",
      "  (1, 2)\t0.36930805406135764\n",
      "  (2, 1)\t0.7514324226348535\n",
      "  (2, 10)\t0.3757162113174268\n",
      "  (2, 9)\t0.28574186296253085\n",
      "  (2, 5)\t0.28574186296253085\n",
      "  (2, 8)\t0.28574186296253085\n",
      "  (2, 6)\t0.221904046872743\n"
     ]
    }
   ],
   "source": [
    "corpus = docs\n",
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "\n",
    "# The TFIDFVectorizer model produces a sparse numpy matrix, because a \n",
    "# TF-IDF matrix usually contains mostly zeros, since most documents \n",
    "# use a small portion of the total words in the vocabulary.\n",
    "model = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# The model becomes a sparse numpy matrix, as in a large corpus there \n",
    "# would be mostly zeros to deal with. todense() brings it back to a \n",
    "# regular numpy matrix for easier viewing.\n",
    "print(model.todense().round(1))\n",
    "HR()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7d4341c0-2ffb-4402-8c32-dca1cb6ecea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and' 'as' 'faster' 'get' 'got' 'hairy' 'harry' 'home' 'is' 'jill' 'not'\n",
      " 'store' 'than' 'the' 'to' 'would']\n",
      "----------------------------------------\n",
      "['[0.2 0.  0.5 0.2 0.2 0.  0.3 0.2 0.  0.  0.  0.2 0.  0.6 0.2 0.2]%', '[0.4 0.  0.4 0.  0.  0.4 0.3 0.  0.4 0.4 0.  0.  0.5 0.  0.  0. ]%', '[0.  0.8 0.  0.  0.  0.3 0.2 0.  0.3 0.3 0.4 0.  0.  0.  0.  0. ]%']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names_out())\n",
    "HR()\n",
    "\n",
    "x = model.toarray()\n",
    "x = list(map(lambda x :str(x) + '%', x.round(1)))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "660e1a0f-aafc-486a-800c-d55c08fa302e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5 0.2 0.2 0.3 0.2 0.2 0.7 0.2 0.2]]\n",
      "----------------------------------------\n",
      "  (0, 4)\t0.1690308509457033\n",
      "  (0, 1)\t0.1690308509457033\n",
      "  (0, 8)\t0.1690308509457033\n",
      "  (0, 5)\t0.1690308509457033\n",
      "  (0, 7)\t0.1690308509457033\n",
      "  (0, 2)\t0.1690308509457033\n",
      "  (0, 3)\t0.3380617018914066\n",
      "  (0, 0)\t0.50709255283711\n",
      "  (0, 6)\t0.6761234037828132\n"
     ]
    }
   ],
   "source": [
    "corpus = [doc_0]\n",
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "\n",
    "# The TFIDFVectorizer model produces a sparse numpy matrix, because a \n",
    "# TF-IDF matrix usually contains mostly zeros, since most documents \n",
    "# use a small portion of the total words in the vocabulary.\n",
    "model = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# The model becomes a sparse numpy matrix, as in a large corpus there \n",
    "# would be mostly zeros to deal with. todense() brings it back to a \n",
    "# regular numpy matrix for easier viewing.\n",
    "print(model.todense().round(1))\n",
    "HR()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e5992c-0680-4f30-aa25-beac88f15fa7",
   "metadata": {},
   "source": [
    "<a name='3.4.4'></a><a id='3.4.4'></a>\n",
    "## 3.4.4 Alternatives\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "Problem: What are the various ways to normalize and smooth frequency weights?\n",
    "\n",
    "Idea: TF-IDF, TF-ICF, Okapi BM25, ATC, LTU, MI, PosMI, T-Test, x², Lin98a, Lin98b, Gref94"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c009005d-f0a8-40b2-91fc-064c9384734d",
   "metadata": {},
   "source": [
    "<a name='3.4.5'></a><a id='3.4.5'></a>\n",
    "## 3.4.5 Okapi BM25\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "Problem: Create a scoring function that goes beyond rewarding term frequency and penalizing document frequency, and also account for document length and term frequency saturation. \n",
    "\n",
    "Idea: The *Okapi BM25* bag-of-words retrieval function ranks a set of documents based on the query terms appearing in each document, regardless of their proximity in within the document. This does not just compute the TF-IDF cosine similarity, it normalizes and smooths the similarity. It also ignores duplicate terms in the query document, clipping the term frequencies for the query vector at 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5faa71-e59e-453c-9408-60bc750d775e",
   "metadata": {},
   "source": [
    "<a name='3.4.6'></a><a id='3.4.6'></a>\n",
    "## 3.4.6 What's Next\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "Problem: What is the next step after converting natural language text to numbers?\n",
    "\n",
    "Idea: We can begin to manipulate and compute with them, refining those numbers to represent the meaning or topic of natural language text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eb09cf-9c48-4ea0-8bc8-7330c14ad399",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

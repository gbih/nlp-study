{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6924c1d7-944e-48be-bae8-0800f445c2c0",
   "metadata": {},
   "source": [
    "<a id='top'></a><a name='top'></a>\n",
    "# Chapter 2: Build your vocabulary (word tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5e538a-4199-4175-9f9a-23bd7056a9d3",
   "metadata": {},
   "source": [
    "* [Introduction](#introduction)\n",
    "* [2.0 Imports and Setup](#2.0)\n",
    "* [2.1 Challenges (a preview of learning)](#2.1)\n",
    "* [2.2 Building your vocabulary with a tokenizer](#2.2)\n",
    "     - [2.2.1 Dot product](#2.2.1)\n",
    "     - [2.2.2 Measuring bag-of-words overlap](#2.2.2)\n",
    "     - [2.2.3 A token improvement](#2.2.3) \n",
    "     - [2.2.4 Extending your vocabulary with n-grams](#2.2.4)\n",
    "     - [2.2.5 Normalizing your vocabulary](#2.2.5)\n",
    "* [2.3 Sentiment](#2.3)\n",
    "    - [2.3.1 VADER - A rule-based sentiment analyzer](#2.3.1)\n",
    "    - [2.3.2 Naive Bayes](#2.3.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c25b1c-e3d6-4630-a412-817e3d623983",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---\n",
    "<a name='introduction'></a><a id='introduction'></a>\n",
    "# Introduction\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "### Dataset\n",
    "\n",
    "* movieReviewSnippets_GroundTruth.csv.gz: [script](#movieReviewSnippets_GroundTruth.csv.gz), [source](https://github.com/totalgood/nlpia/raw/master/src/nlpia/data/hutto_ICWSM_2014/movieReviewSnippets_GroundTruth.csv.gz)\n",
    "* amazonReviewSnippets_GroundTruth.csv.gz: [script](#amazonReviewSnippets_GroundTruth.csv.gz), [source](https://github.com/totalgood/nlpia/raw/master/src/nlpia/data/hutto_ICWSM_2014/amazonReviewSnippets_GroundTruth.csv.gz)\n",
    "\n",
    "### Explore\n",
    "\n",
    "* Tokenizing text into words and n-grams (tokens)\n",
    "* Dealing with nonstandard punctuation and emoticons, like social media posts\n",
    "* Compressing token vocabulary  with stemming and lemmatization\n",
    "* Building a vector representation of a statement\n",
    "* Building a sentiment analyzer from handcrafted token scores\n",
    "\n",
    "\n",
    "### Key points\n",
    "\n",
    "* Implement tokenization and configure a tokenizer for an application.\n",
    "* n-gram tokenization helps retain some of the word order information in a document.\n",
    "* Normalization and stemming consolidate words into groups that improve the \"recall\" for search engines but reduce precision.\n",
    "* Lemmatization and customized tokenizers like `casual_tokenize()` can improve precision and reduce information loss.\n",
    "* Stop words can contain useful information and discarding them is not always helpful.\n",
    "* In natural language processing, composing a numerical vector from text is a particularly \"lossy\" feature extraction process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8786e80-a8db-42f7-a6ee-c3ad2b4b7f44",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='2.0'></a><a id='2.0'></a>\n",
    "# 2.0 Imports and Setup\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18636d78-64cc-4d7b-baa0-8b6495c19e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('setup'):\n",
    "    os.mkdir('setup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "348ff94f-4d2d-47ec-abc3-e786aa7a3eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "req_file = \"setup/requirements_02.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "229f9daa-617c-46fe-b1e6-cd0f3dfa16fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting setup/requirements_02.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile {req_file}\n",
    "isort\n",
    "scikit-learn-intelex\n",
    "vaderSentiment\n",
    "watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ec388c0-36f0-42b0-92dd-eed9afd4d6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "IS_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IS_COLAB:\n",
    "    print(\"Installing packages\")\n",
    "    !pip install --upgrade --quiet -r {req_file}\n",
    "else:\n",
    "    print(\"Running locally.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "652dfc81-bd6a-41ad-8260-6a98a3af5c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "#if IS_COLAB:\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "cc23b160-75ee-41e0-9f93-9387a4c9fa50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting setup/chp02_imports.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile setup/chp02_imports.py\n",
    "import locale\n",
    "import pprint\n",
    "import random\n",
    "import re\n",
    "import warnings\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import casual_tokenize\n",
    "from nltk.tokenize.casual import casual_tokenize\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import \\\n",
    "    ENGLISH_STOP_WORDS as sklearn_stop_words\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from tqdm.auto import tqdm\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from watermark import watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "72993c5a-2528-45e4-84e9-b343147a8b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import locale\n",
      "import pprint\n",
      "import random\n",
      "import re\n",
      "import warnings\n",
      "from collections import Counter\n",
      "\n",
      "import nltk\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "from nltk.stem.porter import PorterStemmer\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "from nltk.tokenize import TreebankWordTokenizer\n",
      "from nltk.tokenize import casual_tokenize\n",
      "from nltk.tokenize.casual import casual_tokenize\n",
      "from nltk.util import ngrams\n",
      "from sklearn.feature_extraction.text import \\\n",
      "    ENGLISH_STOP_WORDS as sklearn_stop_words\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from tqdm.auto import tqdm\n",
      "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
      "from watermark import watermark\n"
     ]
    }
   ],
   "source": [
    "!isort setup/chp02_imports.py --sl\n",
    "!cat setup/chp02_imports.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "3cd6e1e9-abdd-4648-be0d-6758fb69591e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "import pprint\n",
    "import random\n",
    "import re\n",
    "import warnings\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import casual_tokenize\n",
    "from nltk.tokenize.casual import casual_tokenize\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import \\\n",
    "    ENGLISH_STOP_WORDS as sklearn_stop_words\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from tqdm.auto import tqdm\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from watermark import watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6ae00eff-a4bc-491d-a568-1b1000286074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.8.12\n",
      "IPython version      : 7.34.0\n",
      "\n",
      "Compiler    : Clang 13.0.0 (clang-1300.0.29.3)\n",
      "OS          : Darwin\n",
      "Release     : 21.6.0\n",
      "Machine     : x86_64\n",
      "Processor   : i386\n",
      "CPU cores   : 4\n",
      "Architecture: 64bit\n",
      "\n",
      "pandas : 1.5.3\n",
      "re     : 2.2.1\n",
      "numpy  : 1.23.5\n",
      "seaborn: 0.12.1\n",
      "nltk   : 3.8\n",
      "sys    : 3.8.12 (default, Dec 13 2021, 20:17:08) \n",
      "[Clang 13.0.0 (clang-1300.0.29.3)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def HR():\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "def getpreferredencoding(do_setlocale = True):\n",
    "    return \"UTF-8\"\n",
    "\n",
    "locale.getpreferredencoding = getpreferredencoding\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style(\"darkgrid\")\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "random.seed(23)\n",
    "\n",
    "print(watermark(iversions=True,globals_=globals(),python=True,machine=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3136fe4e-7f25-4403-8231-1bf7376b0892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/gb/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302d215f-1284-4ffb-9c33-12900cb61ba7",
   "metadata": {},
   "source": [
    "<a name='2.1'></a><a id='2.1'></a>\n",
    "# 2.1 Challenges\n",
    "\n",
    "## A preview of stemming\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "Problem: How can we create representations of the same cluster of words?\n",
    "\n",
    "Idea: Stemming is a basic strategy to group various inflections of a word into the same \"bucket\" or cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51570e2e-6986-4f5f-8161-6bcfd8745744",
   "metadata": {},
   "source": [
    "<a name='2.2'></a><a id='2.2'></a>\n",
    "## 2.2 Building your vocabulary with a tokenizer\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "Problem: How to transform unstructured data, natural language text, into units of information that are countable discrete elements?\n",
    "\n",
    "Idea: Use a tokenizer to segment text into tokens. The simplest method to tokenize a sentence is to split on whitespace as the \"delimeter\" of words. After this is done, we can create vector representations for each word, which are called *one-hot* vectors. This solves the first problem of NLP, which is to transform words into numbers. \n",
    "\n",
    "Tokenization is the first step in an NLP pipeline, so it can have a big impact on the rest of your pipeline. A tokenizer breaks unstructured data, natural language text, into chunks of information that can be counted as discrete elements. These counts of token occurrences in a document can be used directly as a vector representing that document. This immediately turns an unstructured string (text document) into a numerical data structure suitable for machine learning. The most common use for bag-of-words vectors created this way is for document retrieval, or search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e1cb1c19-7e77-4dd3-a46f-7478b7eef258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thomas',\n",
       " 'Jefferson',\n",
       " 'began',\n",
       " 'building',\n",
       " 'Monticello',\n",
       " 'at',\n",
       " 'the',\n",
       " 'age',\n",
       " 'of',\n",
       " '26.']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"\"\"Thomas Jefferson began building Monticello at the age of 26.\"\"\"\n",
    "sentence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4d22198b-a291-4170-9566-f6f72e5f76f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_sequence = str.split(sentence)\n",
    "vocab = sorted(set(token_sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e6d002ce-00f5-49c1-b14d-8adcf37cb08a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'26., Jefferson, Monticello, Thomas, age, at, began, building, of, the'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "', '.join(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "56ecad2a-f906-4edd-94f6-ca2570345440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens = len(token_sequence)\n",
    "num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4714ae8a-c48c-4335-8e0c-0395f2800125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a2fbfd24-e232-44e1-ba57-d107852c06ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_vectors = np.zeros((num_tokens, vocab_size), int)\n",
    "onehot_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29ee633-6856-4770-8ea3-274f388f6fb8",
   "metadata": {},
   "source": [
    "In programming terms, a one-hot vector is akin to a Python `list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a9286ad0-7941-4468-a59b-13c45ee6b1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple one-hot vector\n",
    "# For each word in the sentence, mark the column \n",
    "# for that word in your vocabulary with a 1.\n",
    "for i, word in enumerate(token_sequence):\n",
    "    onehot_vectors[i, vocab.index(word)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "995c0b30-daae-48c9-91c8-9935cdd5fa67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Representation of this one-sentence document.\n",
    "onehot_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "16a5a643-c1da-46c4-84c5-1cedd5ea4781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'26. Jefferson Monticello Thomas age at began building of the'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check\n",
    "' '.join(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "0233f45d-6c7e-4b1e-99f3-9d3de0f33757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Guten': 1, 'Morgen': 1, 'Rosa': 1})"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(\"Guten Morgen Rosa\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe61a9d1-0526-4b78-958e-ee77e8debfe1",
   "metadata": {},
   "source": [
    "We can use Pandas for a more legible dataset, since we can label each column with the token it represents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "aacac6dc-88a6-4512-ba55-3c3c84c61c77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>26.</th>\n",
       "      <th>Jefferson</th>\n",
       "      <th>Monticello</th>\n",
       "      <th>Thomas</th>\n",
       "      <th>age</th>\n",
       "      <th>at</th>\n",
       "      <th>began</th>\n",
       "      <th>building</th>\n",
       "      <th>of</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   26.  Jefferson  Monticello  Thomas  age  at  began  building  of  the\n",
       "0    0          0           0       1    0   0      0         0   0    0\n",
       "1    0          1           0       0    0   0      0         0   0    0\n",
       "2    0          0           0       0    0   0      1         0   0    0\n",
       "3    0          0           0       0    0   0      0         1   0    0\n",
       "4    0          0           1       0    0   0      0         0   0    0\n",
       "5    0          0           0       0    0   1      0         0   0    0\n",
       "6    0          0           0       0    0   0      0         0   0    1\n",
       "7    0          0           0       0    1   0      0         0   0    0\n",
       "8    0          0           0       0    0   0      0         0   1    0\n",
       "9    1          0           0       0    0   0      0         0   0    0"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Listing 2.2 One-hot vector sequence for the Monticello sentence\n",
    "pd.DataFrame(onehot_vectors, columns=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d61ef0eb-7441-4e52-9b7d-f1438423ea34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>26.</th>\n",
       "      <th>Jefferson</th>\n",
       "      <th>Monticello</th>\n",
       "      <th>Thomas</th>\n",
       "      <th>age</th>\n",
       "      <th>at</th>\n",
       "      <th>began</th>\n",
       "      <th>building</th>\n",
       "      <th>of</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  26. Jefferson Monticello Thomas age at began building of the\n",
       "0                               1                             \n",
       "1             1                                               \n",
       "2                                            1                \n",
       "3                                                     1       \n",
       "4                        1                                    \n",
       "5                                      1                      \n",
       "6                                                            1\n",
       "7                                   1                         \n",
       "8                                                        1    \n",
       "9   1                                                         "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Listing 2.3 Prettier one-hot vectors\n",
    "df = pd.DataFrame(onehot_vectors, columns=vocab)\n",
    "df[df == 0] = ''\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac911743-f8d9-48f5-b100-b06548e39c93",
   "metadata": {},
   "source": [
    "The main disadvantage of one-hot encoding is that a long document can result in a huge vector size needed for a one-hot vector representation. This can result in a very sparse, inefficient data structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "14115c46-3396-4683-9663-5b5fe4e6d44a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'157,500,000'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assume 3000 books with 3500 sentences each, \n",
    "# and 15 rods per sentence.\n",
    "num_rows = 3000 * 3500 * 15\n",
    "f\"{num_rows:,}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "cae57b33-7f30-4144-8b14-5e979f3a3e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'157,500,000,000,000'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assume a million tokens in the NLP pipeline vocabulary.\n",
    "# If we assume a single bit for matrix cell, calculate necessary memory.\n",
    "num_bytes = num_rows * 1_000_000\n",
    "f\"{num_bytes:,}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "707a4e78-444e-4729-b341-242fa44cd411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157500.0"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb = num_bytes / 1e9 # gigabytes\n",
    "gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "bc4acf28-02e2-4dff-a0be-a4e5b9d0ec48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157.5"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb / 1000 # terabytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6496b05b-5d7f-4d8b-aecb-b9eff198c476",
   "metadata": {},
   "source": [
    "As an alternative, we want to compress the meaning of a document down to a single vector, rather than a big table. \n",
    "\n",
    "One alternative is to organize tokens into a bag-of-words vector. This is a word frequency vector, since it only counts the frequence of words, not their order. We can use this single vector to represent a whole document in a single, reasonable-length vector. It would only be as long as the number of unique tokens we want to keep track of.\n",
    "\n",
    "In programming terms, this is akin to a Python `dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "aeafd955-4084-47d8-8393-fb207d0847e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Thomas': 1,\n",
       " 'Jefferson': 1,\n",
       " 'began': 1,\n",
       " 'building': 1,\n",
       " 'Monticello': 1,\n",
       " 'at': 1,\n",
       " 'the': 1,\n",
       " 'age': 1,\n",
       " 'of': 1,\n",
       " '26.': 1}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put tokens into a binary vector indicating the presence or absence of \n",
    "# a particular word in a particular sentence, similar to a book index.\n",
    "# How a single text document looks like as a binary bag-of-words vector\n",
    "sentence_bow = {}\n",
    "for token in sentence.split():\n",
    "    sentence_bow[token] = 1\n",
    "\n",
    "sentence_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f50a6cdb-1a0d-40f6-b874-c3e3762b0637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('26.', 1),\n",
       " ('Jefferson', 1),\n",
       " ('Monticello', 1),\n",
       " ('Thomas', 1),\n",
       " ('age', 1),\n",
       " ('at', 1),\n",
       " ('began', 1),\n",
       " ('building', 1),\n",
       " ('of', 1),\n",
       " ('the', 1)]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(sentence_bow.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "6afeedf2-8e22-486b-b5b9-114f8e56dc30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Thomas': 1,\n",
       " 'Jefferson': 1,\n",
       " 'began': 1,\n",
       " 'building': 1,\n",
       " 'Monticello': 1,\n",
       " 'at': 1,\n",
       " 'the': 1,\n",
       " 'age': 1,\n",
       " 'of': 1,\n",
       " '26.': 1}"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_bow_sorted = dict([(token, 1) for token in sentence.split()])\n",
    "sentence_bow_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "5ada3d73-e370-4a71-ae5a-1ed490bbd552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Thomas</th>\n",
       "      <th>Jefferson</th>\n",
       "      <th>began</th>\n",
       "      <th>building</th>\n",
       "      <th>Monticello</th>\n",
       "      <th>at</th>\n",
       "      <th>the</th>\n",
       "      <th>age</th>\n",
       "      <th>of</th>\n",
       "      <th>26.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sent</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Thomas  Jefferson  began  building  Monticello  at  the  age  of  26.\n",
       "sent       1          1      1         1           1   1    1    1   1    1"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(pd.Series(sentence_bow_sorted), columns=['sent']).T\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "ea208114-b1d5-43c9-8342-d039bfe74851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Thomas</th>\n",
       "      <th>Jefferson</th>\n",
       "      <th>began</th>\n",
       "      <th>building</th>\n",
       "      <th>Monticello</th>\n",
       "      <th>at</th>\n",
       "      <th>the</th>\n",
       "      <th>age</th>\n",
       "      <th>of</th>\n",
       "      <th>26.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sent0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Thomas  Jefferson  began  building  Monticello  at  the  age  of  26.\n",
       "sent0       1          1      1         1           1   1    1    1   1    1\n",
       "sent1       0          0      0         0           0   0    0    0   0    0\n",
       "sent2       0          0      0         0           0   0    1    0   0    0\n",
       "sent3       0          0      0         0           1   0    0    0   0    0"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Listing 2.4 Construct a DataFrame of bag-of-words vectors\n",
    "sentences = \"\"\"Thomas Jefferson began building Monticello at the age of 26.\\n\"\"\"\n",
    "sentences += \"\"\"Construction was done mostly by local masons and carpenters.\\n\"\"\"\n",
    "sentences += \"\"\"He moved into the South Pavilion in 1770.\\n\"\"\"\n",
    "sentences += \"\"\"Turning Monticello into a neoclassical masterpiece was Jefferson's obsession.\"\"\"\n",
    "corpus = {}\n",
    "\n",
    "# sentences = \"\"\"Thomas Jefferson began building Monticello at the age of 26.\n",
    "# Construction was done mostly by local masons and carpenters.\n",
    "# He moved into the South Pavilion in 1770.\n",
    "# Turning Monticello into a neoclassical masterpiece was Jefferson's obsession.\"\"\"\n",
    "\n",
    "corpus = {}\n",
    "\n",
    "for i, sent in enumerate(sentences.split('\\n')):\n",
    "    corpus['sent{}'.format(i)] = dict((tok, 1) for tok in sent.split())\n",
    "\n",
    "df = pd.DataFrame.from_records(corpus).fillna(0).astype(int).T\n",
    "df[df.columns[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84efab2f-e8ff-433b-bfad-8ba1d9bd493d",
   "metadata": {},
   "source": [
    "<a name='2.2.1'></a><a id='2.2.1'></a>\n",
    "### 2.2.1 Dot product\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "Problem: Need a way to check for similarities between sentences.\n",
    "\n",
    "Idea: Count the number of overlapping tokens using a *dot product*. This produces a single scalar value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "237b5306-c65d-4e8d-b876-377d6172eaf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Listing 2.5 Example dot product calculation\n",
    "v1 = np.array([1,2,3])\n",
    "v2 = np.array([2,3,4])\n",
    "v1.dot(v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "74d05520-0a77-4a57-8030-799f093172a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  6, 12])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(v1 * v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "aede7bac-9764-49fd-8bc8-61eb68889c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(v1 * v2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "4dc1ee66-e216-40f3-8384-2bca67e3a0e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([x1 * x2 for x1, x2 in zip(v1, v2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7501e168-cb09-48d2-bf39-b5e2669e52af",
   "metadata": {},
   "source": [
    "<a name='2.2.2'></a><a id='2.2.2'></a>\n",
    "### 2.2.2 Measuring bag-of-words overlap\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "Problem: Need an estimate of how similar two vectors are in the words they use (assuming we can measure the bag of words overlap).\n",
    "\n",
    "Idea: Estimate bag-of-words vector overlap between new sentences and original sentences. This is our first vector space model (VSM) of natural language documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "b508c59c-913a-41d6-bbcb-ff97a09b153a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 2.6 Overlap of word counts for two bag-of-words vectors\n",
    "df2 = df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e0bcf92a-7e79-4242-b60c-20286ff619b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Good': 1, 'morning,': 1, 'Rosa!': 1})"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(\"Good morning, Rosa!\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "735c6f47-c52e-4feb-9da3-b7602b3bbd58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thomas Jefferson began building Monticello at the age of 26.'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentence 0\n",
    "' '.join(corpus['sent0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "311af9bd-95ab-4b59-8b9f-b4153b210e26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Construction was done mostly by local masons and carpenters.'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentence 1\n",
    "' '.join(corpus['sent1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "fb89d19f-1898-4f9a-aed2-6a13e201cf1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He moved into the South Pavilion in 1770.'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentence 2\n",
    "' '.join(corpus['sent2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "e86aaf8f-566b-45c8-9fee-d177d2895030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Turning Monticello into a neoclassical masterpiece was Jefferson's obsession.\""
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentence 3\n",
    "' '.join(corpus['sent3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ed4df4e1-cd27-4af8-9e4e-d920b76f79c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# The maximum, overlap of word counts for two identical sentences.\n",
    "print(df2.sent0.dot(df2.sent0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "3322b7b3-c459-4356-a12e-0a5cdbc02796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No words were used in both sent0 and sent1\n",
    "df2.sent0.dot(df2.sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "621bc746-327b-4fbc-960a-d132dab2fe7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One word was used in both sent0 and sent2\n",
    "df2.sent0.dot(df2.sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "90606eed-dc72-48dd-8be3-8b1aac8acce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One word was used in both sent2 and sent3\n",
    "df2.sent0.dot(df2.sent3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "c0182b9b-8ae9-4b73-acc0-cf8438c691ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1)]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the word that that is shared by sent0 and sent3.\n",
    "# This is the word that gives you the last dot product of 1.\n",
    "[(k, v) for (k, v) in (df2.sent0 & df2.sent2).items() if v]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf75daed-f020-4aa2-a2cb-6b51d7cfa2a2",
   "metadata": {},
   "source": [
    "<a name='2.2.3'></a><a id='2.2.3'></a>\n",
    "### 2.2.3 A token improvement\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "Problem: Need a way to delimit on other characters besides spaces to separate words in sentences.\n",
    "\n",
    "Idea: Use regex for more control over tokenization. \n",
    "\n",
    "`r'[-\\s]'` is equivalent to `r' \\t\\n\\r\\x0b\\x-c'`, eg match a space character (space, \\t, \\r, \\n).\n",
    "\n",
    "We then extend this regex to also include the characters `.,;!?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "cf8da46b-bdad-4d31-85ab-f2daea700df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thomas',\n",
       " 'Jefferson',\n",
       " 'began',\n",
       " 'building',\n",
       " 'Monticello',\n",
       " 'at',\n",
       " 'the',\n",
       " 'age',\n",
       " 'of',\n",
       " '26',\n",
       " '']"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Listing 2.7 Tokenize the Monticello sentence with a regular expression\n",
    "sentence = \"\"\"Thomas Jefferson began building Monticello at the age of 26.\"\"\"\n",
    "tokens = re.split(r'[-\\s.,;!?]+', sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7adf0e7-2d7d-4da5-a371-b920d80dcc18",
   "metadata": {},
   "source": [
    "**Improved Regular Expressions for Separating Words**\n",
    "\n",
    "Problem: We want to improve the speed of the regex pattern.\n",
    "\n",
    "Idea: Compile the regex object via `re.compile()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "13348d39-efe2-4aac-9b02-d9c83bae6fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ', 'the', ' ', 'age', ' ', 'of', ' ', '26', '.', '']"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compiling a regex pattern\n",
    "pattern = re.compile(r\"([-\\s.,;!?])+\")\n",
    "tokens = pattern.split(sentence)\n",
    "tokens[-10:]  # just the last 10 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "e02344fe-4a65-48c0-b5cf-885717e93813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thomas',\n",
       " 'Jefferson',\n",
       " 'began',\n",
       " 'building',\n",
       " 'Monticello',\n",
       " 'at',\n",
       " 'the',\n",
       " 'age',\n",
       " 'of',\n",
       " '26']"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"\"\"Thomas Jefferson began building Monticello at the age of 26.\"\"\"\n",
    "tokens = pattern.split(sentence)\n",
    "[x for x in tokens if x and x not in '- \\t\\n.,;!?']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bbd302-84de-421e-af78-380fe4cabf4b",
   "metadata": {},
   "source": [
    "Next, use the NLTK function RegexpTokenizer to replicate the previous tokenizer example.\n",
    "\n",
    "This tokenizer is better than the previous tokenizer:\n",
    "1. It ignores whitespace tokens\n",
    "2. It also separates sentence-ending trailing punctuation from tokens that do not contain any other punctuation characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "762360b6-4d0a-4b80-af23-ce1cfa3ad373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegexpTokenizer(pattern='\\\\w+|$[0-9.]+|\\\\S+', gaps=False, discard_empty=True, flags=re.UNICODE|re.MULTILINE|re.DOTALL)\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Thomas',\n",
       " 'Jefferson',\n",
       " 'began',\n",
       " 'building',\n",
       " 'Monticello',\n",
       " 'at',\n",
       " 'the',\n",
       " 'age',\n",
       " 'of',\n",
       " '26',\n",
       " '.']"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+|$[0-9.]+|\\S+')\n",
    "print(tokenizer)\n",
    "HR()\n",
    "\n",
    "tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de883b9e-6893-40ca-99f2-cd7c0fa6a44e",
   "metadata": {},
   "source": [
    "An even better tokenizer is the Treebank Word Tokenizer from the NLTK package, as it uses a variety of common rules for English word tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "b2329a66-7134-45cc-a60d-f6f5abc6ee08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<nltk.tokenize.treebank.TreebankWordTokenizer object at 0x13328b760>\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Monticello',\n",
       " 'was',\n",
       " \"n't\",\n",
       " 'designated',\n",
       " 'as',\n",
       " 'UNESCO',\n",
       " 'World',\n",
       " 'Heritage',\n",
       " 'Site',\n",
       " 'until',\n",
       " '1987',\n",
       " '.']"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"\"\"Monticello wasn't designated as UNESCO World Heritage Site until 1987.\"\"\"\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "print(tokenizer)\n",
    "HR()\n",
    "\n",
    "tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "d6091945-cdce-417c-bd9f-508dff538db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Monticello</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>n't</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>designated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>as</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UNESCO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>World</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Heritage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Site</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>until</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0\n",
       "0   Monticello\n",
       "1          was\n",
       "2          n't\n",
       "3   designated\n",
       "4           as\n",
       "5       UNESCO\n",
       "6        World\n",
       "7     Heritage\n",
       "8         Site\n",
       "9        until\n",
       "10        1987\n",
       "11           ."
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a prettier UI\n",
    "pd.DataFrame(tokenizer.tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c7f622-691c-420d-8dc0-11ba99925da3",
   "metadata": {},
   "source": [
    "**Tokenize informal text from social networks such as Twitter and Facebook**\n",
    "\n",
    "Problem: Social network messages often features short, informal, emoticon-filled text, where grammar and spelling conventions vary widely.\n",
    "\n",
    "Idea: Use the NLTK `casual_tokenizer` tokenizer, which is designed especially for this domain and can result in cleaner tokenization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "8a0977b3-0a14-4745-9891-ce1cedcc9ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RT',\n",
       " '@',\n",
       " 'TJMonticello',\n",
       " 'Best',\n",
       " 'day',\n",
       " 'everrrrrrr',\n",
       " 'at',\n",
       " 'Monticello.',\n",
       " 'Awesommmmmmeeeeeeee',\n",
       " 'day',\n",
       " ':',\n",
       " '*',\n",
       " ')']"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = \"\"\"RT @TJMonticello \n",
    "Best day everrrrrrr \n",
    "at Monticello. Awesommmmmmeeeeeeee day :*)\"\"\"\n",
    "\n",
    "tokenizer.tokenize(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "7d067d9b-803a-4c43-84ea-da3fe4242b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RT',\n",
       " '@TJMonticello',\n",
       " 'Best',\n",
       " 'day',\n",
       " 'everrrrrrr',\n",
       " 'at',\n",
       " 'Monticello',\n",
       " '.',\n",
       " 'Awesommmmmmeeeeeeee',\n",
       " 'day',\n",
       " ':*)']"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "casual_tokenize(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b3f108-2df1-4279-b4f6-8d2c553b67f5",
   "metadata": {},
   "source": [
    "<a name='2.2.4'></a><a id='2.2.4'></a>\n",
    "### 2.2.4 Extending your vocabulary with n-grams\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "Problem: Words separated into individual tokens is the default, but this fails in the case of compound words, since we lose the semantic meaning.\n",
    "\n",
    "Idea: Allow word vectors to keep compound words or short word sequences together via *n-grams*, which is a sequence containing up to *n* elements that have been extracted from a sequence of elements, usually a string. This can be n-grams of words and n-gram of characters. Here, we use n-grams of words. However, most 2-grams are pretty rare, and even more so for 3- and 4-grams. Because word combinations are rarer than individual words, the vocabulary sizes will exponentially approach the number of n-grams in all documents in your corpus. If the feature vector dimensionality exceeds the length of all your documents, this feature extraction step becomes counterproductive. As a response, we can document frequency statistics to identify n-grams which are rare enough to not be useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "777c8864-a024-4828-b1f1-8ed267426949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thomas',\n",
       " 'Jefferson',\n",
       " 'began',\n",
       " 'building',\n",
       " 'Monticello',\n",
       " 'at',\n",
       " 'the',\n",
       " 'age',\n",
       " 'of',\n",
       " '26']"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Original 1-gram tokenizer\n",
    "sentence = \"\"\"Thomas Jefferson began building Monticello at the age of 26.\"\"\"\n",
    "pattern = re.compile(r\"([-\\s.,;!?])+\")\n",
    "tokens = pattern.split(sentence)\n",
    "tokens = [x for x in tokens if x and x not in '- \\t\\n.,;!?']\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "0bc55a29-8989-4b9d-a4a4-61fab043eaca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Thomas', 'Jefferson'),\n",
       " ('Jefferson', 'began'),\n",
       " ('began', 'building'),\n",
       " ('building', 'Monticello'),\n",
       " ('Monticello', 'at'),\n",
       " ('at', 'the'),\n",
       " ('the', 'age'),\n",
       " ('age', 'of'),\n",
       " ('of', '26')]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n-gram tokenizer from nltk, 2-gram. This returns a Python generator.\n",
    "# Inspect all the returned n-grams at once via list or Pandas DataFrame.\n",
    "list(ngrams(tokens, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "a175eba6-b8c9-493f-a67c-b978885f63ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thomas</td>\n",
       "      <td>Jefferson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jefferson</td>\n",
       "      <td>began</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>began</td>\n",
       "      <td>building</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>building</td>\n",
       "      <td>Monticello</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Monticello</td>\n",
       "      <td>at</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>at</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>the</td>\n",
       "      <td>age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>age</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>of</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0           1\n",
       "0      Thomas   Jefferson\n",
       "1   Jefferson       began\n",
       "2       began    building\n",
       "3    building  Monticello\n",
       "4  Monticello          at\n",
       "5          at         the\n",
       "6         the         age\n",
       "7         age          of\n",
       "8          of          26"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Different UI via Pandas DataFrame\n",
    "pd.DataFrame(list(ngrams(tokens, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "e61c47ca-d841-4bee-a7d2-ba8da1389a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Thomas', 'Jefferson', 'began'),\n",
       " ('Jefferson', 'began', 'building'),\n",
       " ('began', 'building', 'Monticello'),\n",
       " ('building', 'Monticello', 'at'),\n",
       " ('Monticello', 'at', 'the'),\n",
       " ('at', 'the', 'age'),\n",
       " ('the', 'age', 'of'),\n",
       " ('age', 'of', '26')]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3-gram\n",
    "list(ngrams(tokens, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "b07ce5bb-5f6a-4592-be8a-2904fc049105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thomas</td>\n",
       "      <td>Jefferson</td>\n",
       "      <td>began</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jefferson</td>\n",
       "      <td>began</td>\n",
       "      <td>building</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>began</td>\n",
       "      <td>building</td>\n",
       "      <td>Monticello</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>building</td>\n",
       "      <td>Monticello</td>\n",
       "      <td>at</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Monticello</td>\n",
       "      <td>at</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>at</td>\n",
       "      <td>the</td>\n",
       "      <td>age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>the</td>\n",
       "      <td>age</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>age</td>\n",
       "      <td>of</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0           1           2\n",
       "0      Thomas   Jefferson       began\n",
       "1   Jefferson       began    building\n",
       "2       began    building  Monticello\n",
       "3    building  Monticello          at\n",
       "4  Monticello          at         the\n",
       "5          at         the         age\n",
       "6         the         age          of\n",
       "7         age          of          26"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(list(ngrams(tokens, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "8b5d2546-66d8-42dc-9bf6-e365b54a387a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thomas Jefferson',\n",
       " 'Jefferson began',\n",
       " 'began building',\n",
       " 'building Monticello',\n",
       " 'Monticello at',\n",
       " 'at the',\n",
       " 'the age',\n",
       " 'age of',\n",
       " 'of 26']"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n-grams above are returned as tuples.\n",
    "# If we prefer strings, use join().\n",
    "two_grams = list(ngrams(tokens, 2))\n",
    "[\" \".join(x) for x in two_grams]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605cbcc5-e882-4dc3-a950-e4f5f9afca66",
   "metadata": {},
   "source": [
    "**Stop Words**\n",
    "\n",
    "Problem: Some words occur with high frequency but carry less substantive semantic information about the phrase they are in, and can entail unnecessary computational cost during information extraction from the text.\n",
    "\n",
    "Idea: Represent these elements as \"stop words\", and exclude them from NLP pipelines. A typical stop word list contains around 100 frequent and unimportant words. The NLTK package probably contains the most canonical stop words list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "431519f5-2804-4602-a3e4-029e369f64a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['house', 'fire']"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = ['a', 'an', 'the', 'on', 'of', 'off', 'this', 'is']\n",
    "tokens = ['the', 'house', 'is', 'on', 'fire']\n",
    "tokens_without_stopwords = [x for x in tokens if x not in stop_words]\n",
    "tokens_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "3a2d18d5-bae3-433e-ac99-75c8e36a901f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "----------------------------------------\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours']\n",
      "----------------------------------------\n",
      "['i', 'a', 's', 't', 'd', 'm', 'o', 'y']\n",
      "----------------------------------------\n",
      "['me', 'my', 'we', 'he', 'it', 'am', 'is', 'be', 'do', 'an', 'if', 'or', 'as', 'of', 'at', 'by', 'to', 'up', 'in', 'on', 'no', 'so', 'll', 're', 've', 'ma']\n",
      "----------------------------------------\n",
      "['our', 'you', 'him', 'his', 'she', 'her', 'its', 'who', 'are', 'was', 'has', 'had', 'did', 'the', 'and', 'but', 'for', 'out', 'off', 'why', 'how', 'all', 'any', 'few', 'nor', 'not', 'own', 'too', 'can', 'don', 'now', 'ain', 'isn', 'won']\n"
     ]
    }
   ],
   "source": [
    "# Canonical list of stop words from NLTK\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "print(len(stop_words))\n",
    "HR()\n",
    "\n",
    "print(stop_words[:7])\n",
    "HR()\n",
    "\n",
    "print([sw for sw in stop_words if len(sw) == 1])\n",
    "HR() \n",
    "\n",
    "print([sw for sw in stop_words if len(sw) == 2])\n",
    "HR() \n",
    "\n",
    "print([sw for sw in stop_words if len(sw) == 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "cbbd3cab-20a9-44e9-8331-51113b60a63f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             i\n",
       "1            me\n",
       "2            my\n",
       "3        myself\n",
       "4            we\n",
       "         ...   \n",
       "174     weren't\n",
       "175         won\n",
       "176       won't\n",
       "177      wouldn\n",
       "178    wouldn't\n",
       "Length: 179, dtype: object"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw_series = pd.Series(stop_words)\n",
    "sw_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "eea67260-a717-4730-b647-41d58ec01f4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "750d850450c74c25b1464db581e2cb0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "progress-bar:   0%|          | 0/179 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      2\n",
       "2      2\n",
       "3      6\n",
       "4      2\n",
       "      ..\n",
       "174    7\n",
       "175    3\n",
       "176    5\n",
       "177    6\n",
       "178    8\n",
       "Length: 179, dtype: int64"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rst = sw_series.progress_map(lambda calc: len(calc))\n",
    "rst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "82460deb-4240-4073-8df9-73c320ba83bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = { 'stopWords': stop_words, 'sw_num': rst }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "ad655f2d-1cbd-4592-8a08-37da66589030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stopWords</th>\n",
       "      <th>sw_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>me</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>myself</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>we</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>weren't</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>won</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>won't</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>wouldn</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>wouldn't</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>179 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    stopWords  sw_num\n",
       "0           i       1\n",
       "1          me       2\n",
       "2          my       2\n",
       "3      myself       6\n",
       "4          we       2\n",
       "..        ...     ...\n",
       "174   weren't       7\n",
       "175       won       3\n",
       "176     won't       5\n",
       "177    wouldn       6\n",
       "178  wouldn't       8\n",
       "\n",
       "[179 rows x 2 columns]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw_df = pd.DataFrame(frame)\n",
    "sw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "b5920db6-8b48-4201-b1d3-9a9bc78ce020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sw_df.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "d636633e-a8e2-4783-b340-9bf6c67585c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "318"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comparison with sklearn stop words\n",
    "\n",
    "len(sklearn_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "d8ec463c-0748-4c45-a596-1e8d8af73d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318\n",
      "179\n",
      "----------------------------------------\n",
      "378\n",
      "----------------------------------------\n",
      "119\n"
     ]
    }
   ],
   "source": [
    "# Convert to set so we can use union and intersection operations\n",
    "s1_sk = set(sklearn_stop_words)\n",
    "s2_sw = set(stop_words)\n",
    "\n",
    "print(len(s1_sk))\n",
    "print(len(s2_sw))\n",
    "HR()\n",
    "\n",
    "# NTLK's list contains 60 stop words that aren't in the larger sklearn set.\n",
    "print(len(s2_sw.union(s1_sk)))\n",
    "HR()\n",
    "\n",
    "print(len(s2_sw.intersection(s1_sk)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaebd8a-c91d-4f90-9a84-2dd262ba9158",
   "metadata": {},
   "source": [
    "<a name='2.2.5'></a><a id='2.2.5'></a>\n",
    "### 2.2.5 Normalizing your vocabulary\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "Problem: We still need to reduce vocabulary size to improve the performance of a NLP pipeline and reduce overfitting.\n",
    "\n",
    "Idea: We can \"normalize\" the corpus vocabulary, so that tokens meaning similar things are combined into a single, normalized form. This results in less tokens necessary, and improves the association of meaning across the different \"spellings\" of a token or n-gram. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00b8bc3-cd70-43f5-9efe-cb1f4959d6f5",
   "metadata": {},
   "source": [
    "**Case Folding**\n",
    "\n",
    "Problem: Some words feature multiple spellings and differ only in their capitalization.\n",
    "\n",
    "Idea: *Case folding* or *case normalization* is a technique to return tokens to their normal state before grammar rules and position in the sentence affects their capitalization. Case normalizgin is particulary useful for search engines, since it increases \"recall.\" However, there is a potential loss of information, or \"precision\", and many NLP pipelines do not normalize case at all. For many applications, the efficiency gain (in storage and processing) for reducing the corpus vocabulary by half is outweighed by the loss of semantic information for proper nouns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "ab5ee7fe-1872-41d2-99cb-e86f9ac53da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['House', 'Visitor', 'Center']\n",
      "----------------------------------------\n",
      "['house', 'visitor', 'center']\n"
     ]
    }
   ],
   "source": [
    "# Normalize the capitalization of tokens with a list comprehension\n",
    "tokens = ['House', 'Visitor', 'Center']\n",
    "print(tokens)\n",
    "HR() \n",
    "\n",
    "normalized_tokens = [x.lower() for x in tokens]\n",
    "print(normalized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a41f558-f22d-4af2-9739-79d6237cc07c",
   "metadata": {},
   "source": [
    "**Stemming**\n",
    "\n",
    "Problem: Some words feature only small differences of pluralization, or possessive endings of words, or various verb forms.\n",
    "\n",
    "Idea: *Stemming* is a normalization technique that identifies a common stem among the various forms of a word. Stemming removes suffixes from words. A stem is not required to be a properly spelled word, but merely a token that represents several possible word spellings. Stemming is important for keyword search or information retrieval. However, it greatly reduces the \"precision\" score, and sometimes increases the \"false-positive\" rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "a09c4c33-e5de-4b76-a5db-d9fdd36c8540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "house\n",
      "doctor house call\n"
     ]
    }
   ],
   "source": [
    "# Simple stemmer implemented in Python\n",
    "def stem(phrase):\n",
    "    return ' '.join([re.findall('^(.*ss|.*?)(s)?$', word)[0][0].strip(\"'\") for word in phrase.lower().split()])\n",
    "\n",
    "print(stem('houses'))\n",
    "print(stem(\"Doctor House's calls\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "55693b79-1a99-436f-bb04-1d781307ce50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dish washer's washed dishes\n",
      "----------------------------------------\n",
      "dish washer wash dish\n"
     ]
    }
   ],
   "source": [
    "# NLTK Porter stemmer\n",
    "stemmer = PorterStemmer()\n",
    "sentence = \"dish washer's washed dishes\"\n",
    "print(sentence)\n",
    "HR()\n",
    "result = ' '.join([stemmer.stem(w).strip(\"'\") for w in sentence.split()])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd93ba36-8e17-4ce1-b24f-25f7e1b1700d",
   "metadata": {},
   "source": [
    "**Lemmatization**\n",
    "\n",
    "Problem: Some words contain semantic connections even though their spelling is different.\n",
    "\n",
    "Idea: We can normalize words in this case by using lemmatization, which is extracts the semantic root of these words. Lemmatization is possibly a more accurate technique of normalization than stemming or case normalization because it encompasses semantic meaning. A lemmatizer uses a knowledge base of word synonyms and word endings to ensure only words that mean similar things are consolidated into a single token. Some lemmatizers also use the word's part of speech (POS) tag to help improve accuracy. In general, lemmatizers are better than stemmers for most applications. Th NLTK package provides popular lemmatizer operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "78da4f0c-9fbb-447f-a562-1ac119a75ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/gb/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "58717c8d-d3e6-4a29-9298-36870e9bbd73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'better'"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"better\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "57615380-36aa-4c53-9f24-411326c5ecf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"better\", pos=\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "282415a6-4a56-48e2-ad9f-bb717578261d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"good\", pos=\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "42306fa2-9717-4a59-a411-d7dab6d4890f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'goods'"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"goods\", pos=\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "38eb1816-dadb-4985-9c7c-b2aded10945b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"goods\", pos=\"n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "6ba13ec9-fad8-433b-ba93-25071f04a3ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'goodness'"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"goodness\", pos=\"n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "d52f01ca-1a3a-456b-8fe2-3e62d59909ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'best'"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"best\", pos=\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7940a3-fbec-4fd3-ae36-6a8f34a72929",
   "metadata": {},
   "source": [
    "**Use Cases**\n",
    "\n",
    "Problem: When should you use a lemmatizer or a stemmer?\n",
    "\n",
    "Idea: In general, avoid stemming or lemmatization unless you have a limited amount of text that contains usage and capitalizations. This case may include documents with a lot of jargon, or are from a specialized subfield of science, technology, literature, etc. Usually, most NLP datasets are very big and do not require stemming or lemmatization. However, lemmatization may be useful for non-English languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ece0815-7734-47e2-80df-e999ce58b4f6",
   "metadata": {},
   "source": [
    "<a name='2.3'></a><a id='2.3'></a>\n",
    "## 2.3 Sentiment\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "Problem: Text may express a mood or sentiment and a complete understanding of such text requires measurement of its polarity.\n",
    "\n",
    "Idea: Use sentiment analysis to systematically identify, extract, and quantify the polarity and subjective state of text. We can use either a rule-based algorithm, or a machine learning model which learns data algorithmically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4247fe-6dbf-441a-8347-2813b2210d0c",
   "metadata": {},
   "source": [
    "<a name='2.3.1'></a><a id='2.3.1'></a>\n",
    "### 2.3.1 VADER - A rule-based sentiment analyzer\n",
    "\n",
    "**Valence Aware Dictionary for Sentiment Reasoning**\n",
    "\n",
    "* https://github.com/cjhutto/vaderSentiment\n",
    "* https://ojs.aaai.org/index.php/ICWSM/article/view/14550\n",
    "\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "Problem: There are certain use-cases where specialized knowledge may be useful in determining sentiment.\n",
    "\n",
    "Idea: Use a specialized lexicon, or dictionary that contain a collection of words and compiled using expert knowledge, having been collected for a specific purpose. One such lexicon is Valence Aware Dictionary for sEntiment Reasoning (VADER). This is available in the NLTK package as `nltk.sentiment.vader`. VADER is sensitive to both polarity and intensity of emotion. VADER was specifically designed to analyze social media content. A disadvantage of VADER is that it does not look at all words in a corpus, only about 7,500 words. To add more words, you must add to its dictionary in `SentimentIntensityAnalyzer.lexicon`, which also entails a prior knowledge of the corpus text. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "7939ce0d-c8d1-4402-a74d-b0c81ab6f5b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'beauteousness': 2.7,\n",
       " 'beautician': 1.2,\n",
       " 'beauticians': 0.4,\n",
       " 'beauties': 2.4,\n",
       " 'beautification': 1.9,\n",
       " 'beautifications': 2.4,\n",
       " 'beautified': 2.1,\n",
       " 'beautifier': 1.7,\n",
       " 'beautifiers': 1.7,\n",
       " 'beautifies': 1.8}"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa = SentimentIntensityAnalyzer()\n",
    "\n",
    "# only show a portion\n",
    "dict(list(sa.lexicon.items())[1000:1010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "7ddec4b0-fcb4-422a-a2f3-cefb3d3239c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame([(tok, score) for tok, score in sa.lexicon.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "90bac306-b33a-4bc4-9c66-2f238da666d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$:</td>\n",
       "      <td>-1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>%)</td>\n",
       "      <td>-0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>%-)</td>\n",
       "      <td>-1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&amp;-:</td>\n",
       "      <td>-0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&amp;:</td>\n",
       "      <td>-0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>( '}{' )</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(%</td>\n",
       "      <td>-0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>('-:</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(':</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>((-:</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0    1\n",
       "0        $: -1.5\n",
       "1        %) -0.4\n",
       "2       %-) -1.5\n",
       "3       &-: -0.4\n",
       "4        &: -0.7\n",
       "5  ( '}{' )  1.6\n",
       "6        (% -0.9\n",
       "7      ('-:  2.2\n",
       "8       (':  2.3\n",
       "9      ((-:  2.1"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "8a9f1945-f6df-447b-b46c-2e9f3bad3e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"( '}{' )\", 1.6),\n",
       " (\"can't stand\", -2.0),\n",
       " ('fed up', -1.8),\n",
       " ('screwed up', -1.5)]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for tokens containing spaces\n",
    "[(tok, score) for tok, score in sa.lexicon.items() if \" \" in tok]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "a9bf70bc-65b7-427a-8190-bb00565d2d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.661, 'pos': 0.339, 'compound': 0.6249}"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa.polarity_scores(text=\"Python is very readable and it's great for NLP.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "74933685-fbf4-4487-abb1-a6e313b51acd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.737, 'pos': 0.263, 'compound': 0.431}"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa.polarity_scores(text=\"Python is not a bad choice for most applications.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "c092811d-6efa-4e28-ac97-df1a2629c0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+0.9428: Absolutely perfect! Love it! :-) :-) :-)\n",
      "-0.8768: Horrible! Completely useless. :(\n",
      "-0.1531: It was OK. Some good and some bad things.\n"
     ]
    }
   ],
   "source": [
    "# Check how this rule-based approach works for our example statements.\n",
    "corpus = [\n",
    "    \"Absolutely perfect! Love it! :-) :-) :-)\",\n",
    "    \"Horrible! Completely useless. :(\",\n",
    "    \"It was OK. Some good and some bad things.\"\n",
    "]\n",
    "\n",
    "for doc in corpus:\n",
    "    scores = sa.polarity_scores(doc)\n",
    "    print('{:+}: {}'.format(scores['compound'], doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604d8c4a-0ba6-41f3-97c6-6dc08a35b000",
   "metadata": {},
   "source": [
    "<a name='2.3.2'></a><a id='2.3.2'></a>\n",
    "### 2.3.2 Naive Bayes\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "Problem: There are cases where hand-coding algorithms is not efficient nor effective in sentiment analysis.\n",
    "\n",
    "Idea: Use a Naive Bayes machine learning model to  automatically find keywords in the corpus that are predictive of the target variable. In the Naive Bayes model, the internal coefficients will map words or tokens to scores, as VADER does, which can be useful for explainability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14bc7b7-396d-44d0-bd11-1ec0940f7d67",
   "metadata": {},
   "source": [
    "<a id='movieReviewSnippets_GroundTruth.csv.gz'></a><a name='movieReviewSnippets_GroundTruth.csv.gz'></a>\n",
    "### Dataset: movieReviewSnippets_GroundTruth.csv.gz\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "e1d35eb4-57fe-421a-87aa-aeffe9333b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File data/data_movie_review/movieReviewSnippets_GroundTruth.csv.gz already there; not retrieving.\n",
      "\n",
      "-rw-r--r--  1 gb  staff  565697 Mar 25 11:08 data/data_movie_review/movieReviewSnippets_GroundTruth.csv.gz\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'data/data_movie_review'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "          \n",
    "data_movie_review = f\"{data_dir}/movieReviewSnippets_GroundTruth.csv.gz\"\n",
    "!wget -P {data_dir} -nc https://github.com/totalgood/nlpia/raw/master/src/nlpia/data/hutto_ICWSM_2014/movieReviewSnippets_GroundTruth.csv.gz\n",
    "!ls -l {data_movie_review}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "36bf3ff5-7f24-4222-8659-15ee5f16a257",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv(data_movie_review, compression=\"infer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "e5a2fc7e-761a-4085-a987-91e9405c5e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10605 entries, 0 to 10604\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   id         10605 non-null  int64  \n",
      " 1   sentiment  10605 non-null  float64\n",
      " 2   text       10605 non-null  object \n",
      "dtypes: float64(1), int64(1), object(1)\n",
      "memory usage: 248.7+ KB\n"
     ]
    }
   ],
   "source": [
    "movies.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "8367bc28-b190-4f99-92f3-c1bcec43caa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2.27</td>\n",
       "      <td>The Rock is destined to be the 21st Century's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3.53</td>\n",
       "      <td>The gorgeously elaborate continuation of ''The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>Effective but too tepid biopic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.47</td>\n",
       "      <td>If you sometimes like to go to the movies to h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1.73</td>\n",
       "      <td>Emerges as something rare, an issue movie that...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  sentiment                                               text\n",
       "0   1       2.27  The Rock is destined to be the 21st Century's ...\n",
       "1   2       3.53  The gorgeously elaborate continuation of ''The...\n",
       "2   3      -0.60                     Effective but too tepid biopic\n",
       "3   4       1.47  If you sometimes like to go to the movies to h...\n",
       "4   5       1.73  Emerges as something rare, an issue movie that..."
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.head().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "d2809028-9fe0-4ba4-b6af-9b1e92f6fcf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10605.00</td>\n",
       "      <td>10605.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5303.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3061.54</td>\n",
       "      <td>1.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.00</td>\n",
       "      <td>-3.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2652.00</td>\n",
       "      <td>-1.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5303.00</td>\n",
       "      <td>-0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7954.00</td>\n",
       "      <td>1.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10605.00</td>\n",
       "      <td>3.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  sentiment\n",
       "count  10605.00   10605.00\n",
       "mean    5303.00       0.00\n",
       "std     3061.54       1.92\n",
       "min        1.00      -3.88\n",
       "25%     2652.00      -1.77\n",
       "50%     5303.00      -0.08\n",
       "75%     7954.00       1.83\n",
       "max    10605.00       3.94"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04a39f9-2def-4b65-b04c-9812dfe70063",
   "metadata": {},
   "source": [
    "Tokenize these movies reviews to create a bag of words for each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "85a6d48c-a53c-4f67-82a4-399bd62d0826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10605"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bags_of_words = []\n",
    "\n",
    "for text in movies.text:\n",
    "    bags_of_words.append(Counter(casual_tokenize(text)))\n",
    "    \n",
    "len(bags_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "fca4f260-f0a3-43ba-a357-fe09826f1f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "End\n",
      "CPU times: user 56.7 s, sys: 8.3 s, total: 1min 4s\n",
      "Wall time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Creates a DataFrame object from a structured ndarray, sequence of tuples or dicts, or DataFrame.\n",
    "print(\"Start\")\n",
    "\n",
    "# The from_records() DataFrame constructor takes a sequence of dictionaries. \n",
    "# It creates columns for all the keys, and the values are added to the table \n",
    "# in the appropriate columns, filling missing values with NaN\n",
    "df_bows = pd.DataFrame.from_records(bags_of_words)\n",
    "print(\"End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "9e6d2272-52ba-4c9d-8808-331b6cf52641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "End\n",
      "CPU times: user 2.56 s, sys: 3.41 s, total: 5.97 s\n",
      "Wall time: 5.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Start\")\n",
    "df_bows = df_bows.fillna(0).astype(int)\n",
    "print(\"End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "56a76206-4f93-4101-b0fb-bf17da8b7553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10605, 20756)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "5c8d0c7e-7caf-4bd3-9206-ddce59f03c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>The</th>\n",
       "      <th>Rock</th>\n",
       "      <th>is</th>\n",
       "      <th>destined</th>\n",
       "      <th>to</th>\n",
       "      <th>be</th>\n",
       "      <th>the</th>\n",
       "      <th>21st</th>\n",
       "      <th>Century's</th>\n",
       "      <th>new</th>\n",
       "      <th>...</th>\n",
       "      <th>Ill</th>\n",
       "      <th>slummer</th>\n",
       "      <th>Rashomon</th>\n",
       "      <th>dipsticks</th>\n",
       "      <th>Bearable</th>\n",
       "      <th>Staggeringly</th>\n",
       "      <th></th>\n",
       "      <th>ve</th>\n",
       "      <th>muttering</th>\n",
       "      <th>dissing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  20756 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   The  Rock  is  destined  to  be  the  21st  Century's  new  ...  Ill  \\\n",
       "0    1     1   1         1   2   1    1     1          1    1  ...    0   \n",
       "1    2     0   1         0   0   0    1     0          0    0  ...    0   \n",
       "2    0     0   0         0   0   0    0     0          0    0  ...    0   \n",
       "3    0     0   1         0   4   0    1     0          0    0  ...    0   \n",
       "4    0     0   0         0   0   0    0     0          0    0  ...    0   \n",
       "\n",
       "   slummer  Rashomon  dipsticks  Bearable  Staggeringly    ve  muttering  \\\n",
       "0        0         0          0         0             0  0   0          0   \n",
       "1        0         0          0         0             0  0   0          0   \n",
       "2        0         0          0         0             0  0   0          0   \n",
       "3        0         0          0         0             0  0   0          0   \n",
       "4        0         0          0         0             0  0   0          0   \n",
       "\n",
       "   dissing  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  \n",
       "\n",
       "[5 rows x 20756 columns]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bows.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "f22e3f36-2ee1-4dc8-948d-dbce2db8d7d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>The</th>\n",
       "      <th>Rock</th>\n",
       "      <th>is</th>\n",
       "      <th>destined</th>\n",
       "      <th>to</th>\n",
       "      <th>be</th>\n",
       "      <th>the</th>\n",
       "      <th>21st</th>\n",
       "      <th>Century's</th>\n",
       "      <th>new</th>\n",
       "      <th>...</th>\n",
       "      <th>Schwarzenegger</th>\n",
       "      <th>,</th>\n",
       "      <th>Jean</th>\n",
       "      <th>Claud</th>\n",
       "      <th>Van</th>\n",
       "      <th>Damme</th>\n",
       "      <th>or</th>\n",
       "      <th>Steven</th>\n",
       "      <th>Segal</th>\n",
       "      <th>.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   The  Rock  is  destined  to  be  the  21st  Century's  new  ...  \\\n",
       "0    1     1   1         1   2   1    1     1          1    1  ...   \n",
       "1    2     0   1         0   0   0    1     0          0    0  ...   \n",
       "2    0     0   0         0   0   0    0     0          0    0  ...   \n",
       "3    0     0   1         0   4   0    1     0          0    0  ...   \n",
       "4    0     0   0         0   0   0    0     0          0    0  ...   \n",
       "\n",
       "   Schwarzenegger  ,  Jean  Claud  Van  Damme  or  Steven  Segal  .  \n",
       "0               1  1     1      1    1      1   1       1      1  1  \n",
       "1               0  0     0      0    0      0   0       0      0  4  \n",
       "2               0  0     0      0    0      0   0       0      0  0  \n",
       "3               0  1     0      0    0      0   0       0      0  1  \n",
       "4               0  1     0      0    0      0   0       0      0  1  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bows.head()[list(bags_of_words[0].keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "3e8ad99f-94b0-40d3-95d0-00eef8b37211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10605, 20756)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bows.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fe728c-8cd7-4c27-8686-9391799e60c1",
   "metadata": {},
   "source": [
    "We now have all the data in a data-structure that Naive Bayes model needs to find the keywords that predict sentiment from natural language text.\n",
    "\n",
    "Note that Naive Bayes models are **classifiers**, so we need to convert our output variable (sentiment float) to a discrete label (integer, string, or bool)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "5165b89a-92dd-4b2a-ae62-5c199cdd33bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb = nb.fit(df_bows, movies.sentiment > 0)\n",
    "nb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d9fda3-3bf0-4a01-afa0-bfe18f57398d",
   "metadata": {},
   "source": [
    "Error, as per https://github.com/totalgood/nlpia/issues/33\n",
    "\n",
    "`movies['predicted_sentiment'] = nb.predict_proba(df_bows) * 8 - 4`\n",
    "\n",
    "Convert your binary classification variable (0 or 1) to -4 or 4 so you \n",
    "can compare it to the ground truth sentiment. Use `nb.predict_proba` \n",
    "to get a continuous value.\n",
    "\n",
    "`nb.predict_proba` returns probability estimates for the test vector X.\n",
    "This return an array-like of shape (n_samples, n_classes).\n",
    "However, we only want the n_classes.\n",
    "Use `[:, 1]` to specify "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "524cd0c0-ddd1-4f59-bf4b-0cb387fb824e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2.511515\n",
       "1        3.999904\n",
       "2       -3.655976\n",
       "3        1.940954\n",
       "4        3.910373\n",
       "           ...   \n",
       "10600   -3.166489\n",
       "10601   -1.056805\n",
       "10602   -1.481449\n",
       "10603    3.988988\n",
       "10604   -3.997954\n",
       "Name: predicted_sentiment, Length: 10605, dtype: float64"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies['predicted_sentiment'] = nb.predict_proba(df_bows)[:, 1] * 8 - 4\n",
    "movies['predicted_sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "1687f331-40a1-4af4-937c-e7257f738366",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['error'] = (movies.predicted_sentiment - movies.sentiment).abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "a7c2e585-4439-41e3-9b16-f7805b03b37f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.error.mean().round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "8d5805d0-63b6-4a26-83c6-3147135246a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['sentiment_ispositive'] = (movies.sentiment > 0).astype(int)\n",
    "movies['predicted_ispositive'] = (movies.predicted_sentiment > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "64a5be96-5e50-441b-9cc1-d4ddec9715f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'sentiment', 'text', 'predicted_sentiment', 'error',\n",
       "       'sentiment_ispositive', 'predicted_ispositive'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "538b9dfd-f4f2-4a0a-be7d-b73b7f44f77d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>predicted_sentiment</th>\n",
       "      <th>sentiment_ispositive</th>\n",
       "      <th>predicted_ispositive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.266667</td>\n",
       "      <td>2.511515</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.533333</td>\n",
       "      <td>3.999904</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.600000</td>\n",
       "      <td>-3.655976</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.466667</td>\n",
       "      <td>1.940954</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.733333</td>\n",
       "      <td>3.910373</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.533333</td>\n",
       "      <td>3.995188</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.466667</td>\n",
       "      <td>3.960466</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.266667</td>\n",
       "      <td>-1.918701</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment  predicted_sentiment  sentiment_ispositive  predicted_ispositive\n",
       "0   2.266667             2.511515                     1                     1\n",
       "1   3.533333             3.999904                     1                     1\n",
       "2  -0.600000            -3.655976                     0                     0\n",
       "3   1.466667             1.940954                     1                     1\n",
       "4   1.733333             3.910373                     1                     1\n",
       "5   2.533333             3.995188                     1                     1\n",
       "6   2.466667             3.960466                     1                     1\n",
       "7   1.266667            -1.918701                     1                     0"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies['''sentiment predicted_sentiment sentiment_ispositive predicted_ispositive'''.split()].head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "2d434525-1069-483f-acb5-24fad6f4aa40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9344648750589345"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(movies.predicted_ispositive == movies.sentiment_ispositive).sum() / len(movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21eda84-b24f-4cd1-9341-7f67f6d3de49",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Extra\n",
    "\n",
    "In order to build a real sentiment analyzer like this, we should split the training data, and leave out the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f808330f-1c7c-4e08-858f-72e63233ce95",
   "metadata": {},
   "source": [
    "<a id='amazonReviewSnippets_GroundTruth.csv.gz'></a><a name='amazonReviewSnippets_GroundTruth.csv.gz'></a>\n",
    "### Dataset: amazonReviewSnippets_GroundTruth.csv.gz\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "a6d92932-fb79-4017-b3f1-fa6e6289c2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File data/data_amazonReviewSnippets_review/amazonReviewSnippets_GroundTruth.csv.gz already there; not retrieving.\n",
      "\n",
      "-rw-r--r--  1 gb  staff  136281 Mar 25 11:10 data/data_amazonReviewSnippets_review/amazonReviewSnippets_GroundTruth.csv.gz\n"
     ]
    }
   ],
   "source": [
    "data_amazon_dir = 'data/data_amazonReviewSnippets_review'\n",
    "if not os.path.exists(data_amazon_dir):\n",
    "    os.makedirs(data_amazon_dir)\n",
    "          \n",
    "data_amazon_review = f\"{data_amazon_dir}/amazonReviewSnippets_GroundTruth.csv.gz\"\n",
    "!wget -P {data_amazon_dir} -nc https://github.com/totalgood/nlpia/raw/master/src/nlpia/data/hutto_ICWSM_2014/amazonReviewSnippets_GroundTruth.csv.gz\n",
    "!ls -l {data_amazon_review}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "975d8770-8bab-4462-b1db-9de39d9b882d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>troubleshooting</th>\n",
       "      <th>ad</th>\n",
       "      <th>-</th>\n",
       "      <th>2500</th>\n",
       "      <th>and</th>\n",
       "      <th>2600</th>\n",
       "      <th>no</th>\n",
       "      <th>picture</th>\n",
       "      <th>scrolling</th>\n",
       "      <th>b</th>\n",
       "      <th>...</th>\n",
       "      <th>undone</th>\n",
       "      <th>warrranty</th>\n",
       "      <th>expire</th>\n",
       "      <th>expired</th>\n",
       "      <th>voids</th>\n",
       "      <th>develops</th>\n",
       "      <th>soldier</th>\n",
       "      <th>serving</th>\n",
       "      <th>baghdad</th>\n",
       "      <th>harddisk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  5687 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   troubleshooting  ad  -  2500  and  2600  no  picture  scrolling  b  ...  \\\n",
       "0                1   2  2     1    1     1   1        1          1  1  ...   \n",
       "1                0   0  0     0    0     0   0        0          0  0  ...   \n",
       "2                0   0  0     0    0     0   0        0          0  0  ...   \n",
       "3                0   0  0     0    2     0   0        0          1  0  ...   \n",
       "4                1   0  0     0    0     0   0        0          0  0  ...   \n",
       "\n",
       "   undone  warrranty  expire  expired  voids  develops  soldier  serving  \\\n",
       "0       0          0       0        0      0         0        0        0   \n",
       "1       0          0       0        0      0         0        0        0   \n",
       "2       0          0       0        0      0         0        0        0   \n",
       "3       0          0       0        0      0         0        0        0   \n",
       "4       0          0       0        0      0         0        0        0   \n",
       "\n",
       "   baghdad  harddisk  \n",
       "0        0         0  \n",
       "1        0         0  \n",
       "2        0         0  \n",
       "3        0         0  \n",
       "4        0         0  \n",
       "\n",
       "[5 rows x 5687 columns]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part 1\n",
    "\n",
    "# hutto_products: amazonReviewSnippets_GroundTruth.csv.gz\n",
    "products = pd.read_csv(data_amazon_review, compression=\"infer\")\n",
    "\n",
    "bags_of_words = []\n",
    "\n",
    "for text in products.text:\n",
    "    bags_of_words.append(Counter(casual_tokenize(text)))\n",
    "\n",
    "df_product_bows = pd.DataFrame.from_records(bags_of_words)\n",
    "df_product_bows = df_product_bows.fillna(0).astype(int)\n",
    "df_product_bows.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "b8cada5b-5e61-4d30-a6a2-df37766e6594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['The', 'Rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'Century's',\n",
      "       'new',\n",
      "       ...\n",
      "       'sligtly', 'owner', '81', 'defectively', 'warrranty', 'expire',\n",
      "       'expired', 'voids', 'baghdad', 'harddisk'],\n",
      "      dtype='object', length=23302)\n",
      "----------------------------------------\n",
      "['The' 'Rock' 'is' ... 'voids' 'baghdad' 'harddisk']\n",
      "----------------------------------------\n",
      "(3546, 20756)\n",
      "(10605, 20756)\n"
     ]
    }
   ],
   "source": [
    "df_all_bows = df_bows.append(df_product_bows)\n",
    "print(df_all_bows.columns)\n",
    "HR()\n",
    "\n",
    "df_product_bows = df_all_bows.iloc[len(movies):][df_bows.columns]\n",
    "print(df_all_bows.columns.values)\n",
    "HR()\n",
    "\n",
    "df_product_bows = df_all_bows.iloc[len(movies):][df_bows.columns]\n",
    "print(df_product_bows.shape)\n",
    "print(df_bows.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "b0b38358-dd92-4884-a396-9281dbd0d167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>The</th>\n",
       "      <th>Rock</th>\n",
       "      <th>is</th>\n",
       "      <th>destined</th>\n",
       "      <th>to</th>\n",
       "      <th>be</th>\n",
       "      <th>the</th>\n",
       "      <th>21st</th>\n",
       "      <th>Century's</th>\n",
       "      <th>new</th>\n",
       "      <th>...</th>\n",
       "      <th>Ill</th>\n",
       "      <th>slummer</th>\n",
       "      <th>Rashomon</th>\n",
       "      <th>dipsticks</th>\n",
       "      <th>Bearable</th>\n",
       "      <th>Staggeringly</th>\n",
       "      <th></th>\n",
       "      <th>ve</th>\n",
       "      <th>muttering</th>\n",
       "      <th>dissing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  20756 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   The  Rock  is  destined  to  be  the  21st  Century's  new  ...  Ill  \\\n",
       "0  NaN   NaN   0       NaN   0   0    0     0        NaN    0  ...  NaN   \n",
       "1  NaN   NaN   0       NaN   0   0    0     0        NaN    0  ...  NaN   \n",
       "2  NaN   NaN   0       NaN   0   0    0     0        NaN    0  ...  NaN   \n",
       "3  NaN   NaN   0       NaN   0   0    0     0        NaN    0  ...  NaN   \n",
       "4  NaN   NaN   0       NaN   1   0    2     0        NaN    0  ...  NaN   \n",
       "\n",
       "   slummer  Rashomon  dipsticks  Bearable  Staggeringly     ve  muttering  \\\n",
       "0      NaN       NaN        NaN       NaN           NaN NaN NaN        NaN   \n",
       "1      NaN       NaN        NaN       NaN           NaN NaN NaN        NaN   \n",
       "2      NaN       NaN        NaN       NaN           NaN NaN NaN        NaN   \n",
       "3      NaN       NaN        NaN       NaN           NaN NaN NaN        NaN   \n",
       "4      NaN       NaN        NaN       NaN           NaN NaN NaN        NaN   \n",
       "\n",
       "   dissing  \n",
       "0      NaN  \n",
       "1      NaN  \n",
       "2      NaN  \n",
       "3      NaN  \n",
       "4      NaN  \n",
       "\n",
       "[5 rows x 20756 columns]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_product_bows.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "2579a650-abe8-4264-9947-adb75017d646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>The</th>\n",
       "      <th>Rock</th>\n",
       "      <th>is</th>\n",
       "      <th>destined</th>\n",
       "      <th>to</th>\n",
       "      <th>be</th>\n",
       "      <th>the</th>\n",
       "      <th>21st</th>\n",
       "      <th>Century's</th>\n",
       "      <th>new</th>\n",
       "      <th>...</th>\n",
       "      <th>Ill</th>\n",
       "      <th>slummer</th>\n",
       "      <th>Rashomon</th>\n",
       "      <th>dipsticks</th>\n",
       "      <th>Bearable</th>\n",
       "      <th>Staggeringly</th>\n",
       "      <th></th>\n",
       "      <th>ve</th>\n",
       "      <th>muttering</th>\n",
       "      <th>dissing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  20756 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   The  Rock  is  destined  to  be  the  21st  Century's  new  ...  Ill  \\\n",
       "0    0     0   0         0   0   0    0     0          0    0  ...    0   \n",
       "1    0     0   0         0   0   0    0     0          0    0  ...    0   \n",
       "2    0     0   0         0   0   0    0     0          0    0  ...    0   \n",
       "3    0     0   0         0   0   0    0     0          0    0  ...    0   \n",
       "4    0     0   0         0   1   0    2     0          0    0  ...    0   \n",
       "\n",
       "   slummer  Rashomon  dipsticks  Bearable  Staggeringly    ve  muttering  \\\n",
       "0        0         0          0         0             0  0   0          0   \n",
       "1        0         0          0         0             0  0   0          0   \n",
       "2        0         0          0         0             0  0   0          0   \n",
       "3        0         0          0         0             0  0   0          0   \n",
       "4        0         0          0         0             0  0   0          0   \n",
       "\n",
       "   dissing  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  \n",
       "\n",
       "[5 rows x 20756 columns]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Need to fix nan again\n",
    "df_product_bows = df_product_bows.fillna(0).astype(int)\n",
    "df_product_bows.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "477526b3-674f-4ef2-aef3-7f7025f4a2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2\n",
    "products['ispos'] = (products.sentiment > 0).astype(int)\n",
    "products['pred'] = nb.predict(df_product_bows.values).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "1da17472-4449-41b4-9c43-af1a1bf58991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>ispos</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_1</td>\n",
       "      <td>-0.90</td>\n",
       "      <td>troubleshooting ad-2500 and ad-2600 no picture...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_2</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>repost from january 13, 2004 with a better fit...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_3</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>does your apex dvd player only play dvd audio ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_4</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>or does it play audio and video but scrolling ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_5</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>before you try to return the player or waste h...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  sentiment                                               text  ispos  \\\n",
       "0  1_1      -0.90  troubleshooting ad-2500 and ad-2600 no picture...      0   \n",
       "1  1_2      -0.15  repost from january 13, 2004 with a better fit...      0   \n",
       "2  1_3      -0.20  does your apex dvd player only play dvd audio ...      0   \n",
       "3  1_4      -0.10  or does it play audio and video but scrolling ...      0   \n",
       "4  1_5      -0.50  before you try to return the player or waste h...      0   \n",
       "\n",
       "   pred  \n",
       "0     0  \n",
       "1     0  \n",
       "2     0  \n",
       "3     0  \n",
       "4     0  "
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "4eee9410-d615-4589-b659-41b21694313f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5572476029328821"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(products.pred == products.ispos).sum() / len(products)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35e33fc-5361-4993-8fb8-200f7c939e35",
   "metadata": {},
   "source": [
    "Note: While using the Naive Bayes model, we can use the VADER as a simple benchmark, checking whether a ML approach is better than hard-coded heuristics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3814e520-6018-4846-a1e1-35b5d4138526",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

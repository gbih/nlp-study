{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XrBZIgb6_o32"
   },
   "source": [
    "# Character-level recurrent sequence-to-sequence model\n",
    "\n",
    "https://keras.io/examples/nlp/lstm_seq2seq/\n",
    "\n",
    "**Author:** [fchollet](https://twitter.com/fchollet)<br>\n",
    "**Date created:** 2017/09/29<br>\n",
    "**Last modified:** 2020/04/26<br>\n",
    "**Description:** Character-level recurrent sequence-to-sequence model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YpvROkltz6yE"
   },
   "source": [
    "**Original:**\n",
    "    \n",
    "A ten-minute introduction to sequence-to-sequence learning in Keras\n",
    "    \n",
    "* https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
    "* https://github.com/rstudio/keras/blob/main/vignettes/examples/lstm_seq2seq.py\n",
    "* https://github.com/CosineP/cw-lstm/blob/master/lstm_seq2seq.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N44qTebU_o35"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This example demonstrates how to implement a basic character-level\n",
    "recurrent sequence-to-sequence model. We apply it to translating\n",
    "short English sentences into short French sentences,\n",
    "character-by-character. Note that it is fairly unusual to\n",
    "do character-level machine translation, as word-level\n",
    "models are more common in this domain.\n",
    "\n",
    "**Summary of the algorithm**\n",
    "\n",
    "1. We start with input sequences from a domain (e.g. English sentences)\n",
    "    and corresponding target sequences from another domain\n",
    "    (e.g. French sentences).\n",
    "\n",
    "2. An encoder LSTM turns input sequences to 2 state vectors\n",
    "    (we keep the last LSTM state and discard the outputs).\n",
    "\n",
    "3. A decoder LSTM is trained to turn the target sequences into\n",
    "    the same sequence but offset by one timestep in the future,\n",
    "    a training process called \"teacher forcing\" in this context.\n",
    "    It uses as initial state the state vectors from the encoder.\n",
    "    Effectively, the decoder learns to generate `targets[t+1...]`\n",
    "    given `targets[...t]`, conditioned on the input sequence.\n",
    "\n",
    "4. In inference mode, when we want to decode unknown input sequences, we:\n",
    "    - Encode the input sequence into state vectors\n",
    "    - Start with a target sequence of size 1\n",
    "        (just the start-of-sequence character)\n",
    "    - Feed the state vectors and 1-char target sequence\n",
    "        to the decoder to produce predictions for the next character\n",
    "    - Sample the next character using these predictions\n",
    "        (we simply use argmax).\n",
    "    - Append the sampled character to the target sequence\n",
    "    - Repeat until we generate the end-of-sequence character or we\n",
    "        hit the character limit.\n",
    "\n",
    "**Data download:**\n",
    "\n",
    "* English to French sentence pairs.\n",
    "    - https://www.manythings.org/anki/fra-eng.zip\n",
    "\n",
    "* Lots of neat sentence pairs datasets can be found at:\n",
    "    - https://www.manythings.org/anki/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8h5mVNwE_o37"
   },
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('setup'):\n",
    "    os.mkdir('setup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_file = \"setup/requirements_10_chollet_lstm.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting setup/requirements_10_chollet_lstm.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile {req_file}\n",
    "isort\n",
    "scikit-learn-intelex\n",
    "watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "IS_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IS_COLAB:\n",
    "    print(\"Installing packages\")\n",
    "    !pip install --upgrade --quiet -r {req_file}\n",
    "else:\n",
    "    print(\"Running locally.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_COLAB:\n",
    "    from sklearnex import patch_sklearn\n",
    "    patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting setup/chp10_chollet_lstm_imports.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile setup/chp10_chollet_lstm_imports.py\n",
    "import locale\n",
    "import os\n",
    "import pprint\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tensorflow import keras\n",
    "from tqdm.auto import tqdm\n",
    "from watermark import watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import locale\n",
      "import os\n",
      "import pprint\n",
      "import random\n",
      "import warnings\n",
      "\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "from tensorflow import keras\n",
      "from tqdm.auto import tqdm\n",
      "from watermark import watermark\n"
     ]
    }
   ],
   "source": [
    "!isort setup/chp10_chollet_lstm_imports.py --sl\n",
    "!cat setup/chp10_chollet_lstm_imports.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "import os\n",
    "import pprint\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tensorflow import keras\n",
    "from tqdm.auto import tqdm\n",
    "from watermark import watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.8.12\n",
      "IPython version      : 7.34.0\n",
      "\n",
      "Compiler    : Clang 13.0.0 (clang-1300.0.29.3)\n",
      "OS          : Darwin\n",
      "Release     : 21.6.0\n",
      "Machine     : x86_64\n",
      "Processor   : i386\n",
      "CPU cores   : 4\n",
      "Architecture: 64bit\n",
      "\n",
      "sys    : 3.8.12 (default, Dec 13 2021, 20:17:08) \n",
      "[Clang 13.0.0 (clang-1300.0.29.3)]\n",
      "seaborn: 0.12.1\n",
      "numpy  : 1.23.5\n",
      "keras  : 2.9.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def HR():\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "def getpreferredencoding(do_setlocale = True):\n",
    "    return \"UTF-8\"\n",
    "\n",
    "locale.getpreferredencoding = getpreferredencoding\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style(\"darkgrid\")\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "random.seed(23)\n",
    "\n",
    "print(watermark(iversions=True,globals_=globals(),python=True,machine=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUKolmdW_o39"
   },
   "source": [
    "## Download the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-dGN__jFz6yN",
    "outputId": "3f58941b-3a39-4049-ad85-008e526ed9e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/data_fra_eng\n",
      "fra-eng.zip\n",
      "data/data_fra_eng/fra-eng.zip\n",
      "data/data_fra_eng/fra.txt\n",
      "----------------------------------------\n",
      "File ‘data/data_fra_eng/fra-eng.zip’ already there; not retrieving.\n",
      "total 75608\n",
      "-rw-r--r--  1 gb  staff      1441 Feb 20 22:16 _about.txt\n",
      "-rw-r--r--  1 gb  staff   7155035 Feb 21 08:53 fra-eng.zip\n",
      "-rw-r--r--  1 gb  staff  31547877 Feb 20 22:16 fra.txt\n"
     ]
    }
   ],
   "source": [
    "data_fra_eng_dir = 'data/data_fra_eng'\n",
    "if not os.path.exists(data_fra_eng_dir):\n",
    "    os.makedirs(data_fra_eng_dir)\n",
    "        \n",
    "data_fra_eng_file = 'fra-eng.zip'\n",
    "data_fra_eng_src = f\"{data_fra_eng_dir}/{data_fra_eng_file}\"\n",
    "data_fra_eng_path = f\"{data_fra_eng_dir}/fra.txt\"\n",
    "\n",
    "print(data_fra_eng_dir)\n",
    "print(data_fra_eng_file)\n",
    "print(data_fra_eng_src)\n",
    "print(data_fra_eng_path)\n",
    "HR()\n",
    "\n",
    "!wget -P {data_fra_eng_dir} -O {data_fra_eng_src} -nc \"http://www.manythings.org/anki/fra-eng.zip\"\n",
    "!ls -l {data_fra_eng_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/data_fra_eng/fra-eng.zip\n",
      "data/data_fra_eng/fra.txt\n"
     ]
    }
   ],
   "source": [
    "print(data_fra_eng_src)\n",
    "print(data_fra_eng_path)\n",
    "\n",
    "if not os.path.exists(data_fra_eng_path):\n",
    "    !unzip {data_fra_eng_src} -d {data_fra_eng_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fra_eng_dir = 'models/model_fra_eng'\n",
    "if not os.path.exists(model_fra_eng_dir):\n",
    "    os.makedirs(model_fra_eng_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZn40ZkZ_o3-"
   },
   "source": [
    "## Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "bDy-soKu_o3-"
   },
   "outputs": [],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "# epochs = 100  # Number of epochs to train for.\n",
    "epochs = 1  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 10_000  # Number of samples to train on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TlQv0953_o3_"
   },
   "source": [
    "## Prepare the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "hIqC5VfIz6yR"
   },
   "outputs": [],
   "source": [
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "\n",
    "with open(data_fra_eng_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().split(\"\\n\")\n",
    "\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split(\"\\t\")\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = \"\\t\" + target_text + \"\\n\"\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "n9TD-2wAz6yS",
    "outputId": "a4c91ef4-d1d3-4dba-ca1c-f547a21df491"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Did I wake you?\\tVous ai-je réveillés ?\\tCC-BY 2.0 ('"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v3Xn61qR_o4A",
    "outputId": "0c99b927-9f15-41fa-9a86-8c805adee95c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10000\n",
      "Number of unique input tokens: 71\n",
      "Number of unique output tokens: 93\n",
      "----------------------------------------\n",
      "Max sequence length for inputs: 15\n",
      "Max sequence length for outputs: 59\n",
      "----------------------------------------\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print(\"Number of samples:\", len(input_texts))\n",
    "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
    "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
    "HR()\n",
    "\n",
    "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
    "print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
    "HR()\n",
    "\n",
    "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "\n",
    "print(type(encoder_input_data))\n",
    "print(type(decoder_input_data))\n",
    "print(type(decoder_target_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "mFUdBAztz6yU"
   },
   "outputs": [],
   "source": [
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
    "    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
    "    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
    "    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j5XcR0o3_o4B"
   },
   "source": [
    "## Build the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "zoxI2D4b_o4C"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-30 21:49:53.430309: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "encoder_inputs:\n",
      "\tKerasTensor(type_spec=TensorSpec(shape=(None, None, 71), dtype=tf.float32, name='Encoder_Input'), name='Encoder_Input', description=\"created by layer 'Encoder_Input'\")\n",
      "----------------------------------------\n",
      "decoder_inputs:\n",
      "\tKerasTensor(type_spec=TensorSpec(shape=(None, None, 93), dtype=tf.float32, name='Decoder_Input'), name='Decoder_Input', description=\"created by layer 'Decoder_Input'\")\n",
      "----------------------------------------\n",
      "decoder_outputs:\n",
      "\tKerasTensor(type_spec=TensorSpec(shape=(None, None, 93), dtype=tf.float32, name=None), name='DecoderOutput/Softmax:0', description=\"created by layer 'DecoderOutput'\")\n"
     ]
    }
   ],
   "source": [
    "# Define an input sequence and process it.\n",
    "encoder_inputs = keras.Input(shape=(None, num_encoder_tokens), name=\"Encoder_Input\")\n",
    "encoder = keras.layers.LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = keras.Input(shape=(None, num_decoder_tokens), name=\"Decoder_Input\")\n",
    "\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True, name=\"Decoder_LSTM\")\n",
    "\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\", name=\"DecoderOutput\")\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "HR()\n",
    "\n",
    "print(f\"encoder_inputs:\\n\\t{encoder_inputs}\")\n",
    "HR()\n",
    "print(f\"decoder_inputs:\\n\\t{decoder_inputs}\")\n",
    "HR()\n",
    "print(f\"decoder_outputs:\\n\\t{decoder_outputs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7NCON3Vl_o4D"
   },
   "source": [
    "## Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WXKi63PW_o4D",
    "outputId": "6d9c55af-e77b-4c50-a822-755566d95ada"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Encoder_Input (InputLayer)     [(None, None, 71)]   0           []                               \n",
      "                                                                                                  \n",
      " Decoder_Input (InputLayer)     [(None, None, 93)]   0           []                               \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 256),        335872      ['Encoder_Input[0][0]']          \n",
      "                                 (None, 256),                                                     \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " Decoder_LSTM (LSTM)            [(None, None, 256),  358400      ['Decoder_Input[0][0]',          \n",
      "                                 (None, 256),                     'lstm[0][1]',                   \n",
      "                                 (None, 256)]                     'lstm[0][2]']                   \n",
      "                                                                                                  \n",
      " DecoderOutput (Dense)          (None, None, 93)     23901       ['Decoder_LSTM[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 718,173\n",
      "Trainable params: 718,173\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "125/125 - 37s - loss: 1.1448 - accuracy: 0.7316 - val_loss: 1.0719 - val_accuracy: 0.7140 - 37s/epoch - 300ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) Encoder_Input, Decoder_Input with unsupported characters which will be renamed to encoder_input, decoder_input in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/model_fra_eng/s2s/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/model_fra_eng/s2s/assets\n"
     ]
    }
   ],
   "source": [
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"rmsprop\", \n",
    "    loss=\"categorical_crossentropy\", \n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "hist = model.fit(\n",
    "    [encoder_input_data, decoder_input_data],\n",
    "    decoder_target_data,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Save model\n",
    "model.save(f\"{model_fra_eng_dir}/s2s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FXQjTo_J7PV4"
   },
   "source": [
    "**Note**\n",
    "\n",
    "We don't have a test dataset here, so we don't run `model.evaluate()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HWpexToF_o4D"
   },
   "source": [
    "## Run inference (sampling)\n",
    "\n",
    "1. Encode input and retrieve initial decoder state\n",
    "2. Run one step of decoder with this initial state and a \"start of sequence\" token as target. Output will be the next target token.\n",
    "3. Repeat with the current target token and current states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, None, 71), dtype=tf.float32, name='Encoder_Input'), name='Encoder_Input', description=\"created by layer 'Encoder_Input'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, None, 93), dtype=tf.float32, name='Decoder_Input'), name='Decoder_Input', description=\"created by layer 'Decoder_Input'\")\n",
      "----------------------------------------\n",
      "[<KerasTensor: shape=(None, 256) dtype=float32 (created by layer 'lstm')>, <KerasTensor: shape=(None, 256) dtype=float32 (created by layer 'lstm')>, <KerasTensor: shape=(None, 256) dtype=float32 (created by layer 'lstm')>]\n",
      "----------------------------------------\n",
      "[<KerasTensor: shape=(None, None, 256) dtype=float32 (created by layer 'Decoder_LSTM')>, <KerasTensor: shape=(None, 256) dtype=float32 (created by layer 'Decoder_LSTM')>, <KerasTensor: shape=(None, 256) dtype=float32 (created by layer 'Decoder_LSTM')>]\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, None, 93), dtype=tf.float32, name=None), name='DecoderOutput/Softmax:0', description=\"created by layer 'DecoderOutput'\")\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[0].output)\n",
    "print(model.layers[1].output)\n",
    "HR()\n",
    "print(model.layers[2].output) # lstm_1\n",
    "HR()\n",
    "print(model.layers[3].output)\n",
    "print(model.layers[4].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sampling models\n",
    "# Restore the model and construct the encoder and decoder.\n",
    "model = keras.models.load_model(f\"{model_fra_eng_dir}/s2s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, 256) dtype=float32 (created by layer 'lstm')>,\n",
       " <KerasTensor: shape=(None, 256) dtype=float32 (created by layer 'lstm')>,\n",
       " <KerasTensor: shape=(None, 256) dtype=float32 (created by layer 'lstm')>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = model.layers[2].output\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = model.input[0]  # input_1\n",
    "encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n",
    "encoder_states = [state_h_enc, state_c_enc]\n",
    "\n",
    "encoder_model = keras.Model(encoder_inputs, encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "uphac-ml_o4E"
   },
   "outputs": [],
   "source": [
    "decoder_inputs = model.input[1]  # input_2\n",
    "decoder_state_input_h = keras.Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = keras.Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_lstm = model.layers[3]\n",
    "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs\n",
    ")\n",
    "\n",
    "# decoder_states is a list so we add list to list resulting in a combined list. \n",
    "decoder_states = [state_h_dec, state_c_dec]\n",
    "\n",
    "decoder_dense = model.layers[4]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "\n",
    "decoder_model = keras.Model(\n",
    "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
    ")\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(\n",
    "        input_seq,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    \n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.0\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ll6Z54Uq_o4E"
   },
   "source": [
    "You can now generate decoded sentences as such:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AGZYI0ND_o4F",
    "outputId": "09082309-17dc-402e-b4ab-703f2fe87b25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: Go.\n",
      "Decoded sentence: Je s                                                        \n",
      "----------------------------------------\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Je s                                                        \n",
      "----------------------------------------\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Je s                                                        \n",
      "----------------------------------------\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Je s                                                        \n",
      "----------------------------------------\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Je                                                          \n",
      "----------------------------------------\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Je                                                          \n",
      "----------------------------------------\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Jess                                                        \n",
      "----------------------------------------\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Jess                                                        \n",
      "----------------------------------------\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Jess                                                        \n",
      "----------------------------------------\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Jess                                                        \n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# for seq_index in range(50):\n",
    "for seq_index in range(10):\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)\n",
    "    HR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k0NiW0n7z6yY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

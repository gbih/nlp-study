{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "245aa235-642c-4ddc-98f0-9be26f2449b1",
   "metadata": {},
   "source": [
    "<a id='top'></a><a name='top'></a>\n",
    "# Chapter 10: Sequence-to-sequence models and attention\n",
    "\n",
    "## LSTM models\n",
    "\n",
    "Code is based on this notebook: [A ten-minute introduction to sequence-to-sequence learning in Kera](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html) by François Chollet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0942ff-0c5c-486a-924f-0881157db039",
   "metadata": {},
   "source": [
    "* [Introduction](#introduction)\n",
    "* [10.0 Imports and Setup](#10.0)\n",
    "* [10.1 Code](#10.1)\n",
    "\n",
    "\n",
    "\n",
    "<!-- * [Introduction](#introduction)\n",
    "* [10.0 Imports and Setup](#9.0)\n",
    "* [10.1 Encoder-decoder architecture](#10.1)\n",
    "    - [10.1.1 Decoding thought](#10.1.1)\n",
    "    - [10.1.2 Look familiar?](#10.1.2)\n",
    "    - [10.1.3 Sequence-to-sequence conversation](#10.1.3)\n",
    "    - [10.1.4 LSTM review](#10.1.4)\n",
    "* [10.2 Assembling a sequence-to-sequence pipeline](#10.2)\n",
    "    - [10.2.1 Preparing your dataset for the sequence-to-sequence training](#10.2.1)\n",
    "    - [10.2.2 Sequence-to-sequence model in Keras](#10.2.2)\n",
    "    - [10.2.3 Sequence encoder](#10.2.3)\n",
    "    - [10.2.4 Thought decoder](#10.2.4)\n",
    "    - [10.2.5 Assembling the sequence-to-sequence network](#10.2.5)\n",
    "* [10.3 Training the sequence-to-sequence network](#10.3)\n",
    "    - [10.3.1 Generate output sequences](#10.3.1)\n",
    "* [10.4 Building a chatbot using sequence-to-sequence networks](#10.4)\n",
    "    - [10.4.1 Preparing the corpus for your training](#10.4.1)\n",
    "    - [10.4.2 Building your character dictionary](#10.4.2)\n",
    "    - [10.4.3 Generate one-hot encoded training sets](#10.4.3)\n",
    "    - [10.4.4 Train your sequence-to-sequence chatbot](#10.4.4)\n",
    "    - [10.4.5 Assemble the model for sequence-to-sequence generation](#10.4.5)\n",
    "    - [10.4.6 Predicting a sequence](#10.4.6)\n",
    "    - [10.4.7 Generating a response](#10.4.7)\n",
    "    - [10.4.8 Converse with your chatbot](#10.4.8)\n",
    "* [10.5 Enhancements](#10.5)\n",
    "    - [10.5.1 Reduce training complexity with bucketing](#10.5.1)\n",
    "    - [10.5.2 Paying attention](#10.5.2)\n",
    "* [10.6 In the real world](#10.6) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c29db03-ab66-4d20-981d-b218339c2d24",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='introduction'></a><a id='introduction'></a>\n",
    "# Introduction\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "### Datasets\n",
    "\n",
    "* fra-eng.zip: [script](#fra-eng.zip), [source](http://www.manythings.org/anki/fra-eng.zip)\n",
    "\n",
    "\n",
    "### Explore\n",
    "\n",
    "* Mapping one text sequence to another with a neural network.\n",
    "* Understanding sequence-to-sequence tasks and how they're different from the others we've learned about.\n",
    "* Using encoder-decoder model architectures for translation and chat.\n",
    "* Training a model to pay attention to what is important in a sequence. \n",
    "\n",
    "\n",
    "### Key points\n",
    "\n",
    "* Sequence-to-sequence networks can be built with a modular, reusable encoder-decoder architecture.\n",
    "* The encoder model generates a thought vector, a dense, fixed-dimension vector representation of the information in a variable-length input sequence.\n",
    "* A decoder can use thought vectors to predict (generate) output sequences, including the replies of a chatbot.\n",
    "* Due to the thought vector representation, the input and output sequence lengths don't have to match.\n",
    "* Thought vectors can only hold a limited amount of information. If you need a thought vector to encode more complex concepts, the attention mechanism can help selectively encode what is important in the thought vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bae5a7a-774d-47d0-92c1-e52407827944",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='10.0'></a><a id='10.0'></a>\n",
    "# 10.0 Imports and Setup\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fef9cf49-f247-44ac-879c-b789517c3404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('setup'):\n",
    "    os.mkdir('setup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "317f14ab-235d-488e-8c8b-da7e02b98cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "req_file = \"setup/requirements_10.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b24dd37-e583-4e19-86a2-cb9e7715b75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting setup/requirements_10.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile {req_file}\n",
    "isort\n",
    "scikit-learn-intelex\n",
    "scrapy\n",
    "watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d0bd6bc-e82f-4c9d-8134-f7bd934ee7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "IS_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IS_COLAB:\n",
    "    print(\"Installing packages\")\n",
    "    !pip install --upgrade --quiet -r {req_file}\n",
    "else:\n",
    "    print(\"Running locally.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c188afb-d988-4c4f-be03-ed9f38965f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "#if IS_COLAB:\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "729a7358-512e-4fae-9461-d40e205b38e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting setup/chp10_imports.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile setup/chp10_imports.py\n",
    "import locale\n",
    "import os\n",
    "import pprint\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tqdm.auto import tqdm\n",
    "from watermark import watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a09221f-4034-4737-a3b4-f5b7650510fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing /Users/gb/Desktop/examples/setup/chp10_imports.py\n",
      "import locale\n",
      "import os\n",
      "import pprint\n",
      "import random\n",
      "import warnings\n",
      "\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "from keras.layers import LSTM\n",
      "from keras.layers import Dense\n",
      "from keras.layers import Input\n",
      "from keras.models import Model\n",
      "from tensorflow.keras.layers import Dense\n",
      "from tqdm.auto import tqdm\n",
      "from watermark import watermark\n"
     ]
    }
   ],
   "source": [
    "!isort setup/chp10_imports.py --sl\n",
    "!cat setup/chp10_imports.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a243be6-d118-4213-9cca-de08f2ea5992",
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "import os\n",
    "import pprint\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tqdm.auto import tqdm\n",
    "from watermark import watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54ad1006-90d0-4d81-918c-6f832a02ce06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.8.12\n",
      "IPython version      : 7.34.0\n",
      "\n",
      "Compiler    : Clang 13.0.0 (clang-1300.0.29.3)\n",
      "OS          : Darwin\n",
      "Release     : 21.6.0\n",
      "Machine     : x86_64\n",
      "Processor   : i386\n",
      "CPU cores   : 4\n",
      "Architecture: 64bit\n",
      "\n",
      "numpy  : 1.23.5\n",
      "sys    : 3.8.12 (default, Dec 13 2021, 20:17:08) \n",
      "[Clang 13.0.0 (clang-1300.0.29.3)]\n",
      "seaborn: 0.12.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def HR():\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "def getpreferredencoding(do_setlocale = True):\n",
    "    return \"UTF-8\"\n",
    "\n",
    "locale.getpreferredencoding = getpreferredencoding\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style(\"darkgrid\")\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(watermark(iversions=True,globals_=globals(),python=True,machine=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25497430-a333-43b7-b355-890150716cc3",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='10.1'></a><a id='10.1'></a>\n",
    "# 10.1 Code\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1e8610-84a3-4e03-a8ef-0491c0719687",
   "metadata": {},
   "source": [
    "<a id='fra-eng.zip'></a><a name='fra-eng.zip'></a>\n",
    "### Dataset: aclImdb_v1.tar.gz\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a505ce2-a071-422d-8d9f-271f0b02f72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://www.manythings.org/anki/\n",
    "# http://www.manythings.org/anki/fra-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cfb79d9-1350-46ff-b604-4f7c272aa82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘data/data_fra_eng/fra-eng.zip’ already there; not retrieving.\n",
      "total 75608\n",
      "-rw-r--r--  1 gb  staff      1441 Feb 20 22:16 _about.txt\n",
      "-rw-r--r--  1 gb  staff   7155035 Feb 21 08:53 fra-eng.zip\n",
      "-rw-r--r--  1 gb  staff  31547877 Feb 20 22:16 fra.txt\n"
     ]
    }
   ],
   "source": [
    "data_fra_eng_dir = 'data/data_fra_eng'\n",
    "if not os.path.exists(data_fra_eng_dir):\n",
    "    os.makedirs(data_fra_eng_dir)\n",
    "        \n",
    "data_fra_eng_file = 'fra-eng.zip'\n",
    "data_fra_eng_src = f\"{data_fra_eng_dir}/{data_fra_eng_file}\"\n",
    "data_fra_eng_path = f\"{data_fra_eng_dir}/fra.txt\"\n",
    "!wget -P {data_fra_eng_dir} -O {data_fra_eng_src} -nc \"http://www.manythings.org/anki/fra-eng.zip\"\n",
    "!ls -l {data_fra_eng_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc323f71-6229-4f41-8b18-5194ee5a1af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/data_fra_eng/fra-eng.zip\n",
      "data/data_fra_eng/fra.txt\n"
     ]
    }
   ],
   "source": [
    "print(data_fra_eng_src)\n",
    "print(data_fra_eng_path)\n",
    "\n",
    "if not os.path.exists(data_fra_eng_path):\n",
    "    !unzip {data_fra_eng_src} -d {data_fra_eng_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a38f616-53f4-416e-a10c-b4c8b66e1b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "#epochs = 100  # Number of epochs to train for.\n",
    "epochs = 1  # Number of epochs to train for.\n",
    "\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 10_000  # Number of samples to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44862657-16d8-4c6c-8764-47e921d72ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "\n",
    "with open(data_fra_eng_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().split(\"\\n\")\n",
    "\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split('\\t')\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "385a4dde-64cf-4eb3-b658-b872fc038100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Did I wake you?\\tVous ai-je réveillés ?\\tCC-BY 2.0 ('"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7f81bed-f23e-4723-819b-88f3437199eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10,000\n",
      "Number of unique input tokens: 71\n",
      "Number of unique output tokens: 93\n",
      "----------------------------------------\n",
      "Max sequence length for inputs: 15\n",
      "Max sequence length for outputs: 59\n",
      "----------------------------------------\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print(f\"Number of samples: {len(input_texts):,}\")\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "HR()\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "HR()\n",
    "\n",
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "print(type(encoder_input_data))\n",
    "print(type(decoder_input_data))\n",
    "print(type(decoder_target_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43a5fa0a-9cd0-4fdf-baa0-1e923888438f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04db6ac5-fd70-43fe-a996-35fe6d6b0b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-30 22:41:59.152719: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb0ee4aa-4028-44eb-8b64-23423a219db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 - 37s - loss: 0.9723 - val_loss: 1.0528 - 37s/epoch - 298ms/step\n"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "model.compile(\n",
    "    optimizer='rmsprop', \n",
    "    loss='categorical_crossentropy'\n",
    ")\n",
    "\n",
    "hist = model.fit(\n",
    "    [encoder_input_data, decoder_input_data], \n",
    "    decoder_target_data,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Save model\n",
    "# model.save('s2s.h5')\n",
    "\n",
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states\n",
    ")\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(\n",
    "        input_seq,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    \n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d263fb3c-891f-46ab-ad91-ef0e70c7f832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Take one sequence (part of the training test for trying out decoding.\n",
      "\n",
      "Input sentence: Go.\n",
      "Decoded sentence: ouee                                                        \n",
      "----------------------------------------\n",
      "Input sentence: Go.\n",
      "Decoded sentence: ouee                                                        \n",
      "----------------------------------------\n",
      "Input sentence: Go.\n",
      "Decoded sentence: ouee                                                        \n",
      "----------------------------------------\n",
      "Input sentence: Go.\n",
      "Decoded sentence: ouee                                                        \n",
      "----------------------------------------\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Toeee                                                       \n",
      "----------------------------------------\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Toeee                                                       \n",
      "----------------------------------------\n",
      "Input sentence: Run!\n",
      "Decoded sentence: oueee                                                       \n",
      "----------------------------------------\n",
      "Input sentence: Run!\n",
      "Decoded sentence: oueee                                                       \n",
      "----------------------------------------\n",
      "Input sentence: Run!\n",
      "Decoded sentence: oueee                                                       \n",
      "----------------------------------------\n",
      "Input sentence: Run!\n",
      "Decoded sentence: oueee                                                       \n",
      "----------------------------------------\n",
      "Input sentence: Run!\n",
      "Decoded sentence: oueee                                                       \n",
      "----------------------------------------\n",
      "Input sentence: Run!\n",
      "Decoded sentence: oueee                                                       \n",
      "----------------------------------------\n",
      "Input sentence: Run!\n",
      "Decoded sentence: oueee                                                       \n",
      "----------------------------------------\n",
      "Input sentence: Run!\n",
      "Decoded sentence: oueee                                                       \n",
      "----------------------------------------\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Toeee                                                       \n",
      "----------------------------------------\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Toeee                                                       \n",
      "----------------------------------------\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Toeee                                                       \n",
      "----------------------------------------\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Toeee                                                       \n",
      "----------------------------------------\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Toeee                                                       \n",
      "----------------------------------------\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Toeee                                                       \n",
      "----------------------------------------\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Take one sequence (part of the training test for trying out decoding.\")\n",
    "print()\n",
    "\n",
    "for seq_index in range(20):\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)\n",
    "    HR()\n",
    "    \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc739104-2a26-428e-a2ba-f48be93abc68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

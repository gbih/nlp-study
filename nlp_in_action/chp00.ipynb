{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c60b237-1ec7-4aec-8b23-c750b37eae20",
   "metadata": {},
   "source": [
    "<a id='top'></a><a name='top'></a>\n",
    "# [Natural Language Processing in Action, v1](https://www.manning.com/books/natural-language-processing-in-action/)\n",
    "\n",
    "[Github repo](https://github.com/totalgood/nlpia)\n",
    "\n",
    "\n",
    "1. [Packets of thought (NLP overview)](#1.0)\n",
    "2. [Build your vocabulary (word tokenization)](#2.0)\n",
    "3. [Math with words (TF-IDF vectors)](#3.0)\n",
    "4. [Finding meaning in word counts (semantic analysis)](#4.0)\n",
    "5. [Baby steps with neural networks (perceptrons and backpropagation)](#5.0)\n",
    "6. [Reasoning with word vectors (Word2vec)](#6.0)\n",
    "7. [Getting words in order with convolutional neural networks (CNNs)](#7.0)\n",
    "8. [Loopy (recurrent) neural networks (RNNs)](#8.0)\n",
    "9. [Improving retention with long short-term memory networks](#9.0)\n",
    "10. [Sequence-to-sequence models and attention](#10.0)\n",
    "11. [Information extraction (named entity extraction and question answering)](#11.0)\n",
    "12. [Getting chatty (dialog engines)](#12.0)\n",
    "13. [Scaling up (optimization, parallelization, and batch processing)](#13.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dd873a-aff5-4805-aa14-dbdd11c1841f",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='1.0'></a><a id='1.0'></a>\n",
    "# Chapter 1: Packets of thought\n",
    "## NLP overview\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "## Overview\n",
    "1. What natural language processing (NLP) is.\n",
    "2. Why NLP is hard and only recently has become widespread.\n",
    "3. When word order and grammar is important and when it can be ignored.\n",
    "4. How a chatbot combines many of the tools of NLP.\n",
    "5. How to use a regular expression to build the start of a tiny chatbot.\n",
    "\n",
    "## Summary \n",
    "1. NLP can be very useful.\n",
    "2. The meaning and intent of words can be deciphered by machines.\n",
    "3. A smart NLP pipeline will be able to deal with ambiguity.\n",
    "4. We can teach machines common sense knowledge without spending a lifetime training them.\n",
    "5. Chatbots can be thought of as semantic search engines.\n",
    "6. Regular expressions are useful for more than just search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ec03e77-f2ca-459d-ba0b-276452308826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/gb/Desktop/examples\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cd8d63-558e-4b16-9c95-f3aef8009fd7",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='2.0'></a><a id='2.0'></a>\n",
    "# Chapter 2: Build your vocabulary\n",
    "## Word tokenization\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "## Overview\n",
    "1. Tokenizing your text into words and n-grams (tokens).\n",
    "2. Dealing with nonstandard punctuation and emoticons, like social media posts.\n",
    "3. Compressing your token vocabulary with stemming and lemmatization.\n",
    "4. Building a vector representation of a statement.\n",
    "5. Building a sentiment analyzer from handcrafted token scores. \n",
    "\n",
    "## Summary\n",
    "1. Implement tokenization and configuration of tokenizers for applications.\n",
    "2. n-gram tokenization helps retain some of the word order information in a document.\n",
    "3. Normalization and stemming consolidate words into groups that improve the \"recall\" for search engines but reduce precision.\n",
    "4. Lemmatization and customized tokenizers like `casual_tokenize()` can improve precision and reduce information loss.\n",
    "5. Stop words can contain useful information, and discarding them is not always helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13af023d-093f-456d-a1e3-6928c8244500",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='3.0'></a><a id='3.0'></a>\n",
    "# Chapter 3: Math with words \n",
    "## TF-IDF vectors\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "## Overview\n",
    "1. Counting words and term frequencies to analyze meaning.\n",
    "2. Predicting word occurrence probabilities with Zipf's Law.\n",
    "3. Vector representation of words and how to start using them.\n",
    "4. Finding relevant documents from a corpus using inverse document frequencies.\n",
    "5. Estimating the similarity of pairs of documents with cosine similarity and Okapi BM25.\n",
    "\n",
    "## Summary\n",
    "1. Any web-scale search engine with millisecond response times has the power of a TF-IDF term document matrix hidden under the hood.\n",
    "2. Term frequencies must be weighed by their inverse document frequency to ensure the most important, most meaningful words are given the heft they deserve.\n",
    "3. Zipf's Law can help you predict the frequencies of all sorts of things, including words, characters and people.\n",
    "4. The rows of a TF-IDF term document matrix can be used as a vector representation of the meanings of those individual words to create a vector space model of word semantics.\n",
    "5. Euclidean distance and similarity between pairs of high dimensional vectors doesn't adequately represent their similarity for most NLP applications.\n",
    "6. Cosine distance, the amount of \"overlap\" between vectors, can be calculated efficiently by just multiplying the elements of normalized vectors together and summing up those products.\n",
    "7. Cosine distance is the go-to similarity score for most natural language vector representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b2e7c0-08fd-4806-bddd-c4b3fa650a1a",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='4.0'></a><a id='4.0'></a>\n",
    "# Chapter 4: Finding meaning in word counts\n",
    "## Semantic analysis\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "## Overview\n",
    "1. Analyzing semantics (meaning) to create topic vectors.\n",
    "2. Semantic search using the similarity between topic vectors.\n",
    "3. Scalable semantic analysis and semantic search for large corpora.\n",
    "4. Using semantic components (topics) as features in your NLP pipeline.\n",
    "5. Navigating high-dimensional vector spaces.\n",
    "\n",
    "## Summary\n",
    "1. You can use SVD for semantic analysis to decompose and transform TF-IDF and BOW vectors into topic vectors.\n",
    "2. Use LDiA when you need to compute explainable topic vectors.\n",
    "3. No matter how much you create your topic vectors, they can be used for semantic search to find documents based on their meaning.\n",
    "4. Topic vectors can be used to predict whether a social post is spam or is likely to be \"liked.\"\n",
    "5. Now you know how to sidestep around the curse of dimensionality to find approximate nearest neighbors in your semantic vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c497b31-7bbd-4f07-839d-8c5d0a9495f2",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='5.0'></a><a id='5.0'></a>\n",
    "# Chapter 5: Baby steps with neural networks\n",
    "## Perceptrons and backpropagation\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. Learning the history of neural networks.\n",
    "2. Stacking perceptrons.\n",
    "3. Understanding backpropagation.\n",
    "4. Seeing the knobs to turn on neural networks.\n",
    "5. Implementing a basic neural network in Keras.\n",
    "\n",
    "## Summary\n",
    "1. Minimizing a cost function is a path toward learning.\n",
    "2. A backpropagation algorithm is the means by which a network learns.\n",
    "3. The amount a weight contributes to a model's error is directly related to the amount it needs to be updated.\n",
    "4. Neural networks are, at their heart, optimization engines.\n",
    "5. Watch out for pitfalls (local minima) during training by monitoring the gradual reduction in error.\n",
    "6. Keras helps make all of this neural network math accessible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97bede9-0632-4f8b-9935-d424ef23474f",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='6.0'></a><a id='6.0'></a>\n",
    "# Chapter 6: Reasoning with word vectors\n",
    "## Word2vec\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "## Overview\n",
    "1. Understanding how word vectors are created.\n",
    "2. Using pretrained models for your applications. \n",
    "3. Reasoning with word vectors to solve real problems.\n",
    "4. Viusalizing word vectors.\n",
    "5. Uncovering some surprising uses for word embeddings.\n",
    "\n",
    "## Summary\n",
    "1. Learn how word vectors and vector-oriented reasoning can solve some surprisingly subtle problems, like analogy questions and nonsynonomy relationships between words.\n",
    "2. We can now train Word2vec and other word vector embeddings on the words we use in applications so the NLP pipeline is not \"polluted\" by the GoogleNews meaning of words inherent in most Word2vec pretrained models.\n",
    "3. We can use gensim to explore, visualize and even build our own word vector vocabularies.\n",
    "4. A PCA projection of geographic word vectors like US city names can reveal the cultural closeness of places (as expressed in literature) that are georgraphically far apart.\n",
    "5. If you respect sentence boundaries with n-grams and are efficient at setting up word pairs for training, you can greatly improve the accuracy of your latent semantic analysis word embeddings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fd3858-794b-4543-bc61-7159d85bbce8",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='7.0'></a><a id='7.0'></a>\n",
    "# Chapter 7: Getting words in order with convolutional neural networks\n",
    "## CNNs\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "## Overview\n",
    "1. Using neural networks for NLP.\n",
    "2. Finding meaning in word patterns.\n",
    "3. Building a CNN.\n",
    "4. Vectorizing natural language text in a way that suits neural networks.\n",
    "5. Training a CNN.\n",
    "6. Classifying the sentiment of novel text.\n",
    "\n",
    "## Summary\n",
    "1. A convolution is a window sliding over something larger (keeping the focus on a subset of the greater whole).\n",
    "2. Neural networks can treat text just as they treat images and \"see\" them.\n",
    "3. Handicapping the learning process with dropout actually helps.\n",
    "4. Sentiment exists not only in the words but in the patterns that are used.\n",
    "5. Neural networks have many knobs you can turn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6887e7b-5776-4848-9694-2138b0b45524",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='8.0'></a><a id='8.0'></a>\n",
    "# Chapter 8: Loopy neural networks\n",
    "## RNNs\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "## Overview\n",
    "1. Creating memory in a neural net.\n",
    "2. Building a recurrent neural net.\n",
    "3. Data handling for RNNs.\n",
    "4. Backpropagating through time (BPTT).\n",
    "\n",
    "## Summary\n",
    "1. In natural language sequences (words or characters), what came before is important to your model's understanding of the sequence.\n",
    "2. Splitting a natural language statement along the dimensions of time (tokens) can help your machine deepen its understanding of natural language.\n",
    "3. You can backpropagate errors in time (tokens), as well as in the layers of a deep learning network.\n",
    "4. Because RNNs are particularly deep neural nets, RNN gradients are particularly temperamental, and they may disappear or explode.\n",
    "5. Efficiently modeling natural language character sequences wasn't possible until recurrent neural nets were applied to the task.\n",
    "6. Weights in an RNN are adjusted in aggregate across time for a given sample.\n",
    "7. You can use different methods to examine the output of recurrent neural nets.\n",
    "8. You can model the natural language sequence in a document by passing the sequence of tokens through an RNN backward and forward simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2490336e-3a1a-4177-aaac-2be7bc29150c",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='9.0'></a><a id='9.0'></a>\n",
    "# Chapter 9: Improving retention with long short-term memory networks\n",
    "## LSTM\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "## Overview\n",
    "1. Adding deeper memory to recurrent neural nets.\n",
    "2. Gating information inside neural nets.\n",
    "3. Classifying and generating text.\n",
    "4. Modeling language patterns.\n",
    "\n",
    "## Summary\n",
    "1. Remembering information with memory units enables more accurate and general models of the sequence.\n",
    "2. It's important to forget information that is no longer relevant.\n",
    "3. Only some new information needs to be retained for the upcoming input, and LSTMs can be trained to find it.\n",
    "4. If you can predict what comes next, you can generate novel text from probabilities.\n",
    "5. Character-based models can more efficiently and successfully learn from small, focussed corpora than word-based models (GB: EXPLORE THIS AGAIN!)\n",
    "6. LSTM thought vectors capture much more than just the sum of the words in a statement.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41941a56-45ee-45fd-b3b1-a2657eafe6c3",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='10.0'></a><a id='10.0'></a>\n",
    "# Chapter 10: Sequence-to-sequence models and attention\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "## Overview\n",
    "1. Mapping one text sequence to another with a neural network.\n",
    "2. Understanding how sequence-to-sequence tasks differ.\n",
    "3. Using encoder-decoder model architectures for translation and chat.\n",
    "4. Training a model to pay attention to what is important in a sequence.\n",
    "\n",
    "## Summary\n",
    "1. Sequence-to-sequence networks can be built with a modular, reusable encoder-decoder architecture.\n",
    "2. The encoder model generates a thought vector, a dense fixed-dimension vector representation of the information in a variable-length input sequence.\n",
    "3. A decoder can use thought vectors to predict (generate) output sequences, including the replies of a chatbot.\n",
    "4. Due to the thought vector representation, the input and output sequence lenghts don't have to match.\n",
    "5. Thought vectors can only hold a limited amount of information. If you need a thought vector to encode more complex concepts, the attention mechanism can help selectively encode what is important in the thought vector.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba97d2bf-bc05-4563-800c-f386f324481c",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='11.0'></a><a id='11.0'></a>\n",
    "# Chapter 11: Information extraction\n",
    "\n",
    "## Named entity extraction and question answering\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "## Overview\n",
    "1. Sentence segmentation\n",
    "2. Named entity recognition (NER)\n",
    "3. Numerical information extraction\n",
    "4. Part-of-speech (POS) tagging and dependency tree parsing\n",
    "5. Logical relation extraction and knowledge bases\n",
    "\n",
    "## Summary\n",
    "1. A knowledge graph can be built to store relationships between entities.\n",
    "2. Regular expressions are a mini-programming language that can isolate and extract information.\n",
    "3. Part-of-speech tagging allows you to extract relationships between entities mentioned in a sentence.\n",
    "4. Segmenting sentences requires more than just splitting on periods and exclamation marks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f570e92-729c-4d1c-b916-c68e09e6f688",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='12.0'></a><a id='12.0'></a>\n",
    "# Chapter 12: Getting chatty\n",
    "\n",
    "## Dialog engines\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "## Overview\n",
    "1. Understanding four chatbot approaches.\n",
    "2. Finding out what Artificial Intelligence Markup Language is all about.\n",
    "3. Understanding the difference between chatbot pipelines and other NLP pipelines.\n",
    "4. Learning about a hybrid chatbot architecture that combines the best ideas into one.\n",
    "5. Using machine learning to make your chatbot get smarter over time.\n",
    "6. Giving your chatbot agency - enabling it to spontaneously say what's on its mind.\n",
    "\n",
    "## Summary\n",
    "1. By combining multiple proven approaches, you can build an intelligent dialog engine.\n",
    "2. Breaking \"ties\" between the replies generated by the four main chatbot approaches is one key to intelligence.\n",
    "3. You can teach machines a lifetime of knowledge without spending a lifetime programming them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a644e364-8f36-4ff1-9421-f6467143ead5",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='13.0'></a><a id='13.0'></a>\n",
    "# Chapter 13: Scaling up\n",
    "\n",
    "## Optimization, parallelization, and batch processing\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "## Overview\n",
    "1. Scaling up an NLP pipeline.\n",
    "2. Speeding up search with indexing.\n",
    "3. Batch processing to reduce your memory footprint.\n",
    "4. Parallelization to speed up NLP.\n",
    "5. Running NLP model training on a GPU.\n",
    "\n",
    "## Summary\n",
    "1. Locality-sensitive hashes like `Annoy` make the promise of latent semantic indexing a reality.\n",
    "2. GPUs speed up model training, reducing the turn-around time on your models, making it easier to build models faster.\n",
    "3. CPU parallelization can make sense for algorithms that don't benefit from speedier multiplication of large matrices.\n",
    "4. You can bypass the system RAM bottleneck using Python's generators, saving you money on your GPU and CPU instances.\n",
    "5. Google's TensorBoard can help you visualize and extract natural language embeddings that you might not have thought of otherwise.\n",
    "6. Mastering NLP parallelization can expand your brainpower by giving you a society of minds - machine clusters to help you think."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac450d2-8223-4c51-919a-0fd797fee48f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

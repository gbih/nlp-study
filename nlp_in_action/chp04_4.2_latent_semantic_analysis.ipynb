{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d850a95-29a1-4948-9c7d-02b209f3a02b",
   "metadata": {
    "id": "3d850a95-29a1-4948-9c7d-02b209f3a02b"
   },
   "source": [
    "<a id='top'></a><a name='top'></a>\n",
    "# Chapter 4: Finding meaning in word counts (semantic analysis)\n",
    "\n",
    "## 4.2 Latent semantic analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fb858e-93ff-41c5-8e26-37c70bb0007c",
   "metadata": {
    "id": "d4fb858e-93ff-41c5-8e26-37c70bb0007c"
   },
   "source": [
    "* [Introduction](#introduction)\n",
    "* [4.0 Imports and Setup](#4.0)\n",
    "* [4.2 Latent semantic analysis](#4.2)\n",
    "    - [4.2.1 Your thought experiment made real](#4.2.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee9bf07-3035-4166-bbb7-d7ba2d2a67cf",
   "metadata": {
    "id": "7ee9bf07-3035-4166-bbb7-d7ba2d2a67cf"
   },
   "source": [
    "---\n",
    "<a name='introduction'></a><a id='introduction'></a>\n",
    "# Introduction\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "### Datasets\n",
    "\n",
    "* cats_and_dogs_sorted.txt: [script](#cats_and_dogs_sorted.txt), [source](https://github.com/totalgood/nlpia/raw/master/src/nlpia/data/cats_and_dogs_sorted.txt)\n",
    "\n",
    "### Explore\n",
    "\n",
    "* Analyzing semantics (meaning) to create topic vectors\n",
    "* Semantic search using the similarity between topic vectors\n",
    "* Scalable semantic analysis and semantic search for large copora\n",
    "* Using semantic components (topics) as features in your NLP pipeline\n",
    "* Navigating high-dimensional vector spaces\n",
    "\n",
    "\n",
    "### Key points\n",
    "\n",
    "* You can use SVD for semantic analysis to decompose and transform TF-IDF\n",
    "* Use LDiA when you need to compute explainable topic vectors\n",
    "* No matter how you create your topic vectors, they can be used for semantic search to find documents based on their meaning\n",
    "* Topic vectors can be used to predict whether a social post is spam or is likely to be \"liked\"\n",
    "* We can sidestep the curse of dimensionality to approximate nearest neighbors in a semantic vector space\n",
    "\n",
    "---\n",
    "\n",
    "Latent semantic analysis is based on the oldest and most commonly-used technique for dimension reduction, singular value decomposition. SVD was in widespread use long before the term \"machine learning\" even existed. SVD decomposes a matrix into three square matrices, one of which is diagonal.\n",
    "\n",
    "Using SVD, LSA can break down your TF-IDF term-document matrix into three simpler matrices. And they can be multiplied back together to produce the original matrix, without any changes. This is like factorization of a large integer. Big whoop. But these three simpler matrices from SVD reveal properties about the original TF-IDF matrix that you can exploit to simplify it. You can truncate those matrices (ignore some rows and columns) before multiplying them back together, which reduces the number of dimensions you have to deal with in your vector space model.\n",
    "\n",
    "These truncated matrices don’t give the exact same TF-IDF matrix you started with — they give you a better one. Your new representation of the documents contains the essence, the “latent semantics” of those documents. That’s why SVD is used in other fields for things such as compression. It captures the essence of a dataset and ignores the noise. A JPEG image is ten times smaller than the original bitmap, but it still contains all the information of the original image.\n",
    "\n",
    "When you use SVD this way in natural language processing, you call it latent semantic analysis. LSA uncovers the semantics, or meaning, of words that is hidden and waiting to be uncovered.\n",
    "\n",
    "Latent semantic analysis is a mathematical technique for finding the “best” way to linearly transform (rotate and stretch) any set of NLP vectors, like your TF-IDF vectors or bag-of-words vectors. And the “best” way for many applications is to line up the axes (dimensions) in your new vectors with the greatest “spread” or variance in the word frequencies. You can then eliminate those dimensions in the new vector space that don’t contribute much to the variance in the vectors from document to document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28d418f-975f-497b-9a83-169fb911a0ad",
   "metadata": {
    "id": "e28d418f-975f-497b-9a83-169fb911a0ad"
   },
   "source": [
    "---\n",
    "<a name='4.0'></a><a id='4.0'></a>\n",
    "# 4.0 Imports and Setup\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6227bd6-d034-4ad3-86d6-a07c5e78716b",
   "metadata": {
    "id": "b6227bd6-d034-4ad3-86d6-a07c5e78716b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('setup'):\n",
    "    os.mkdir('setup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91127bef-d081-46d6-99cb-7203814d9a3b",
   "metadata": {
    "id": "91127bef-d081-46d6-99cb-7203814d9a3b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "req_file = \"setup/requirements_04.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d86fec3-3bb5-4df9-8514-aac0e766bf30",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1d86fec3-3bb5-4df9-8514-aac0e766bf30",
    "outputId": "794dd9db-57df-4e6d-d9a8-92bb0c3ba0bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting setup/requirements_04.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile {req_file}\n",
    "isort\n",
    "scikit-learn-intelex\n",
    "scrapy\n",
    "watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a0d752e-c897-4120-ae57-375257381a29",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4a0d752e-c897-4120-ae57-375257381a29",
    "outputId": "67a8532a-94d2-4914-a30b-3df38062cd3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "IS_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IS_COLAB:\n",
    "    print(\"Installing packages\")\n",
    "    !pip install --upgrade --quiet -r {req_file}\n",
    "else:\n",
    "    print(\"Running locally.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "559c66cc-dce5-4c25-a9cd-9c090a89f883",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "# if IS_COLAB:\n",
    "from sklearnex import patch_sklearn \n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "827eff72-dcd3-4cc6-9d26-20d959dd8f0e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "827eff72-dcd3-4cc6-9d26-20d959dd8f0e",
    "outputId": "c620ae66-1756-4ae3-ab02-5f7f52766df5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting setup/chp04_4.2_imports.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile setup/chp04_4.2_imports.py\n",
    "import locale\n",
    "import os\n",
    "import pprint\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa\n",
    "from nltk.tokenize import casual_tokenize\n",
    "from nltk.tokenize.casual import casual_tokenize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm.auto import tqdm\n",
    "from watermark import watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00f41d56-423e-41b9-984f-00c5bb19ae10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "00f41d56-423e-41b9-984f-00c5bb19ae10",
    "outputId": "addf8b62-d717-4339-ec3a-a6d945ad42e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import locale\n",
      "import os\n",
      "import pprint\n",
      "import random\n",
      "import warnings\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "from mpl_toolkits.mplot3d import Axes3D  # noqa\n",
      "from nltk.tokenize import casual_tokenize\n",
      "from nltk.tokenize.casual import casual_tokenize\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from tqdm.auto import tqdm\n",
      "from watermark import watermark\n"
     ]
    }
   ],
   "source": [
    "!isort setup/chp04_4.2_imports.py --sl\n",
    "!cat setup/chp04_4.2_imports.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8371a67-5f08-49e8-9c43-4d4e7613654c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b8371a67-5f08-49e8-9c43-4d4e7613654c",
    "outputId": "f30a4048-681a-468d-830d-bbcaa40d65cf"
   },
   "outputs": [],
   "source": [
    "import locale\n",
    "import os\n",
    "import pprint\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa\n",
    "from nltk.tokenize import casual_tokenize\n",
    "from nltk.tokenize.casual import casual_tokenize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm.auto import tqdm\n",
    "from watermark import watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8704e77a-43ea-48ea-9ba3-ce84b66c36f4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8704e77a-43ea-48ea-9ba3-ce84b66c36f4",
    "outputId": "ff17e01c-0f45-4baf-9b77-dedbc17c968c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.8.12\n",
      "IPython version      : 7.34.0\n",
      "\n",
      "Compiler    : Clang 13.0.0 (clang-1300.0.29.3)\n",
      "OS          : Darwin\n",
      "Release     : 21.6.0\n",
      "Machine     : x86_64\n",
      "Processor   : i386\n",
      "CPU cores   : 4\n",
      "Architecture: 64bit\n",
      "\n",
      "numpy  : 1.23.5\n",
      "pandas : 1.5.3\n",
      "seaborn: 0.12.1\n",
      "sys    : 3.8.12 (default, Dec 13 2021, 20:17:08) \n",
      "[Clang 13.0.0 (clang-1300.0.29.3)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def HR():\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "def getpreferredencoding(do_setlocale = True):\n",
    "    return \"UTF-8\"\n",
    "\n",
    "locale.getpreferredencoding = getpreferredencoding\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style(\"darkgrid\")\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(watermark(iversions=True,globals_=globals(),python=True,machine=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8c5593-5957-4d87-87a4-2b39560b80db",
   "metadata": {
    "id": "bc8c5593-5957-4d87-87a4-2b39560b80db"
   },
   "source": [
    "---\n",
    "<a name='4.2'></a><a id='4.2'></a>\n",
    "# 4.2 Latent semantic analysis (in-depth explanation)\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "Problem: What is the underlying mechanism of LSA?\n",
    "\n",
    "Idea: SVD linear algebra\n",
    "\n",
    "Importance: Using SVD, LSA can break down a TF-IDF term-document matrix into three simpler matrices. These matrices can be multiplied back together in truncated form. These truncated matrices don't return the exact TF-IDF document matrix, but a simpler representation containing the essence, the \"latent semantics\" of those documents. This is akin to compression as used in JPEG images. When SVD is used this way in NLP, this is called latent semantic analysis. LSA uncovers the semantics, or meaning, of words that is hidden. Latent semantic analysis is a mathematical technique for finding the \"best\" way to linearly transform any set of NLP vectors. \n",
    "\n",
    "LSA is a way to train a machine to recognize the meaning (semantics) of words and phrases by giving the machine some example usages. Like people, machines can learn better semantics from example usages of words much faster and easier than they can from dictionary definitions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7986a2e-f9d9-438b-ae7b-d4d62d2ae073",
   "metadata": {
    "id": "c7986a2e-f9d9-438b-ae7b-d4d62d2ae073"
   },
   "source": [
    "<a name='4.2.1'></a><a id='4.2.1'></a>\n",
    "## 4.2.1 Your thought experiment made real\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "Problem: Explore computing specified topics from our thought experiment.\n",
    "\n",
    "Idea: Use an algorithm to try specifying topics such as \"animalness\", \"petness\", etc from our thought experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d708e9-9172-4dea-bece-62ee3538c51c",
   "metadata": {},
   "source": [
    "<a id='cats_and_dogs_sorted.txt'></a><a name='cats_and_dogs_sorted.txt'></a>\n",
    "### Dataset: cats_and_dogs_sorted.txt\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "S2HOnR2o2nMd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S2HOnR2o2nMd",
    "outputId": "bb23a2e9-cdb6-479c-f39d-2d3f5e9c560f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘data/data_cats_dogs/cats_and_dogs_sorted.txt’ already there; not retrieving.\n",
      "\n",
      "-rw-r--r--  1 gb  staff  10095 Mar 26 16:19 data/data_cats_dogs/cats_and_dogs_sorted.txt\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'data/data_cats_dogs'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "    \n",
    "data_cats_dogs = f\"{data_dir}/cats_and_dogs_sorted.txt\"\n",
    "!wget -P {data_dir} -nc https://github.com/totalgood/nlpia/raw/master/src/nlpia/data/cats_and_dogs_sorted.txt\n",
    "!ls -l {data_cats_dogs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22151fb5-8203-4e5d-998b-d397ee5e3fd0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "22151fb5-8203-4e5d-998b-d397ee5e3fd0",
    "outputId": "4cd71955-d6d4-49de-fdff-61ee90909322"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NYC is the Big Apple.\n",
      "NYC is known as the Big Apple.\n",
      "I love NYC!\n",
      "I wore a hat to the Big Apple party in NYC.\n",
      "Come to NYC. See the Big Apple!\n",
      "Manhattan is called the Big Apple.\n",
      "New York is a big city for a small cat.\n",
      "The lion, a big cat, is the king of the jungle.\n",
      "I love my pet cat.\n",
      "I love New York City (NYC).\n"
     ]
    }
   ],
   "source": [
    "!head {data_cats_dogs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61d3ce8b-2418-4a0d-b274-8d35594ddb10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "61d3ce8b-2418-4a0d-b274-8d35594ddb10",
    "outputId": "ae5565c7-0801-4010-9ee2-6aa33f2afff3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NYC is the Big Apple.', 'NYC is known as the Big Apple.', 'I love NYC!', 'I wore a hat to the Big Apple party in NYC.', 'Come to NYC. See the Big Apple!']\n",
      "----------------------------------------\n",
      "NYC is the Big Apple. NYC is known as the Big Apple. I love NYC! I wore a hat to the Big Apple party\n"
     ]
    }
   ],
   "source": [
    "with open(data_cats_dogs, 'r') as f:\n",
    "    contents_raw = [stripped for line in f if (stripped := line.strip())]\n",
    "    \n",
    "print(contents_raw[:5])\n",
    "HR()\n",
    "corpus = ' '.join(contents_raw)\n",
    "print(corpus[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69eacea7-b929-4ed1-a8b1-a1b47d32e5b5",
   "metadata": {},
   "source": [
    "---\n",
    "Topic-word matrix for LSA on 16 short sentences about cats, dogs, and NYC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dade4230-9867-4b81-87ed-e1eabc219462",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dade4230-9867-4b81-87ed-e1eabc219462",
    "outputId": "126c8aa6-ff30-4726-8bb0-9771cb29dd17"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>apple</th>\n",
       "      <th>lion</th>\n",
       "      <th>nyc</th>\n",
       "      <th>love</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>top0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cat  dog  apple  lion  nyc  love\n",
       "top0  1.0  0.0    0.0   0.0  0.0   0.0\n",
       "top1  0.0  1.0    0.0   0.0  0.0   0.0\n",
       "top2  0.0  0.0    1.0   0.0  0.0   0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From ch04_catdog_lsa_3x6x16.py\n",
    "\n",
    "NUM_TOPICS = 3\n",
    "NUM_WORDS = 6\n",
    "NUM_DOCS = NUM_PRETTY = 16\n",
    "SAVE_SORTED_CORPUS = ''  # 'cats_and_dogs_sorted.txt'\n",
    "STOPWORDS = []\n",
    "SYNONYMS = {}\n",
    "\n",
    "stemmer = None  # PorterStemmer()\n",
    "\n",
    "def normalize_corpus_words(corpus, stemmer=stemmer, synonyms=SYNONYMS, stopwords=STOPWORDS):\n",
    "    docs = [doc.lower() for doc in corpus]\n",
    "    docs = [casual_tokenize(doc) for doc in docs]\n",
    "    docs = [[synonyms.get(w, w) for w in words if w not in stopwords] for words in docs]\n",
    "    if stemmer:\n",
    "        docs = [[stemmer.stem(w) for w in words if w not in stopwords] for words in docs]\n",
    "    docs = [[synonyms.get(w, w) for w in words if w not in stopwords] for words in docs]\n",
    "    docs = [' '.join(w for w in words if w not in stopwords) for words in docs]\n",
    "    return docs\n",
    "\n",
    "def tokenize(text, vocabulary, synonyms=SYNONYMS, stopwords=STOPWORDS):\n",
    "    doc = normalize_corpus_words([text.lower()], synonyms=synonyms, stopwords=stopwords)[0]\n",
    "    stems = [w for w in doc.split() if w in vocabulary]\n",
    "    return stems\n",
    "\n",
    "fun_words = vocabulary = 'cat dog apple lion nyc love big small'\n",
    "fun_stems = normalize_corpus_words([fun_words])[0].split()[:NUM_WORDS]\n",
    "fun_words = fun_words.split()\n",
    "\n",
    "\n",
    "# do it all over again on a tiny portion of the corpus and vocabulary\n",
    "docs = normalize_corpus_words(corpus)\n",
    "\n",
    "tfidfer = TfidfVectorizer(\n",
    "    min_df=1, \n",
    "    max_df=.99, \n",
    "    stop_words=None, \n",
    "    token_pattern=r'(?u)\\b\\w+\\b',\n",
    "    vocabulary=fun_stems\n",
    ")\n",
    "\n",
    "tfidf_dense = pd.DataFrame(tfidfer.fit_transform(docs).todense())\n",
    "id_words = [(i, w) for (w, i) in tfidfer.vocabulary_.items()]\n",
    "tfidf_dense.columns = list(zip(*sorted(id_words)))[1]\n",
    "tfidfer.use_idf = False\n",
    "tfidfer.norm = None\n",
    "\n",
    "bow_dense = pd.DataFrame(tfidfer.fit_transform(docs).todense())\n",
    "bow_dense.columns = list(zip(*sorted(id_words)))[1]\n",
    "bow_dense = bow_dense.astype(int)\n",
    "\n",
    "tfidfer.use_idf = True\n",
    "tfidfer.norm = 'l2'\n",
    "bow_pretty = bow_dense.copy()\n",
    "bow_pretty = bow_pretty[fun_stems]\n",
    "bow_pretty['text'] = corpus\n",
    "for col in fun_stems:\n",
    "    bow_pretty.loc[bow_pretty[col] == 0, col] = ''\n",
    "\n",
    "# print(bow_pretty)\n",
    "word_tfidf_dense = pd.DataFrame(tfidfer.transform(fun_stems).todense())\n",
    "word_tfidf_dense.columns = list(zip(*sorted(id_words)))[1]\n",
    "word_tfidf_dense.index = fun_stems\n",
    "\n",
    "tfidf_pretty = tfidf_dense.copy()\n",
    "tfidf_pretty = tfidf_pretty[fun_stems]\n",
    "tfidf_pretty = tfidf_pretty.round(2)\n",
    "for col in fun_stems:\n",
    "    tfidf_pretty.loc[tfidf_pretty[col] == 0, col] = ''\n",
    "\n",
    "tfidf_zeros = tfidf_dense.T.sum()[tfidf_dense.T.sum() == 0]\n",
    "\n",
    "[corpus[i] for i in tfidf_zeros.index]\n",
    "\n",
    "pcaer = PCA(n_components=NUM_TOPICS)\n",
    "\n",
    "doc_topic_vectors = pd.DataFrame(\n",
    "    pcaer.fit_transform(tfidf_dense.values), \n",
    "    columns=['top{}'.format(i) for i in range(NUM_TOPICS)]\n",
    ")\n",
    "\n",
    "doc_topic_vectors['text'] = corpus\n",
    "pd.options.display.max_colwidth = 55\n",
    "\n",
    "word_topic_vectors = pd.DataFrame(\n",
    "    pcaer.transform(word_tfidf_dense.values),\n",
    "    columns=['top{}'.format(i) for i in range(NUM_TOPICS)]\n",
    ")\n",
    "\n",
    "word_topic_vectors.index = fun_stems\n",
    "\n",
    "def tfidf_search(text, corpus=tfidf_dense, corpus_text=corpus):\n",
    "    \"\"\" search for the most relevant document \"\"\"\n",
    "    tokens = tokenize(text, vocabulary=corpus.columns)\n",
    "    tfidf_vector_query = np.array(tfidfer.transform([' '.join(tokens)]).todense())[0]\n",
    "    query_series = pd.Series(tfidf_vector_query, index=corpus.columns)\n",
    "\n",
    "    return corpus_text[query_series.dot(corpus.T).values.argmax()]\n",
    "\n",
    "def topic_search(text, corpus=doc_topic_vectors, pcaer=pcaer, corpus_text=corpus):\n",
    "    \"\"\" search for the most relevant document \"\"\"\n",
    "    tokens = tokenize(text, vocabulary=corpus.columns)\n",
    "    tfidf_vector_query = np.array(tfidfer.transform([' '.join(tokens)]).todense())[0]\n",
    "    topic_vector_query = pcaer.transform([tfidf_vector_query])\n",
    "    query_series = pd.Series(topic_vector_query, index=corpus.columns)\n",
    "    return corpus_text[query_series.dot(corpus.T).values.argmax()]\n",
    "\n",
    "\n",
    "U, Sigma, VT = np.linalg.svd(tfidf_dense.T)  # <1> Transpose the doc-word tfidf matrix, because SVD works on column vectors\n",
    "S = Sigma.copy()\n",
    "S[4:] = 0\n",
    "doc_labels = ['doc{}'.format(i) for i in range(len(tfidf_dense))]\n",
    "U_df = pd.DataFrame(U, index=fun_stems, columns=fun_stems)\n",
    "VT_df = pd.DataFrame(VT, index=doc_labels, columns=doc_labels)\n",
    "ndim = 2\n",
    "truncated_tfidf = U[:, :ndim].dot(np.diag(Sigma)[:ndim, :ndim]).dot(VT.T[:, :ndim].T)\n",
    "\n",
    "\n",
    "word_topic_vectors.T.round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9954ca-7744-441d-8bf0-9af4472311ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sx4ik9G0T5v9"
   },
   "source": [
    "<a id='top'></a><a name='top'></a>\n",
    "# Chapter 9: Text Summarization\n",
    "\n",
    "**Blueprints for Text Analysis Using Python**\n",
    "\n",
    "PROBLEMS WITH SCRIPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fkU3BQ1LYGWn"
   },
   "source": [
    "* [Introduction](#introduction)\n",
    "* [9.0 Imports and Setup](#9.0)\n",
    "* [9.1 Text Summarization](#9.1)\n",
    "    - [9.1.1 Extractive Methods](#9.1.1)\n",
    "    - [9.1.2 Data Preprocessing](#9.1.2)\n",
    "* [9.2 Blueprint: Summarizing Text Using Topic Representation](#9.2)\n",
    "    - [9.2.1 Identifying Important Words with TF-IDF Values](#9.2.1)\n",
    "    - [9.2.2 LSA Algorithm](#9.2.2)\n",
    "* [9.3 Blueprint: Summarizing Text Using an Indicator Representation](#9.3)\n",
    "* [9.4 Measuring the Performance of Text Summarization Methods](#9.4)\n",
    "* [9.5 Blueprint: Summarizing Text Using Machine Learning](#9.5)\n",
    "    - [9.5.1 Step 1: Creating Target Labels](#9.5.1)\n",
    "    - [9.5.2 Step 2: Adding Features to Assist Model Prediction](#9.5.2)\n",
    "    - [9.5.3 Step 3: Build a Machine Learning Model](#9.5.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2xUHHOuYGWr"
   },
   "source": [
    "---\n",
    "<a name='introduction'></a><a id='introduction'></a>\n",
    "# Introduction\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "### Dataset\n",
    "\n",
    "* acl2017.tex: [script](#acl2017.tex), [source](https://raw.githubusercontent.com/blueprints-for-text-analytics-python/blueprints-text/master/ch09/acl2017.tex)\n",
    "* predicting-the-next-u-s-recession-idUSKCN1V31JE: [script](#predicting-the-next-u-s-recession-idUSKCN1V31JE), [source](https://www.reuters.com/article/us-usa-economy-watchlist-graphic/predicting-the-next-u-s-recession-idUSKCN1V31JE)\n",
    "* travel_threads.csv.gz : [script](#travel_threads.csv.gz), [source](https://github.com/blueprints-for-text-analytics-python/blueprints-text/raw/master/data/travel-forum-threads/travel_threads.csv.gz)\n",
    "\n",
    "\n",
    "### Explore\n",
    "\n",
    "* Analyzing different types of text data.\n",
    "* Examine specific text data characteristics useful in determining the choice of summarization method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EXuYMAhxYGWt"
   },
   "source": [
    "---\n",
    "<a name='9.0'></a><a id='9.0'></a>\n",
    "# 9.0 Imports and Setup\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "R2oYvKS5BGOM"
   },
   "outputs": [],
   "source": [
    "# Start with clean project\n",
    "# !rm -f *.gz\n",
    "# !rm -f *.py\n",
    "# !rm -f *.txt\n",
    "# !rm -fr articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GulFEtI9BGON"
   },
   "outputs": [],
   "source": [
    "!mkdir articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "O2yQoLdVUCBy"
   },
   "outputs": [],
   "source": [
    "req_file = \"requirements_09.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZToiNYlDUCEb",
    "outputId": "8907252d-2278-4293-d010-c46e9958e115"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements_09.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile {req_file}\n",
    "isort\n",
    "rouge-score\n",
    "spacy\n",
    "sumy\n",
    "textacy\n",
    "textdistance\n",
    "tqdm\n",
    "watermark\n",
    "Wikipedia-API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "12MjfAenUCG0",
    "outputId": "235cb193-faf6-4f56-976c-c4731cac8aad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "IS_COLAB = 'google.colab' in sys.modules\n",
    "if IS_COLAB:\n",
    "    print(\"Installing packages\")\n",
    "    !pip install --upgrade --quiet -r {req_file}\n",
    "else:\n",
    "    print(\"Running locally.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G_B5kYM1YGWz",
    "outputId": "9ad1fd36-f344-4ce7-a235-25875613183d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing imports.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile imports.py\n",
    "import html\n",
    "import locale\n",
    "import os.path\n",
    "import pprint\n",
    "import random\n",
    "import re\n",
    "import reprlib\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import rouge_score\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "import sumy\n",
    "import textacy\n",
    "import textdistance\n",
    "import wikipediaapi\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil import parser\n",
    "from nltk import tokenize\n",
    "from rouge_score import rouge_scorer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from spacy.tokenizer import Tokenizer as spacy_Tokenizer\n",
    "from spacy.util import (compile_infix_regex, compile_prefix_regex,\n",
    "                        compile_suffix_regex)\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.nlp.tokenizers import Tokenizer as sumy_Tokenizer\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "from sumy.utils import get_stop_words\n",
    "from textacy.preprocessing import replace\n",
    "from tqdm import tqdm\n",
    "from watermark import watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iQk0_JfXYGW1",
    "outputId": "151119cb-5a94-48dc-babf-264554991edc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import html\n",
      "import locale\n",
      "import os.path\n",
      "import pprint\n",
      "import random\n",
      "import re\n",
      "import reprlib\n",
      "import warnings\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import nltk\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import requests\n",
      "import rouge_score\n",
      "import seaborn as sns\n",
      "import spacy\n",
      "import sumy\n",
      "import textacy\n",
      "import textdistance\n",
      "import wikipediaapi\n",
      "from bs4 import BeautifulSoup\n",
      "from dateutil import parser\n",
      "from nltk import tokenize\n",
      "from rouge_score import rouge_scorer\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.model_selection import GroupShuffleSplit\n",
      "from spacy.tokenizer import Tokenizer as spacy_Tokenizer\n",
      "from spacy.util import (compile_infix_regex, compile_prefix_regex,\n",
      "                        compile_suffix_regex)\n",
      "from sumy.nlp.stemmers import Stemmer\n",
      "from sumy.nlp.tokenizers import Tokenizer as sumy_Tokenizer\n",
      "from sumy.parsers.plaintext import PlaintextParser\n",
      "from sumy.summarizers.lsa import LsaSummarizer\n",
      "from sumy.summarizers.text_rank import TextRankSummarizer\n",
      "from sumy.utils import get_stop_words\n",
      "from textacy.preprocessing import replace\n",
      "from tqdm import tqdm\n",
      "from watermark import watermark\n"
     ]
    }
   ],
   "source": [
    "!isort imports.py\n",
    "!cat imports.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S-IP1C14T5wG",
    "outputId": "38c5e741-50cc-4c1d-ab68-c33cf88ba6c7"
   },
   "outputs": [],
   "source": [
    "import html\n",
    "import locale\n",
    "import os.path\n",
    "import pprint\n",
    "import random\n",
    "import re\n",
    "import reprlib\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import rouge_score\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "import sumy\n",
    "import textacy\n",
    "import textdistance\n",
    "import wikipediaapi\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil import parser\n",
    "from nltk import tokenize\n",
    "from rouge_score import rouge_scorer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from spacy.tokenizer import Tokenizer as spacy_Tokenizer\n",
    "from spacy.util import (compile_infix_regex, compile_prefix_regex,\n",
    "                        compile_suffix_regex)\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.nlp.tokenizers import Tokenizer as sumy_Tokenizer\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "from sumy.utils import get_stop_words\n",
    "from textacy.preprocessing import replace\n",
    "from tqdm import tqdm\n",
    "from watermark import watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yFasvLnkUeK6",
    "outputId": "b536334d-c3c9-4dbb-881a-1361da886f78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.11.5\n",
      "IPython version      : 8.18.1\n",
      "\n",
      "Compiler    : Clang 14.0.0 (clang-1400.0.29.202)\n",
      "OS          : Darwin\n",
      "Release     : 21.6.0\n",
      "Machine     : x86_64\n",
      "Processor   : i386\n",
      "CPU cores   : 4\n",
      "Architecture: 64bit\n",
      "\n",
      "sumy        : 0.11.0\n",
      "dateutil    : 2.9.0.post0\n",
      "numpy       : 1.26.0\n",
      "textacy     : 0.13.0\n",
      "pandas      : 2.2.2\n",
      "spacy       : 3.7.4\n",
      "re          : 2.2.1\n",
      "rouge_score : 0.1.2\n",
      "sys         : 3.11.5 (main, Jan 16 2024, 17:25:53) [Clang 14.0.0 (clang-1400.0.29.202)]\n",
      "matplotlib  : 3.9.2\n",
      "requests    : 2.32.3\n",
      "nltk        : 3.8.1\n",
      "seaborn     : 0.13.2\n",
      "wikipediaapi: (0, 7, 1)\n",
      "textdistance: 4.6.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def HR():\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "def getpreferredencoding(do_setlocale = True):\n",
    "    return \"UTF-8\"\n",
    "\n",
    "locale.getpreferredencoding = getpreferredencoding\n",
    "warnings.filterwarnings('ignore')\n",
    "BASE_DIR = '.'\n",
    "sns.set_style(\"darkgrid\")\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "LANGUAGE = \"english\"\n",
    "\n",
    "print(watermark(iversions=True, globals_=globals(),python=True, machine=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s1sVfsBNYGW4",
    "outputId": "83ac7325-c2e8-468c-b576-a411033911db"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/gpb/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloads\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "w_je7kUeV-mf"
   },
   "outputs": [],
   "source": [
    "def regex_clean(text):\n",
    "    # convert html escapes like &amp; to characters.\n",
    "    text = html.unescape(text) \n",
    "    # tags like <tab>\n",
    "    text = re.sub(r'<[^<>]*>', ' ', text)\n",
    "    # markdown URLs like [Some text](https://....)\n",
    "    text = re.sub(r'\\[([^\\[\\]]*)\\]\\([^\\(\\)]*\\)', r'\\1', text)\n",
    "    # text or code in brackets like [0]\n",
    "    text = re.sub(r'\\[[^\\[\\]]*\\]', ' ', text)\n",
    "    # standalone sequences of specials, matches &# but not #cool\n",
    "    text = re.sub(r'(?:^|\\s)[&#<>{}\\[\\]+|\\\\:-]{1,}(?:\\s|$)', ' ', text)\n",
    "    # standalone sequences of hyphens like --- or ==\n",
    "    text = re.sub(r'(?:^|\\s)[\\-=\\+]{2,}(?:\\s|$)', ' ', text)\n",
    "    # sequences of white spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "rbCPL_NcT5wK"
   },
   "outputs": [],
   "source": [
    "def download_article(url):\n",
    "    # check if article already there\n",
    "    filename = url.split(\"/\")[-1] + \".html\"\n",
    "    if not os.path.isfile(filename):\n",
    "        r = requests.get(url)\n",
    "        with open(filename, \"w+\") as f:\n",
    "            f.write(r.text)\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4tQV9F8bSXb"
   },
   "source": [
    "**Setting up parse_article for Beautiful Soup**\n",
    "\n",
    "* Right click on article element for 'Inspect Accessibility Properties'\n",
    "* Copy entry for DOMNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "qV9TJXk6T5wK"
   },
   "outputs": [],
   "source": [
    "def parse_article(article_file):\n",
    "    print(f\"ARTICLE_FILE: {article_file}\")\n",
    "    HR()\n",
    "    with open(article_file, \"r\") as f:\n",
    "        html = f.read()\n",
    "    r = {}\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # r['id'] = soup.select_one(\"div.StandardArticle_inner-container\")['id']\n",
    "    r['url'] = soup.find(\"link\", {'rel': 'canonical'})['href']\n",
    "    r['headline'] = soup.h1.text\n",
    "    \n",
    "    #r['section'] = soup.select_one(\"div.ArticleHeader_channel a\").text\n",
    "    \n",
    "    r['text'] = soup.select_one(\"p.Paragraph-paragraph-2Bgue.ArticleBody-para-TD_9x\").text\n",
    "    # r['text'] = soup.select_one(\"div.StandardArticleBody_body\").text\n",
    "\n",
    "    r['authors'] = [a.text \n",
    "                    for a in soup.select(\"div.BylineBar_first-container.ArticleHeader_byline-bar\\\n",
    "                                          div.BylineBar_byline span\")]\n",
    "    r['time'] = soup.find(\"meta\", { 'property': \"og:article:published_time\"})['content']\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNhPIQ6gYGW8"
   },
   "source": [
    "<a name='9.1'></a><a id='9.1'></a>\n",
    "# 9.1 Text Summarization\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxja6OnHYGW9"
   },
   "source": [
    "<a name='9.1.1'></a><a id='9.1.1'></a>\n",
    "## 9.1.1 Extractive Methods\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "No source code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VqlJkDN0YGW9"
   },
   "source": [
    "<a name='9.1.2'></a><a id='9.1.2'></a>\n",
    "## 9.1.2 Data Preprocessing\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='news-sitemap'></a><a name='news-sitemap'></a>\n",
    "### Dataset: news-sitemap\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RoCm5K37T5wL",
    "outputId": "2c209e17-3b20-496f-be34-3750120d7519"
   },
   "outputs": [],
   "source": [
    "r = reprlib.Repr()\n",
    "r.maxstring = 800\n",
    "article_dir = 'articles'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_name1 = \"what-is-5g-and-who-are-the-major-players-idUSKCN1GR1IN.html\"\n",
    "# !wget -P {article_dir} -nc https://www.reuters.com/article/us-qualcomm-m-a-broadcom-5g/what-is-5g-and-who-are-the-major-players-idUSKCN1GR1IN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--@ 1 gpb  staff  586738 11  3 17:04 articles/what-is-5g-and-who-are-the-major-players-idUSKCN1GR1IN.html\n"
     ]
    }
   ],
   "source": [
    "!ls -l articles/{article_name1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TCruaHHLXmoM",
    "outputId": "065c29d3-1b8d-4c06-a1ec-b3980b138a5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARTICLE_FILE: articles/what-is-5g-and-who-are-the-major-players-idUSKCN1GR1IN.html\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m article1 \u001b[38;5;241m=\u001b[39m \u001b[43mparse_article\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43marticle_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43marticle_name1\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArticle Published on\u001b[39m\u001b[38;5;124m'\u001b[39m, r\u001b[38;5;241m.\u001b[39mrepr(article1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m (r\u001b[38;5;241m.\u001b[39mrepr(article1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "Cell \u001b[0;32mIn[17], line 15\u001b[0m, in \u001b[0;36mparse_article\u001b[0;34m(article_file)\u001b[0m\n\u001b[1;32m     11\u001b[0m r[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheadline\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mh1\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#r['section'] = soup.select_one(\"div.ArticleHeader_channel a\").text\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m r[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msoup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_one\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mp.Paragraph-paragraph-2Bgue.ArticleBody-para-TD_9x\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# r['text'] = soup.select_one(\"div.StandardArticleBody_body\").text\u001b[39;00m\n\u001b[1;32m     18\u001b[0m r[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthors\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [a\u001b[38;5;241m.\u001b[39mtext \n\u001b[1;32m     19\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiv.BylineBar_first-container.ArticleHeader_byline-bar\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124m                                      div.BylineBar_byline span\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "article1 = parse_article(f\"{article_dir}/{article_name1}\")\n",
    "\n",
    "print ('Article Published on', r.repr(article1['time']))\n",
    "print (r.repr(article1['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zEhjTmh3T5wM"
   },
   "source": [
    "---\n",
    "<a name='9.2'></a><a id='9.2'></a>\n",
    "# 9.2 Blueprint: Summarizing Text Using Topic Representation\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrqnGltJT5wN"
   },
   "source": [
    "<a name='9.2.1'></a><a id='9.2.1'></a>\n",
    "## 9.2.1 Identifying Important Words with TF-IDF Values\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "The simplest approach for summarizing text is to identify important sentences based on an aggregate of the TF-IDF values of the words in the sentence. \n",
    "\n",
    "Here, we apply the TF-IDF vectorization and then aggregate the values to a sentence level. We can generate a score for each sentence as a sum of the TF-IDF values for each word in that sentence. This means a sentence with a high score contains many important words, relative to other sentences in the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ox5DmG5DT5wN"
   },
   "outputs": [],
   "source": [
    "sentences = tokenize.sent_tokenize(article1['text'])\n",
    "tfidfVectorizer = TfidfVectorizer()\n",
    "words_tfidf = tfidfVectorizer.fit_transform(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qschI6ARGfKV"
   },
   "source": [
    "Here, there are approximately 20 sentences in the article. We create a condensed summary that is only 10% of the size of the original article. We sum up the TF-IDF values for each sentence, and use ng.argsort to sort them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3eczfQG7T5wN",
    "outputId": "e09c796a-d3d3-4cf7-ca7d-3cc8ae41e47b"
   },
   "outputs": [],
   "source": [
    "# Parameter to specify number of summary sentences required\n",
    "num_summary_sentence = 3\n",
    "\n",
    "# Sort the sentences in descending order by the sum of TF-IDF values\n",
    "sent_sum = words_tfidf.sum(axis=1)\n",
    "important_sent = np.argsort(sent_sum, axis=0)[::-1]\n",
    "\n",
    "# Print three most important sentences in the order they appear in the article\n",
    "for i in range(0, len(sentences)):\n",
    "    if i in important_sent[:num_summary_sentence]:\n",
    "        print (sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scjd8ehGT5wO"
   },
   "outputs": [],
   "source": [
    "def tfidf_summary(text, num_summary_sentence):\n",
    "    summary_sentence = []\n",
    "    sentences = tokenize.sent_tokenize(text)\n",
    "    tfidfVectorizer = TfidfVectorizer()\n",
    "    words_tfidf = tfidfVectorizer.fit_transform(sentences)\n",
    "    sentence_sum = words_tfidf.sum(axis=1)\n",
    "    important_sentences = np.argsort(sentence_sum, axis=0)[::-1]\n",
    "    for i in range(0, len(sentences)):\n",
    "        if i in important_sentences[:num_summary_sentence]:\n",
    "            summary_sentence.append(sentences[i])\n",
    "    return summary_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_pPKWuycT5wO",
    "tags": []
   },
   "source": [
    "<a name='9.2.2'></a><a id='9.2.2'></a>\n",
    "## 9.2.2 LSA Algorithm\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "LSA is a general-purpose method used for topic modeling, document similarity, and other tasks. LSA assumes that words close in meaning will occur in the same documents. \n",
    "\n",
    "https://github.com/miso-belica/sumy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JK8AaJxbT5wP",
    "outputId": "60ec6f04-1755-4f73-bb2c-7a13e482dd7e"
   },
   "outputs": [],
   "source": [
    "LANGUAGE = \"english\"\n",
    "stemmer = Stemmer(LANGUAGE)\n",
    "parser = PlaintextParser.from_string(article1['text'], sumy_Tokenizer(LANGUAGE))\n",
    "summarizer = LsaSummarizer(stemmer)\n",
    "summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "\n",
    "for sentence in summarizer(parser.document, num_summary_sentence):\n",
    "    print (str(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UTShujjfT5wP"
   },
   "outputs": [],
   "source": [
    "def lsa_summary(text, num_summary_sentence):\n",
    "    summary_sentence = []\n",
    "    LANGUAGE = \"english\"\n",
    "    stemmer = Stemmer(LANGUAGE)\n",
    "    parser = PlaintextParser.from_string(text, sumy_Tokenizer(LANGUAGE))\n",
    "    summarizer = LsaSummarizer(stemmer)\n",
    "    summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "    for sentence in summarizer(parser.document, num_summary_sentence):\n",
    "        summary_sentence.append(str(sentence))\n",
    "    return summary_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='predicting-the-next-u-s-recession-idUSKCN1V31JE'></a><a name='predicting-the-next-u-s-recession-idUSKCN1V31JE'></a>\n",
    "### Dataset: predicting-the-next-u-s-recession-idUSKCN1V31JE\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jmmbbRaxYGXB",
    "outputId": "ff4cdf2c-0f3a-422b-d207-c88cb464f5fb"
   },
   "outputs": [],
   "source": [
    "article_name2 = 'predicting-the-next-u-s-recession-idUSKCN1V31JE'\n",
    "!wget -P {article_dir} -nc -q \"https://www.reuters.com/article/us-usa-economy-watchlist-graphic/predicting-the-next-u-s-recession-idUSKCN1V31JE\"\n",
    "!ls -l {article_dir}/{article_name2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yNNVXo3iT5wQ",
    "outputId": "735b4b4b-fe14-40f4-e817-6fbbad2a638e"
   },
   "outputs": [],
   "source": [
    "r.maxstring = 800\n",
    "article2 = parse_article(f\"{article_dir}/{article_name2}\")\n",
    "print ('Article Published', r.repr(article1['time']))\n",
    "HR()\n",
    "print (r.repr(article2['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "id": "45e9T-Wm0NO2",
    "outputId": "ad7f255c-0451-4c48-dc5d-577b4f4e0c44"
   },
   "outputs": [],
   "source": [
    "article2['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y5kUXp__T5wQ",
    "outputId": "59286762-6b76-4df0-9d18-986f7cef172e"
   },
   "outputs": [],
   "source": [
    "summary_sentence = tfidf_summary(article2['text'], num_summary_sentence)\n",
    "\n",
    "for sentence in summary_sentence:\n",
    "    print (sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eRbH3vO8T5wR",
    "outputId": "7504fedc-3fa0-4e06-e28a-6f3514632906"
   },
   "outputs": [],
   "source": [
    "summary_sentence = lsa_summary(article2['text'], num_summary_sentence)\n",
    "\n",
    "for sentence in summary_sentence:\n",
    "    print (sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5vN6VK0gT5wR"
   },
   "source": [
    "---\n",
    "<a name='9.3'></a><a id='9.3'></a>\n",
    "# 9.3 Blueprint: Summarizing Text Using an Indicator Representation\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VoTtDaXyT5wR",
    "outputId": "00d9ead9-d1fa-433b-ce84-c5bb3987e0bb"
   },
   "outputs": [],
   "source": [
    "parser = PlaintextParser.from_string(article2['text'], sumy_Tokenizer(LANGUAGE))\n",
    "summarizer = TextRankSummarizer(stemmer)\n",
    "summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "\n",
    "for sentence in summarizer(parser.document, num_summary_sentence):\n",
    "    print (str(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CWQUQhKgT5wR"
   },
   "outputs": [],
   "source": [
    "def textrank_summary(text, num_summary_sentence):\n",
    "    summary_sentence = []\n",
    "    LANGUAGE = \"english\"\n",
    "    stemmer = Stemmer(LANGUAGE)\n",
    "    parser = PlaintextParser.from_string(text, sumy_Tokenizer(LANGUAGE))\n",
    "    summarizer = TextRankSummarizer(stemmer)\n",
    "    summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "    for sentence in summarizer(parser.document, num_summary_sentence):\n",
    "        summary_sentence.append(str(sentence))\n",
    "    return summary_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zhPkZlj5T5wR",
    "outputId": "b0f4d6c1-bd85-4886-ca28-6786ae1f2008"
   },
   "outputs": [],
   "source": [
    "parser = PlaintextParser.from_string(article1['text'], sumy_Tokenizer(LANGUAGE))\n",
    "summarizer = TextRankSummarizer(stemmer)\n",
    "summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "\n",
    "for sentence in summarizer(parser.document, num_summary_sentence):\n",
    "    print (str(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Xv6jO4YT5wS"
   },
   "outputs": [],
   "source": [
    "wiki_wiki = wikipediaapi.Wikipedia(\n",
    "        language='en',\n",
    "        extract_format=wikipediaapi.ExtractFormat.WIKI\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jfO1ODdQT5wS"
   },
   "outputs": [],
   "source": [
    "r.maxstring = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OB_AYiWOT5wT",
    "outputId": "27d9766e-1ddf-49a5-830d-8721e26be753"
   },
   "outputs": [],
   "source": [
    "p_wiki = wiki_wiki.page('Mongol_invasion_of_Europe')\n",
    "print (r.repr(p_wiki.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fifZzXl5T5wT",
    "outputId": "7ed05202-c622-4f42-dd4b-5124b5458a47"
   },
   "outputs": [],
   "source": [
    "r.maxstring = 200\n",
    "\n",
    "num_summary_sentence = 10\n",
    "\n",
    "summary_sentence = textrank_summary(p_wiki.text, num_summary_sentence)\n",
    "\n",
    "for sentence in summary_sentence:\n",
    "    print (sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='acl2017.tex'></a><a name='acl2017.tex'></a>\n",
    "### Dataset: acl2017.tex\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DjIkaLBPiZPg",
    "outputId": "b93da2bf-9a13-4f06-d371-bb078064d1c6"
   },
   "outputs": [],
   "source": [
    "filename = 'acl2017.tex'\n",
    "!wget -P {article_dir} -nc -q https://raw.githubusercontent.com/blueprints-for-text-analytics-python/blueprints-text/master/ch09/acl2017.tex\n",
    "!ls -l {article_dir}/{filename}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SYFrBtDsT5wU",
    "outputId": "fdf86661-d0b4-433a-8bf8-de6fb3f51403"
   },
   "outputs": [],
   "source": [
    "parser = PlaintextParser.from_file(f\"{article_dir}/{filename}\", sumy.nlp.tokenizers.Tokenizer(LANGUAGE))\n",
    "summarizer = TextRankSummarizer(stemmer)\n",
    "summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "\n",
    "for sentence in summarizer(parser.document, 5):\n",
    "    print (str(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZP_5Ae4EYGXK"
   },
   "source": [
    "---\n",
    "<a name='9.4'></a><a id='9.4'></a>\n",
    "# 9.4 Measuring the Performance of Text Summarization Methods\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VI7H8-gBT5wV"
   },
   "outputs": [],
   "source": [
    "def print_rouge_score(rouge_score):\n",
    "    for k,v in rouge_score.items():\n",
    "        print (k, 'Precision:', \"{:.2f}\".format(v.precision), 'Recall:', \"{:.2f}\".format(v.recall), 'fmeasure:', \"{:.2f}\".format(v.fmeasure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gxqz4APkT5wV",
    "outputId": "8fbfc8de-b6b6-43c3-a7c9-1a797108f745"
   },
   "outputs": [],
   "source": [
    "num_summary_sentence = 3\n",
    "gold_standard = article2['headline']\n",
    "summary = \"\"\n",
    "\n",
    "summary = ''.join(textrank_summary(article2['text'], num_summary_sentence))\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
    "scores = scorer.score(gold_standard, summary)\n",
    "print_rouge_score(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Obm8tOIT5wV",
    "outputId": "eaae4672-c8dc-46f6-88e0-00ba2480ea40"
   },
   "outputs": [],
   "source": [
    "summary = ''.join(lsa_summary(article2['text'], num_summary_sentence))\n",
    "scores = scorer.score(gold_standard, summary)\n",
    "print_rouge_score(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LZc0G6l-T5wW",
    "outputId": "5ba05bcb-056c-4f65-e1cd-95e0a7289ea4"
   },
   "outputs": [],
   "source": [
    "num_summary_sentence = 10 ##\n",
    "gold_standard = p_wiki.summary\n",
    "\n",
    "summary = ''.join(textrank_summary(p_wiki.text, num_summary_sentence))\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge2','rougeL'], use_stemmer=True)\n",
    "scores = scorer.score(gold_standard, summary)\n",
    "print_rouge_score(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1A3P7AaOT5wW",
    "outputId": "049e9a1b-01bc-40b3-cf80-b930ad2b763b"
   },
   "outputs": [],
   "source": [
    "summary = ''.join(lsa_summary(p_wiki.text, num_summary_sentence))\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge2','rougeL'], use_stemmer=True)\n",
    "scores = scorer.score(gold_standard, summary)\n",
    "print_rouge_score(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2rfqjRScT5wX"
   },
   "source": [
    "---\n",
    "<a name='9.5'></a><a id='9.5'></a>\n",
    "# 9.5 Blueprint: Summarizing Text Using Machine Learning\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KrGpUWhxT5wX"
   },
   "source": [
    "<a name='9.5.1'></a><a id='9.5.1'></a>\n",
    "## 9.5.1 Step 1: Creating target labels\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='travel_threads.csv.gz'></a><a name='travel_threads.csv.gz'></a>\n",
    "### Dataset: travel_threads.csv.gz\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7WdUwuMwVPNV",
    "outputId": "090b12e2-6b99-4a3c-dd3e-3d64fb4cbdc4"
   },
   "outputs": [],
   "source": [
    "file = \"travel_threads.csv.gz\"\n",
    "!wget -nc -q https://github.com/blueprints-for-text-analytics-python/blueprints-text/raw/master/data/travel-forum-threads/travel_threads.csv.gz\n",
    "!ls -l {file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "co61rar-T5wY",
    "outputId": "f712f724-9edd-4dfc-d5ac-83abd8ee8555"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(file, sep='|', dtype={'ThreadID': 'object'})\n",
    "df[df['ThreadID']=='60763_5_3122150'].head(1).T\n",
    "\n",
    "# You can view the actual post here ###\n",
    "# URL - https://www.tripadvisor.com/ShowTopic-g60763-i5-k3122150-Which_attractions_need_to_be_pre_booked-New_York_City_New_York.html ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sxtOHfLwT5wZ",
    "outputId": "ca9e7915-a58c-46c3-b39f-f64e036bf2d8"
   },
   "outputs": [],
   "source": [
    "# Re-using the blueprint from Chapter 4 but adapting to add additional steps specific to this dataset\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "    # use default patterns except the ones matched by re.search\n",
    "    prefixes = [pattern for pattern in nlp.Defaults.prefixes \n",
    "                if pattern not in ['-', '_', '#']]\n",
    "    suffixes = [pattern for pattern in nlp.Defaults.suffixes\n",
    "                if pattern not in ['_']]\n",
    "    infixes  = [pattern for pattern in nlp.Defaults.infixes\n",
    "                if not re.search(pattern, 'xx-xx')]\n",
    "\n",
    "    return spacy_Tokenizer(\n",
    "        vocab          = nlp.vocab, \n",
    "        rules          = nlp.Defaults.tokenizer_exceptions,\n",
    "        prefix_search  = compile_prefix_regex(prefixes).search,\n",
    "        suffix_search  = compile_suffix_regex(suffixes).search,\n",
    "        infix_finditer = compile_infix_regex(infixes).finditer,\n",
    "        token_match    = nlp.Defaults.token_match\n",
    "    )\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n5viEyOom-cf"
   },
   "outputs": [],
   "source": [
    "def extract_lemmas(doc, **kwargs):\n",
    "    return [t.lemma_ for t in textacy.extract.words(doc, **kwargs)]\n",
    "\n",
    "def extract_noun_chunks(doc, include_pos=['NOUN'], sep='_'):\n",
    "    chunks = []\n",
    "    for noun_chunk in doc.noun_chunks:\n",
    "        chunk = [token.lemma_ for token in noun_chunk\n",
    "                 if token.pos_ in include_pos]\n",
    "        if len(chunk) >= 2:\n",
    "            chunks.append(sep.join(chunk))\n",
    "    return chunks\n",
    "\n",
    "def extract_entities(doc, include_types=None, sep='_'):\n",
    "\n",
    "    ents = textacy.extract.entities(doc, \n",
    "             include_types=include_types, \n",
    "             exclude_types=None, \n",
    "             drop_determiners=True, \n",
    "             min_freq=1)\n",
    "    \n",
    "    return [re.sub('\\s+', sep, e.lemma_)+'/'+e.label_ for e in ents]\n",
    "\n",
    "def clean(text):\n",
    "    # Replace URLs\n",
    "    text = replace.urls(text)\n",
    "    \n",
    "    # Replace semi-colons (relevant in Java code ending)\n",
    "    text = text.replace(';','')\n",
    "    \n",
    "    # Replace character tabs (present as literal in description field)\n",
    "    text = text.replace('\\t','')\n",
    "    \n",
    "    # Find and remove any stack traces - doesn't fix all code fragments but removes many exceptions\n",
    "    start_loc = text.find(\"Stack trace:\")\n",
    "    text = text[:start_loc]\n",
    "    \n",
    "    # Remove Hex Code\n",
    "    text = re.sub(r'(\\w+)0x\\w+', '', text)\n",
    "    \n",
    "    # Initialize Spacy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # From Blueprint function\n",
    "    lemmas = extract_lemmas(\n",
    "        doc, \n",
    "        exclude_pos = ['PART', 'PUNCT', 'DET', 'PRON', 'SYM', 'SPACE', 'NUM'],\n",
    "        filter_stops = True,\n",
    "        filter_nums = True,\n",
    "        filter_punct = True\n",
    "    )\n",
    "\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JYlwE4jzT5wZ",
    "outputId": "6118efad-1fbf-4cdd-eeb9-56ee55e7b5d3"
   },
   "outputs": [],
   "source": [
    "# Applying regex based cleaning function\n",
    "df['text'] = df['text'].progress_apply(regex_clean)\n",
    "\n",
    "# Extracting lemmas using spacy pipeline\n",
    "df['lemmas'] = df['text'].progress_apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JFsv4I-XT5wZ"
   },
   "outputs": [],
   "source": [
    "gss = GroupShuffleSplit(\n",
    "    n_splits=1, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_split, test_split = next(\n",
    "    gss.split(\n",
    "        df, \n",
    "        groups=df['ThreadID']\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "obWI2GmHT5wa",
    "outputId": "4c1c2577-a620-497b-eea1-e0133cf49f45"
   },
   "outputs": [],
   "source": [
    "train_df = df.iloc[train_split]\n",
    "test_df = df.iloc[test_split]\n",
    "\n",
    "print ('Number of threads for Training ', train_df['ThreadID'].nunique())\n",
    "print ('Number of threads for Testing ', test_df['ThreadID'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EwqSpy6-T5wc",
    "outputId": "eb09825b-5434-4f80-cb67-8eb7357ef2c7"
   },
   "outputs": [],
   "source": [
    "compression_factor = 0.3\n",
    "\n",
    "train_df['similarity'] = train_df.progress_apply(\n",
    "    lambda x: textdistance.jaro_winkler(x.text, x.summary), axis=1)\n",
    "\n",
    "train_df[\"rank\"] = train_df.groupby(\"ThreadID\")[\"similarity\"].rank(\n",
    "    \"max\", ascending=False)\n",
    "\n",
    "topN = lambda x: x <= np.ceil(compression_factor * x.max())\n",
    "train_df['summaryPost'] = train_df.groupby('ThreadID')['rank'].progress_apply(topN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "VWh-WfxDT5wc",
    "outputId": "7d6e3758-4045-4d0b-de0c-69614494ad12"
   },
   "outputs": [],
   "source": [
    "train_df[['text','summaryPost']][train_df['ThreadID']=='60763_5_3122150'].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dXLErCM0T5wc"
   },
   "source": [
    "<a name='9.5.2'></a><a id='9.5.2'></a>\n",
    "## 9.5.2 Step 2: Adding Features to Assist Model Prediction\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DX5elsJtT5wc",
    "outputId": "435c884d-1196-4922-a3b5-6c11d952d04d"
   },
   "outputs": [],
   "source": [
    "train_df['titleSimilarity'] = train_df.progress_apply(\n",
    "    lambda x: textdistance.jaro_winkler(x.text, x.Title), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RWbGpkMET5we"
   },
   "outputs": [],
   "source": [
    "## Adding post length as a feature\n",
    "train_df['textLength'] = train_df['text'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3fKo5XylT5we"
   },
   "outputs": [],
   "source": [
    "train_df.loc[train_df['textLength'] <= 20, 'summaryPost'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xlYPJ3JpT5we"
   },
   "outputs": [],
   "source": [
    "feature_cols = ['titleSimilarity','textLength','postNum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UzQqvWHET5we"
   },
   "outputs": [],
   "source": [
    "train_df['combined'] = [\n",
    "    ' '.join(map(str, l)) for l in train_df['lemmas'] if l is not '']\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df=10, ngram_range=(1, 2), stop_words=\"english\")\n",
    "tfidf_result = tfidf.fit_transform(train_df['combined']).toarray()\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf_result, columns=tfidf.get_feature_names_out())\n",
    "\n",
    "tfidf_df.columns = [\"word_\" + str(x) for x in tfidf_df.columns]\n",
    "tfidf_df.index = train_df.index\n",
    "train_df_tf = pd.concat([train_df[feature_cols], tfidf_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vsB84rAST5we",
    "outputId": "00ccc809-d825-4286-d024-b741544862e6"
   },
   "outputs": [],
   "source": [
    "test_df['similarity'] = test_df.progress_apply(lambda x: textdistance.jaro_winkler(x.text, x.summary), axis=1)\n",
    "test_df[\"rank\"] = test_df.groupby(\"ThreadID\")[\"similarity\"].rank(\"max\", ascending=False)\n",
    "\n",
    "topN = lambda x: x <= np.ceil(compression_factor * x.max())\n",
    "test_df['summaryPost'] = test_df.groupby('ThreadID')['rank'].progress_apply(topN)\n",
    "\n",
    "test_df['titleSimilarity'] = test_df.progress_apply(lambda x: textdistance.jaro_winkler(x.text, x.Title), axis=1)\n",
    "\n",
    "test_df['textLength'] = test_df['text'].str.len()\n",
    "\n",
    "test_df.loc[test_df['textLength'] <= 20, 'summaryPost'] = False\n",
    "\n",
    "test_df['combined'] = [' '.join(map(str, l)) for l in test_df['lemmas'] if l is not '']\n",
    "\n",
    "tfidf_result = tfidf.transform(test_df['combined']).toarray()\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf_result, columns = tfidf.get_feature_names_out())\n",
    "tfidf_df.columns = [\"word_\" + str(x) for x in tfidf_df.columns]\n",
    "tfidf_df.index = test_df.index\n",
    "test_df_tf = pd.concat([test_df[feature_cols], tfidf_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ofY2PJYT5we"
   },
   "source": [
    "<a name='9.5.3'></a><a id='9.5.3'></a>\n",
    "## 9.5.3 Step 3: Build a Machine Learning Model\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rqv6WfVbYGXZ"
   },
   "source": [
    "### API Notes\n",
    "\n",
    "[`sklearn.ensemble.RandomForestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "\n",
    "A random forest classifier.\n",
    "\n",
    "A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the max_samples parameter if bootstrap=True (default), otherwise the whole dataset is used to build each tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lmAVoK9XT5wf",
    "outputId": "f21c99be-9736-4153-9e6a-dac5d954836a"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model1 = RandomForestClassifier(\n",
    "    random_state=20,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "ktw3aqVnDXbA",
    "outputId": "e3e10dbc-ba62-4380-b677-e02a63540e3a"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# This takes a lot of time to run\n",
    "\n",
    "model1.fit(\n",
    "    train_df_tf, \n",
    "    train_df['summaryPost']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VS-PyziMT5wf"
   },
   "outputs": [],
   "source": [
    "# Function to calculate rouge_score for each thread\n",
    "def calculate_rouge_score(x, column_name):\n",
    "    # Get the original summary - only first value since they are repeated\n",
    "    ref_summary = x['summary'].values[0]\n",
    "    \n",
    "    # Join all posts that have been predicted as summary\n",
    "    predicted_summary = ''.join(x['text'][x[column_name]])\n",
    "    \n",
    "    # Return the rouge score for each ThreadID\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
    "    scores = scorer.score(ref_summary, predicted_summary)\n",
    "    return scores['rouge1'].fmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qgohb9YDT5wg",
    "outputId": "9004cf05-4135-4e34-9086-79e620b6a383"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "test_df['predictedSummaryPost'] = model1.predict(test_df_tf)\n",
    "print('Mean ROUGE-1 Score for test threads',\n",
    "      test_df.groupby('ThreadID')[['summary','text','predictedSummaryPost']] \\\n",
    "      .progress_apply(calculate_rouge_score, column_name='predictedSummaryPost').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "diSdDO6bT5wg",
    "outputId": "25086bfc-855f-480a-86ff-958c9d195055"
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "random.seed(2)\n",
    "random.sample(test_df['ThreadID'].unique().tolist(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Wdjro5klT5wh",
    "outputId": "f7c3f6a8-c563-4cff-b617-c03945fe0ac0"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "example_df = test_df[test_df['ThreadID'] == '60974_588_2180141']\n",
    "print('Total number of posts', example_df['postNum'].max())\n",
    "print('Number of summary posts',\n",
    "      example_df[example_df['predictedSummaryPost']].count().values[0])\n",
    "print('Title: ', example_df['Title'].values[0])\n",
    "example_df[['postNum', 'text']][example_df['predictedSummaryPost']]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
